{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named torch",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-d4b7de365436>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbleu_score\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msentence_bleu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named torch"
     ]
    }
   ],
   "source": [
    "from collections import Counter, defaultdict\n",
    "from IPython import display\n",
    "from nltk import word_tokenize\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from PIL import Image\n",
    "from torch import nn, optim\n",
    "from torch.autograd import Variable\n",
    "from torchvision import models, transforms\n",
    "\n",
    "import json\n",
    "import matplotlib\n",
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepocessing\n",
    "\n",
    "The following code preprocesses the data, collecting the image array as needed and preparing for ANP classifying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vso_images_folder = \"data/vso/vso_images_with_cc/\"\n",
    "\n",
    "train_anp_tags = []\n",
    "train_image_addresses = []\n",
    "train_image_to_anp_tag = {}\n",
    "for subdir in os.listdir(vso_images_folder):\n",
    "    if subdir.endswith(\"_train\"):\n",
    "        train_anp_tags.append(subdir.replace(\"_train\", \"\").replace(\"_\", \" \"))\n",
    "        for filename in os.listdir(vso_images_folder + subdir):\n",
    "            if filename.endswith(\".jpg\"):\n",
    "                train_image_addresses.append(vso_images_folder + subdir + \"/\"  + filename)\n",
    "                train_image_to_anp_tag[vso_images_folder + subdir + \"/\"  + filename] = subdir.replace(\"_train\", \"\").replace(\"_\", \" \")\n",
    "\n",
    "validation_anp_tags = []\n",
    "validation_image_addresses = []\n",
    "validation_image_to_anp_tag = {}\n",
    "for subdir in os.listdir(vso_images_folder):\n",
    "        if subdir.endswith(\"_validation\"):\n",
    "                validation_anp_tags.append(subdir.replace(\"_validation\", \"\").replace(\"_\", \" \"))\n",
    "                for filename in os.listdir(vso_images_folder + subdir):\n",
    "                        if filename.endswith(\".jpg\"):\n",
    "                                validation_image_addresses.append(vso_images_folder + subdir + \"/\"  + filename)\n",
    "                                validation_image_to_anp_tag[vso_images_folder + subdir + \"/\"  + filename] = subdir.replace(\"_validation\", \"\").replace(\"_\", \" \")\n",
    "\n",
    "test_anp_tags = []\n",
    "test_image_addresses = []\n",
    "test_image_to_anp_tag = {}\n",
    "for subdir in os.listdir(vso_images_folder):\n",
    "        if subdir.endswith(\"_test\"):\n",
    "                test_anp_tags.append(subdir.replace(\"_test\", \"\").replace(\"_\", \" \"))\n",
    "                for filename in os.listdir(vso_images_folder + subdir):\n",
    "                        if filename.endswith(\".jpg\"):\n",
    "                                test_image_addresses.append(vso_images_folder + subdir + \"/\"  + filename)\n",
    "                                test_image_to_anp_tag[vso_images_folder + subdir + \"/\"  + filename] = subdir.replace(\"_test\", \"\").replace(\"_\", \" \")\n",
    "\n",
    "anp_tag_to_vector = {}\n",
    "for i, tag in enumerate(train_anp_tags):\n",
    "    anp_vector = np.zeros(len(train_anp_tags))\n",
    "    anp_vector[i] = 1\n",
    "    anp_tag_to_vector[tag] = anp_vector\n",
    "                                \n",
    "print(\"Number of train images: \", len(train_image_to_anp_tag))\n",
    "print(\"Number of validation images: \", len(validation_image_to_anp_tag))\n",
    "print(\"Number of test images: \", len(test_image_to_anp_tag))\n",
    "\n",
    "least_height = 1000000\n",
    "least_width = 1000000\n",
    "count = 0\n",
    "for img in train_image_addresses:\n",
    "    try:\n",
    "        height, width = Image.open(img).size\n",
    "        if height <= 256 or width <= 256:\n",
    "            count += 1\n",
    "            train_image_addresses.remove(img)\n",
    "            continue\n",
    "        if height < least_height:\n",
    "            least_height = height\n",
    "        if width < least_width:\n",
    "            least_width = width\n",
    "    except:\n",
    "        print(\"Error occured at \", img)\n",
    "        train_image_addresses.remove(img)\n",
    "        \n",
    "for img in validation_image_addresses:\n",
    "    try:\n",
    "        height, width = Image.open(img).size\n",
    "        if height <= 256 or width <= 256:\n",
    "            count += 1\n",
    "            validation_image_addresses.remove(img)\n",
    "            continue\n",
    "        if height < least_height:\n",
    "            least_height = height\n",
    "        if width < least_width:\n",
    "            least_width = width\n",
    "    except:\n",
    "        print(\"Error occured at \", img)\n",
    "        validation_image_addresses.remove(img)\n",
    "        \n",
    "for img in test_image_addresses:\n",
    "    try:\n",
    "        height, width = Image.open(img).size\n",
    "        if height <= 256 or width <= 256:\n",
    "            count += 1\n",
    "            test_image_addresses.remove(img)\n",
    "            continue\n",
    "        if height < least_height:\n",
    "            least_height = height\n",
    "        if width < least_width:\n",
    "            least_width = width\n",
    "    except:\n",
    "        print(\"Error occured at \", img)\n",
    "        test_image_addresses.remove(img)\n",
    "        \n",
    "print(\"Removed images: \", count)\n",
    "        \n",
    "print(\"Minimum height of an image: \", least_height)\n",
    "print(\"Minimum width of an image: \", least_width)\n",
    "    \n",
    "img_size = 256\n",
    "loader = transforms.Compose([\n",
    "  transforms.Resize(img_size),\n",
    "  transforms.CenterCrop(img_size),\n",
    "  transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "def load_image(filename, volatile=False):\n",
    "    \"\"\"\n",
    "    Simple function to load and preprocess the images.\n",
    "    \"\"\"\n",
    "    print(filename)\n",
    "    image = Image.open(filename).convert('RGB')\n",
    "    image_tensor = loader(image).float()\n",
    "    image_var = Variable(image_tensor, volatile=volatile).unsqueeze(0)\n",
    "    return image_var.cuda()\n",
    "\n",
    "#print(load_image('data/vso/vso_images_with_cc/amazing_flowers/1066918516_e27cbf795e.jpg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample Data Points\n",
    "\n",
    "Showing a few examples of the sample data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display.display(display.Image(train_image_addresses[0]))\n",
    "print(\"Image: \", train_image_addresses[0])\n",
    "print(\"Associated ANP tag: \", train_image_to_anp_tag[train_image_addresses[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Defition\n",
    "\n",
    "The following define the model used for the ANP classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANPClassifier(nn.Module):\n",
    "    def __init__(self, output_size):\n",
    "        super(ANPClassifier, self).__init__()\n",
    "        \n",
    "        self.output_size = output_size\n",
    "        self.resnet = torchvision.models.resnet101(pretrained=False)\n",
    "        self.resnet.fc = nn.Linear(in_features=2048, out_features=output_size, bias=True)\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.resnet(X)\n",
    "\n",
    "main_model = ANPClassifier(len(train_anp_tags))\n",
    "main_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "Following describes training procedure used to form the ANP classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_CUDA = True # switch to true when training on GPU(s)\n",
    "\n",
    "def train_pass(image_input, target_output, model, optimizer, criterion):\n",
    "    \"\"\"\n",
    "    Given batch of images, completes one pass of training on the model,\n",
    "    using the given optimizer and criterion.\n",
    "    \"\"\"\n",
    "\n",
    "    if USE_CUDA:\n",
    "        image_input = image_input.cuda()\n",
    "        target_output = target_output.cuda()\n",
    "        model = model.cuda()\n",
    "        criterion = criterion.cuda()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    model_output = model(image_input)\n",
    "    \n",
    "    loss = criterion(model_output, target_output)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.data.cpu().numpy()\n",
    "\n",
    "def load_batch(image_addresses, volatile=False):\n",
    "    \n",
    "    img_tensor = load_image(image_addresses[0])\n",
    "    for i in range(1, len(image_addresses)):\n",
    "        img_tensor = torch.cat((img_tensor, load_image(image_addresses[i])))\n",
    "        \n",
    "    target_tensor = torch.from_numpy(anp_tag_to_vector[train_image_to_anp_tag[image_addresses[0]]]).unsqueeze(0)\n",
    "    for i in range(1, len(image_addresses)):\n",
    "        target_tensor = torch.cat((target_tensor, torch.from_numpy(anp_tag_to_vector[train_image_to_anp_tag[image_addresses[i]]]).unsqueeze(0)))\n",
    "        \n",
    "    return img_tensor, target_tensor.float()\n",
    "\n",
    "def train(model, learning_rate=0.0001, batch_size=50, epochs=50):\n",
    "    \n",
    "    # defining criterion and optimizer\n",
    "    criterion = nn.MultiLabelSoftMarginLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr = learning_rate)\n",
    "    \n",
    "    number_of_batches = math.ceil(len(train_image_addresses)/batch_size)\n",
    "    indexes = np.arange(len(train_image_addresses))\n",
    "    \n",
    "    loss_arr = []\n",
    "    for epoch in range(epochs):\n",
    "        avgloss = 0.0\n",
    "        start = time.time()\n",
    "        avg_loss_arr = []\n",
    "        for batch in range(number_of_batches):\n",
    "            train_indexes = [train_image_addresses[i] for i in indexes[batch*batch_size:(batch+1)*batch_size]]\n",
    "            image_batch, target_batch = load_batch(train_indexes)\n",
    "            loss = train_pass(image_batch, target_batch, model, optimizer, criterion)\n",
    "            avgloss += loss\n",
    "            if batch%50 == 0:\n",
    "                print (\"Done Batch:\", batch, \"\\tAverage Loss Per Batch:\", avgloss/(batch+1), \"\\t Current Batch Loss: \", loss)\n",
    "        loss_arr.append(avgloss/(batch+1))\n",
    "        print (\"Epoch:\",epoch, \"\\tTime:\", time.time() - start, \"\\tAverage Loss Per Batch::\", avgloss/(batch+1))\n",
    "        torch.save({'epoch': epoch ,'state_dict': decoder.state_dict(),'optimizer': decoder_optimizer.state_dict()}, open(\"outputs/anp_classifier_batch_\"+str(epoch), \"wb+\"))\n",
    "    loss_arr = np.array(loss_arr)\n",
    "    np.save(open('outputs/loss_anp_classifier', 'wb+'), loss_arr)\n",
    "\n",
    "train(main_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:amazonei_tensorflow_p27]",
   "language": "python",
   "name": "conda-env-amazonei_tensorflow_p27-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
