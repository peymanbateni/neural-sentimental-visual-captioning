{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Design 1 without pre-trained weights\n",
    "\n",
    "Here we experiment with our model being trained with basic initialization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "from IPython import display\n",
    "from nltk import word_tokenize\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from PIL import Image\n",
    "from torch import nn, optim\n",
    "from torch.autograd import Variable\n",
    "from torchvision import models, transforms\n",
    "from random import shuffle\n",
    "\n",
    "import json\n",
    "import matplotlib\n",
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepocessing\n",
    "\n",
    "The following code preprocesses the data, collecting the image array as needed and preparing for ANP classifying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train images:  336630\n",
      "Number of validation images:  72352\n",
      "Number of test images:  72340\n"
     ]
    }
   ],
   "source": [
    "vso_images_folder = \"data/vso/vso_images_with_cc/\"\n",
    "\n",
    "train_anp_tags = []\n",
    "train_image_addresses = []\n",
    "train_image_to_anp_tag = {}\n",
    "for subdir in os.listdir(vso_images_folder):\n",
    "    if subdir.endswith(\"_train\"):\n",
    "        train_anp_tags.append(subdir.replace(\"_train\", \"\").replace(\"_\", \" \"))\n",
    "        for filename in os.listdir(vso_images_folder + subdir):\n",
    "            if filename.endswith(\".jpg\"):\n",
    "                train_image_addresses.append(vso_images_folder + subdir + \"/\"  + filename)\n",
    "                train_image_to_anp_tag[vso_images_folder + subdir + \"/\"  + filename] = subdir.replace(\"_train\", \"\").replace(\"_\", \" \")\n",
    "\n",
    "validation_anp_tags = []\n",
    "validation_image_addresses = []\n",
    "validation_image_to_anp_tag = {}\n",
    "for subdir in os.listdir(vso_images_folder):\n",
    "        if subdir.endswith(\"_validation\"):\n",
    "                validation_anp_tags.append(subdir.replace(\"_validation\", \"\").replace(\"_\", \" \"))\n",
    "                for filename in os.listdir(vso_images_folder + subdir):\n",
    "                        if filename.endswith(\".jpg\"):\n",
    "                                validation_image_addresses.append(vso_images_folder + subdir + \"/\"  + filename)\n",
    "                                validation_image_to_anp_tag[vso_images_folder + subdir + \"/\"  + filename] = subdir.replace(\"_validation\", \"\").replace(\"_\", \" \")\n",
    "\n",
    "test_anp_tags = []\n",
    "test_image_addresses = []\n",
    "test_image_to_anp_tag = {}\n",
    "for subdir in os.listdir(vso_images_folder):\n",
    "        if subdir.endswith(\"_test\"):\n",
    "                test_anp_tags.append(subdir.replace(\"_test\", \"\").replace(\"_\", \" \"))\n",
    "                for filename in os.listdir(vso_images_folder + subdir):\n",
    "                        if filename.endswith(\".jpg\"):\n",
    "                                test_image_addresses.append(vso_images_folder + subdir + \"/\"  + filename)\n",
    "                                test_image_to_anp_tag[vso_images_folder + subdir + \"/\"  + filename] = subdir.replace(\"_test\", \"\").replace(\"_\", \" \")\n",
    "\n",
    "anp_tag_to_vector = {}\n",
    "for i, tag in enumerate(train_anp_tags):\n",
    "    anp_vector = np.zeros(len(train_anp_tags))\n",
    "    anp_vector[i] = 1\n",
    "    anp_tag_to_vector[tag] = anp_vector\n",
    "    \n",
    "# randomizing order of the examples\n",
    "shuffle(train_image_addresses)\n",
    "shuffle(validation_image_addresses)\n",
    "shuffle(test_image_addresses)\n",
    "                                \n",
    "print(\"Number of train images: \", len(train_image_to_anp_tag))\n",
    "print(\"Number of validation images: \", len(validation_image_to_anp_tag))\n",
    "print(\"Number of test images: \", len(test_image_to_anp_tag))\n",
    "    \n",
    "img_size = 224\n",
    "loader = transforms.Compose([\n",
    "  transforms.Resize(img_size),\n",
    "  transforms.CenterCrop(img_size),\n",
    "  transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "def load_image(filename, volatile=False):\n",
    "    \"\"\"\n",
    "    Simple function to load and preprocess the images.\n",
    "    \"\"\"\n",
    "    image = Image.open(filename).convert('RGB')\n",
    "    image_tensor = loader(image).float()\n",
    "    image_var = Variable(image_tensor, volatile=volatile).unsqueeze(0)\n",
    "    return image_var.cuda()\n",
    "\n",
    "#print(load_image('data/vso/vso_images_with_cc/amazing_flowers/1066918516_e27cbf795e.jpg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample Data Points\n",
    "\n",
    "Showing a few examples of the sample data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image:  data/vso/vso_images_with_cc/traditional_wedding_train/5400402131_b6b53c8dbb.jpg\n",
      "Associated ANP tag:  traditional wedding\n"
     ]
    }
   ],
   "source": [
    "display.display(display.Image(train_image_addresses[0]))\n",
    "print(\"Image: \", train_image_addresses[0])\n",
    "print(\"Associated ANP tag: \", train_image_to_anp_tag[train_image_addresses[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Defition\n",
    "\n",
    "The following define the model used for the ANP classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ANPClassifier(\n",
       "  (resnet): ResNet(\n",
       "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace)\n",
       "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (layer1): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "      )\n",
       "      (4): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "      )\n",
       "      (5): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "      )\n",
       "      (6): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "      )\n",
       "      (7): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "      )\n",
       "      (8): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "      )\n",
       "      (9): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "      )\n",
       "      (10): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "      )\n",
       "      (11): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "      )\n",
       "      (12): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "      )\n",
       "      (13): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "      )\n",
       "      (14): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "      )\n",
       "      (15): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "      )\n",
       "      (16): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "      )\n",
       "      (17): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "      )\n",
       "      (18): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "      )\n",
       "      (19): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "      )\n",
       "      (20): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "      )\n",
       "      (21): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "      )\n",
       "      (22): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "      )\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    (fc): Linear(in_features=2048, out_features=1553, bias=True)\n",
       "  )\n",
       "  (log_softmax): LogSoftmax()\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class ANPClassifier(nn.Module):\n",
    "    def __init__(self, output_size):\n",
    "        super(ANPClassifier, self).__init__()\n",
    "        \n",
    "        self.output_size = output_size\n",
    "        self.resnet = torchvision.models.resnet101(pretrained=False)\n",
    "        self.resnet.fc = nn.Linear(in_features=2048, out_features=output_size, bias=True)\n",
    "        self.log_softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.log_softmax(self.resnet(X))\n",
    "\n",
    "main_model = ANPClassifier(len(train_anp_tags))\n",
    "main_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "Following describes training procedure used to form the ANP classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done Batch: 0 \tAverage Loss Per Batch: 7.6471781730651855 \t Current Batch Loss:  7.647178\n",
      "Done Batch: 50 \tAverage Loss Per Batch: 7.2824063768573835 \t Current Batch Loss:  7.3618574\n",
      "Done Batch: 100 \tAverage Loss Per Batch: 7.173195565100944 \t Current Batch Loss:  7.035472\n",
      "Done Batch: 150 \tAverage Loss Per Batch: 7.111777684546464 \t Current Batch Loss:  7.079618\n",
      "Done Batch: 200 \tAverage Loss Per Batch: 7.067351770638234 \t Current Batch Loss:  7.0172553\n",
      "Done Batch: 250 \tAverage Loss Per Batch: 7.039838709204321 \t Current Batch Loss:  6.934287\n",
      "Done Batch: 300 \tAverage Loss Per Batch: 7.010447584513414 \t Current Batch Loss:  6.7635217\n",
      "Done Batch: 350 \tAverage Loss Per Batch: 6.991123708904299 \t Current Batch Loss:  6.769773\n",
      "Done Batch: 400 \tAverage Loss Per Batch: 6.973582628064619 \t Current Batch Loss:  6.969829\n",
      "Done Batch: 450 \tAverage Loss Per Batch: 6.952198602672162 \t Current Batch Loss:  6.747315\n",
      "Done Batch: 500 \tAverage Loss Per Batch: 6.9339071547913695 \t Current Batch Loss:  6.667378\n",
      "Done Batch: 550 \tAverage Loss Per Batch: 6.917975856691004 \t Current Batch Loss:  6.6890874\n",
      "Done Batch: 600 \tAverage Loss Per Batch: 6.903481684984661 \t Current Batch Loss:  6.7596126\n",
      "Done Batch: 650 \tAverage Loss Per Batch: 6.88955230610345 \t Current Batch Loss:  6.576742\n",
      "Done Batch: 700 \tAverage Loss Per Batch: 6.875516606465556 \t Current Batch Loss:  6.655823\n",
      "Done Batch: 750 \tAverage Loss Per Batch: 6.8625646259749775 \t Current Batch Loss:  6.940918\n",
      "Done Batch: 800 \tAverage Loss Per Batch: 6.852172795604082 \t Current Batch Loss:  6.7301683\n",
      "Done Batch: 850 \tAverage Loss Per Batch: 6.842907796315105 \t Current Batch Loss:  6.7461686\n",
      "Done Batch: 900 \tAverage Loss Per Batch: 6.830988293350338 \t Current Batch Loss:  6.6980004\n",
      "Done Batch: 950 \tAverage Loss Per Batch: 6.821681067269182 \t Current Batch Loss:  6.461323\n",
      "Done Batch: 1000 \tAverage Loss Per Batch: 6.811907265212509 \t Current Batch Loss:  6.7596393\n",
      "Done Batch: 1050 \tAverage Loss Per Batch: 6.801986450699825 \t Current Batch Loss:  6.5644426\n",
      "Done Batch: 1100 \tAverage Loss Per Batch: 6.7926886287415496 \t Current Batch Loss:  6.4773097\n",
      "Done Batch: 1150 \tAverage Loss Per Batch: 6.784243965231782 \t Current Batch Loss:  6.616527\n",
      "Done Batch: 1200 \tAverage Loss Per Batch: 6.775541606890371 \t Current Batch Loss:  6.6945276\n",
      "Done Batch: 1250 \tAverage Loss Per Batch: 6.7685059304241175 \t Current Batch Loss:  6.451107\n",
      "Done Batch: 1300 \tAverage Loss Per Batch: 6.760671549994243 \t Current Batch Loss:  6.459217\n",
      "Done Batch: 1350 \tAverage Loss Per Batch: 6.753617331153988 \t Current Batch Loss:  6.4818916\n",
      "Done Batch: 1400 \tAverage Loss Per Batch: 6.7469035811631874 \t Current Batch Loss:  6.562151\n",
      "Done Batch: 1450 \tAverage Loss Per Batch: 6.740265207402382 \t Current Batch Loss:  6.5729704\n",
      "Done Batch: 1500 \tAverage Loss Per Batch: 6.733401978675085 \t Current Batch Loss:  6.455375\n",
      "Done Batch: 1550 \tAverage Loss Per Batch: 6.726878394932996 \t Current Batch Loss:  6.6154985\n",
      "Done Batch: 1600 \tAverage Loss Per Batch: 6.71986659894058 \t Current Batch Loss:  6.32599\n",
      "Done Batch: 1650 \tAverage Loss Per Batch: 6.713282911796269 \t Current Batch Loss:  6.650007\n",
      "Done Batch: 1700 \tAverage Loss Per Batch: 6.706236646429081 \t Current Batch Loss:  6.4234014\n",
      "Done Batch: 1750 \tAverage Loss Per Batch: 6.700915133456105 \t Current Batch Loss:  6.790111\n",
      "Done Batch: 1800 \tAverage Loss Per Batch: 6.695296952355113 \t Current Batch Loss:  6.70101\n",
      "Done Batch: 1850 \tAverage Loss Per Batch: 6.68980352818032 \t Current Batch Loss:  6.5915523\n",
      "Done Batch: 1900 \tAverage Loss Per Batch: 6.683784457020606 \t Current Batch Loss:  6.5510955\n",
      "Done Batch: 1950 \tAverage Loss Per Batch: 6.677530009828428 \t Current Batch Loss:  6.3501854\n",
      "Done Batch: 2000 \tAverage Loss Per Batch: 6.671627926147323 \t Current Batch Loss:  6.353567\n",
      "Done Batch: 2050 \tAverage Loss Per Batch: 6.66573397721505 \t Current Batch Loss:  6.3763924\n",
      "Done Batch: 2100 \tAverage Loss Per Batch: 6.660761442824468 \t Current Batch Loss:  6.377356\n",
      "Done Batch: 2150 \tAverage Loss Per Batch: 6.655073654036475 \t Current Batch Loss:  6.591764\n",
      "Done Batch: 2200 \tAverage Loss Per Batch: 6.6500065940881194 \t Current Batch Loss:  6.621632\n",
      "Done Batch: 2250 \tAverage Loss Per Batch: 6.645242215897548 \t Current Batch Loss:  6.260437\n",
      "Done Batch: 2300 \tAverage Loss Per Batch: 6.640910683896737 \t Current Batch Loss:  6.43561\n",
      "Done Batch: 2350 \tAverage Loss Per Batch: 6.636441657919419 \t Current Batch Loss:  6.3838873\n",
      "Done Batch: 2400 \tAverage Loss Per Batch: 6.6327257130554145 \t Current Batch Loss:  6.317827\n",
      "Done Batch: 2450 \tAverage Loss Per Batch: 6.628741301891027 \t Current Batch Loss:  6.490174\n",
      "Done Batch: 2500 \tAverage Loss Per Batch: 6.624384243266195 \t Current Batch Loss:  6.710415\n",
      "Done Batch: 2550 \tAverage Loss Per Batch: 6.620177503194776 \t Current Batch Loss:  6.4207172\n",
      "Done Batch: 2600 \tAverage Loss Per Batch: 6.615542129478469 \t Current Batch Loss:  6.0733128\n",
      "Done Batch: 2650 \tAverage Loss Per Batch: 6.611538201716656 \t Current Batch Loss:  6.113682\n",
      "Done Batch: 2700 \tAverage Loss Per Batch: 6.607839132582245 \t Current Batch Loss:  6.4289875\n",
      "Done Batch: 2750 \tAverage Loss Per Batch: 6.603700889495883 \t Current Batch Loss:  6.3485737\n",
      "Done Batch: 2800 \tAverage Loss Per Batch: 6.599761578322563 \t Current Batch Loss:  6.145143\n",
      "Done Batch: 2850 \tAverage Loss Per Batch: 6.595359030375352 \t Current Batch Loss:  6.1921306\n",
      "Done Batch: 2900 \tAverage Loss Per Batch: 6.591890591171354 \t Current Batch Loss:  6.1740017\n",
      "Done Batch: 2950 \tAverage Loss Per Batch: 6.588127941085055 \t Current Batch Loss:  6.3432555\n",
      "Done Batch: 3000 \tAverage Loss Per Batch: 6.583870971016151 \t Current Batch Loss:  6.231003\n",
      "Done Batch: 3050 \tAverage Loss Per Batch: 6.580056427893425 \t Current Batch Loss:  6.2683997\n",
      "Done Batch: 3100 \tAverage Loss Per Batch: 6.5761667083671345 \t Current Batch Loss:  6.422716\n",
      "Done Batch: 3150 \tAverage Loss Per Batch: 6.572663934296632 \t Current Batch Loss:  6.389165\n",
      "Done Batch: 3200 \tAverage Loss Per Batch: 6.5691786914719374 \t Current Batch Loss:  6.178778\n",
      "Done Batch: 3250 \tAverage Loss Per Batch: 6.565536595534926 \t Current Batch Loss:  6.1010547\n",
      "Done Batch: 3300 \tAverage Loss Per Batch: 6.562248134930832 \t Current Batch Loss:  6.2992496\n",
      "Done Batch: 3350 \tAverage Loss Per Batch: 6.55848748304708 \t Current Batch Loss:  6.189756\n",
      "Epoch: 0 \tTime: 4785.327814817429 \tAverage Loss Per Batch:: 6.557256993321237\n",
      "Done Batch: 0 \tAverage Loss Per Batch: 6.410312652587891 \t Current Batch Loss:  6.4103127\n",
      "Done Batch: 50 \tAverage Loss Per Batch: 6.317742357067034 \t Current Batch Loss:  6.5472693\n",
      "Done Batch: 100 \tAverage Loss Per Batch: 6.331513367076911 \t Current Batch Loss:  6.420727\n",
      "Done Batch: 150 \tAverage Loss Per Batch: 6.321075360506575 \t Current Batch Loss:  6.1598263\n",
      "Done Batch: 200 \tAverage Loss Per Batch: 6.318085257686786 \t Current Batch Loss:  6.368044\n",
      "Done Batch: 250 \tAverage Loss Per Batch: 6.318755461400249 \t Current Batch Loss:  6.3687983\n",
      "Done Batch: 300 \tAverage Loss Per Batch: 6.3119298263245645 \t Current Batch Loss:  6.3372693\n",
      "Done Batch: 350 \tAverage Loss Per Batch: 6.310506736451065 \t Current Batch Loss:  6.263376\n",
      "Done Batch: 400 \tAverage Loss Per Batch: 6.310241360319523 \t Current Batch Loss:  6.510004\n",
      "Done Batch: 450 \tAverage Loss Per Batch: 6.304355488119527 \t Current Batch Loss:  6.2929554\n",
      "Done Batch: 500 \tAverage Loss Per Batch: 6.300721546371064 \t Current Batch Loss:  6.2131176\n",
      "Done Batch: 550 \tAverage Loss Per Batch: 6.298002142655222 \t Current Batch Loss:  5.986912\n",
      "Done Batch: 600 \tAverage Loss Per Batch: 6.296936469942877 \t Current Batch Loss:  6.1914043\n",
      "Done Batch: 650 \tAverage Loss Per Batch: 6.296498013714674 \t Current Batch Loss:  6.194788\n",
      "Done Batch: 700 \tAverage Loss Per Batch: 6.294852388738396 \t Current Batch Loss:  6.2108674\n",
      "Done Batch: 750 \tAverage Loss Per Batch: 6.291959965117921 \t Current Batch Loss:  6.524148\n",
      "Done Batch: 800 \tAverage Loss Per Batch: 6.291182647185974 \t Current Batch Loss:  6.28766\n",
      "Done Batch: 850 \tAverage Loss Per Batch: 6.290639629655383 \t Current Batch Loss:  6.30496\n",
      "Done Batch: 900 \tAverage Loss Per Batch: 6.286586659862252 \t Current Batch Loss:  6.2629657\n",
      "Done Batch: 950 \tAverage Loss Per Batch: 6.285802153758822 \t Current Batch Loss:  6.131186\n",
      "Done Batch: 1000 \tAverage Loss Per Batch: 6.283866442643203 \t Current Batch Loss:  6.301062\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done Batch: 1050 \tAverage Loss Per Batch: 6.281436174285627 \t Current Batch Loss:  6.144527\n",
      "Done Batch: 1100 \tAverage Loss Per Batch: 6.278715153589344 \t Current Batch Loss:  6.046143\n",
      "Done Batch: 1150 \tAverage Loss Per Batch: 6.2776065361593 \t Current Batch Loss:  6.2965627\n",
      "Done Batch: 1200 \tAverage Loss Per Batch: 6.275304093547507 \t Current Batch Loss:  6.3214307\n",
      "Done Batch: 1250 \tAverage Loss Per Batch: 6.2735538520782494 \t Current Batch Loss:  6.121847\n",
      "Done Batch: 1300 \tAverage Loss Per Batch: 6.271409780222301 \t Current Batch Loss:  6.1027055\n",
      "Done Batch: 1350 \tAverage Loss Per Batch: 6.269823248698745 \t Current Batch Loss:  5.9896355\n",
      "Done Batch: 1400 \tAverage Loss Per Batch: 6.2681535238202 \t Current Batch Loss:  6.2343726\n",
      "Done Batch: 1450 \tAverage Loss Per Batch: 6.266797699655523 \t Current Batch Loss:  6.106239\n",
      "Done Batch: 1500 \tAverage Loss Per Batch: 6.264650282265741 \t Current Batch Loss:  6.057255\n",
      "Done Batch: 1550 \tAverage Loss Per Batch: 6.261775876690233 \t Current Batch Loss:  6.230685\n",
      "Done Batch: 1600 \tAverage Loss Per Batch: 6.259107382129238 \t Current Batch Loss:  6.1149473\n",
      "Done Batch: 1650 \tAverage Loss Per Batch: 6.2567426524402014 \t Current Batch Loss:  6.3581276\n",
      "Done Batch: 1700 \tAverage Loss Per Batch: 6.25350517977132 \t Current Batch Loss:  6.198484\n",
      "Done Batch: 1750 \tAverage Loss Per Batch: 6.252061364311684 \t Current Batch Loss:  6.4041224\n",
      "Done Batch: 1800 \tAverage Loss Per Batch: 6.250658744841665 \t Current Batch Loss:  6.348855\n",
      "Done Batch: 1850 \tAverage Loss Per Batch: 6.24920497360518 \t Current Batch Loss:  6.2906604\n",
      "Done Batch: 1900 \tAverage Loss Per Batch: 6.246354998318162 \t Current Batch Loss:  6.164098\n",
      "Done Batch: 1950 \tAverage Loss Per Batch: 6.243830926231333 \t Current Batch Loss:  6.2328825\n",
      "Done Batch: 2000 \tAverage Loss Per Batch: 6.241356784376366 \t Current Batch Loss:  6.108687\n",
      "Done Batch: 2050 \tAverage Loss Per Batch: 6.238644175155171 \t Current Batch Loss:  6.157891\n",
      "Done Batch: 2100 \tAverage Loss Per Batch: 6.236665250913465 \t Current Batch Loss:  6.188729\n",
      "Done Batch: 2150 \tAverage Loss Per Batch: 6.234282183902089 \t Current Batch Loss:  6.374577\n",
      "Done Batch: 2200 \tAverage Loss Per Batch: 6.232757501199212 \t Current Batch Loss:  6.5130434\n",
      "Done Batch: 2250 \tAverage Loss Per Batch: 6.234143536337637 \t Current Batch Loss:  6.1481023\n",
      "Done Batch: 2300 \tAverage Loss Per Batch: 6.234310923115889 \t Current Batch Loss:  6.2598443\n",
      "Done Batch: 2350 \tAverage Loss Per Batch: 6.233285731328188 \t Current Batch Loss:  6.154051\n",
      "Done Batch: 2400 \tAverage Loss Per Batch: 6.233097782834079 \t Current Batch Loss:  6.1095366\n",
      "Done Batch: 2450 \tAverage Loss Per Batch: 6.233112292793614 \t Current Batch Loss:  6.309758\n",
      "Done Batch: 2500 \tAverage Loss Per Batch: 6.232382321729511 \t Current Batch Loss:  6.542637\n",
      "Done Batch: 2550 \tAverage Loss Per Batch: 6.231361617391505 \t Current Batch Loss:  6.153731\n",
      "Done Batch: 2600 \tAverage Loss Per Batch: 6.229759913506484 \t Current Batch Loss:  5.850492\n",
      "Done Batch: 2650 \tAverage Loss Per Batch: 6.228652135150381 \t Current Batch Loss:  5.8658986\n",
      "Done Batch: 2700 \tAverage Loss Per Batch: 6.227457031326265 \t Current Batch Loss:  6.190305\n",
      "Done Batch: 2750 \tAverage Loss Per Batch: 6.22574900488817 \t Current Batch Loss:  6.13617\n",
      "Done Batch: 2800 \tAverage Loss Per Batch: 6.224208291961823 \t Current Batch Loss:  5.938786\n",
      "Done Batch: 2850 \tAverage Loss Per Batch: 6.222240357347139 \t Current Batch Loss:  5.916315\n",
      "Done Batch: 2900 \tAverage Loss Per Batch: 6.220993754207081 \t Current Batch Loss:  5.883432\n",
      "Done Batch: 2950 \tAverage Loss Per Batch: 6.219396996441553 \t Current Batch Loss:  6.104885\n",
      "Done Batch: 3000 \tAverage Loss Per Batch: 6.217296162910042 \t Current Batch Loss:  6.000562\n",
      "Done Batch: 3050 \tAverage Loss Per Batch: 6.215682424978521 \t Current Batch Loss:  6.0137305\n",
      "Done Batch: 3100 \tAverage Loss Per Batch: 6.213628960670636 \t Current Batch Loss:  6.2804236\n",
      "Done Batch: 3150 \tAverage Loss Per Batch: 6.212065152991806 \t Current Batch Loss:  6.106043\n",
      "Done Batch: 3200 \tAverage Loss Per Batch: 6.210320295895759 \t Current Batch Loss:  5.941596\n",
      "Done Batch: 3250 \tAverage Loss Per Batch: 6.208686027333246 \t Current Batch Loss:  5.8873663\n",
      "Done Batch: 3300 \tAverage Loss Per Batch: 6.20724270886199 \t Current Batch Loss:  6.0928845\n",
      "Done Batch: 3350 \tAverage Loss Per Batch: 6.205325493916438 \t Current Batch Loss:  5.9380426\n",
      "Epoch: 1 \tTime: 4481.933912992477 \tAverage Loss Per Batch:: 6.204628426064927\n",
      "Done Batch: 0 \tAverage Loss Per Batch: 6.277066230773926 \t Current Batch Loss:  6.277066\n",
      "Done Batch: 50 \tAverage Loss Per Batch: 6.09517173206105 \t Current Batch Loss:  6.3931837\n",
      "Done Batch: 100 \tAverage Loss Per Batch: 6.112633653206401 \t Current Batch Loss:  6.1983795\n",
      "Done Batch: 150 \tAverage Loss Per Batch: 6.100242052646662 \t Current Batch Loss:  6.0094256\n",
      "Done Batch: 200 \tAverage Loss Per Batch: 6.099232332030339 \t Current Batch Loss:  6.174188\n",
      "Done Batch: 250 \tAverage Loss Per Batch: 6.100833932717008 \t Current Batch Loss:  6.184318\n",
      "Done Batch: 300 \tAverage Loss Per Batch: 6.092393228778016 \t Current Batch Loss:  6.0667424\n",
      "Done Batch: 350 \tAverage Loss Per Batch: 6.094409829870588 \t Current Batch Loss:  5.931996\n",
      "Done Batch: 400 \tAverage Loss Per Batch: 6.095135731590061 \t Current Batch Loss:  6.2103558\n",
      "Done Batch: 450 \tAverage Loss Per Batch: 6.088399835277819 \t Current Batch Loss:  6.091393\n",
      "Done Batch: 500 \tAverage Loss Per Batch: 6.083570143419825 \t Current Batch Loss:  5.957243\n",
      "Done Batch: 550 \tAverage Loss Per Batch: 6.079107775229508 \t Current Batch Loss:  5.8675694\n",
      "Done Batch: 600 \tAverage Loss Per Batch: 6.077322450533088 \t Current Batch Loss:  5.9269276\n",
      "Done Batch: 650 \tAverage Loss Per Batch: 6.076228165223668 \t Current Batch Loss:  6.017042\n",
      "Done Batch: 700 \tAverage Loss Per Batch: 6.074904676510842 \t Current Batch Loss:  5.9622984\n",
      "Done Batch: 750 \tAverage Loss Per Batch: 6.073393175986095 \t Current Batch Loss:  6.400581\n",
      "Done Batch: 800 \tAverage Loss Per Batch: 6.073243033424596 \t Current Batch Loss:  6.05752\n",
      "Done Batch: 850 \tAverage Loss Per Batch: 6.072859101233836 \t Current Batch Loss:  6.075257\n",
      "Done Batch: 900 \tAverage Loss Per Batch: 6.07046312285581 \t Current Batch Loss:  5.958606\n",
      "Done Batch: 950 \tAverage Loss Per Batch: 6.06978998174176 \t Current Batch Loss:  5.9009705\n",
      "Done Batch: 1000 \tAverage Loss Per Batch: 6.066830343537993 \t Current Batch Loss:  6.1361213\n",
      "Done Batch: 1050 \tAverage Loss Per Batch: 6.064905374419904 \t Current Batch Loss:  5.9074426\n",
      "Done Batch: 1100 \tAverage Loss Per Batch: 6.062294590592276 \t Current Batch Loss:  5.8628674\n",
      "Done Batch: 1150 \tAverage Loss Per Batch: 6.061971835526666 \t Current Batch Loss:  6.0882554\n",
      "Done Batch: 1200 \tAverage Loss Per Batch: 6.060107388762411 \t Current Batch Loss:  6.0869026\n",
      "Done Batch: 1250 \tAverage Loss Per Batch: 6.05926263970818 \t Current Batch Loss:  5.8936267\n",
      "Done Batch: 1300 \tAverage Loss Per Batch: 6.057305568736118 \t Current Batch Loss:  5.8951087\n",
      "Done Batch: 1350 \tAverage Loss Per Batch: 6.056190877557242 \t Current Batch Loss:  5.751874\n",
      "Done Batch: 1400 \tAverage Loss Per Batch: 6.0551092599137695 \t Current Batch Loss:  6.0328126\n",
      "Done Batch: 1450 \tAverage Loss Per Batch: 6.054364385808607 \t Current Batch Loss:  5.8753395\n",
      "Done Batch: 1500 \tAverage Loss Per Batch: 6.052978089934266 \t Current Batch Loss:  5.874618\n",
      "Done Batch: 1550 \tAverage Loss Per Batch: 6.050539418238351 \t Current Batch Loss:  6.0576196\n",
      "Done Batch: 1600 \tAverage Loss Per Batch: 6.048111512912652 \t Current Batch Loss:  5.944679\n",
      "Done Batch: 1650 \tAverage Loss Per Batch: 6.046052788619487 \t Current Batch Loss:  6.109707\n",
      "Done Batch: 1700 \tAverage Loss Per Batch: 6.043387790906436 \t Current Batch Loss:  6.085017\n",
      "Done Batch: 1750 \tAverage Loss Per Batch: 6.042325227074048 \t Current Batch Loss:  6.111587\n",
      "Done Batch: 1800 \tAverage Loss Per Batch: 6.041328890332905 \t Current Batch Loss:  6.0784254\n",
      "Done Batch: 1850 \tAverage Loss Per Batch: 6.040508810215419 \t Current Batch Loss:  6.1416874\n",
      "Done Batch: 1900 \tAverage Loss Per Batch: 6.037614219129744 \t Current Batch Loss:  5.973963\n",
      "Done Batch: 1950 \tAverage Loss Per Batch: 6.0350258407807855 \t Current Batch Loss:  6.0709753\n",
      "Done Batch: 2000 \tAverage Loss Per Batch: 6.033070149629013 \t Current Batch Loss:  5.8805237\n",
      "Done Batch: 2050 \tAverage Loss Per Batch: 6.031139572093337 \t Current Batch Loss:  5.9774623\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done Batch: 2100 \tAverage Loss Per Batch: 6.029427861327616 \t Current Batch Loss:  6.003984\n",
      "Done Batch: 2150 \tAverage Loss Per Batch: 6.0276275077901404 \t Current Batch Loss:  6.195223\n",
      "Done Batch: 2200 \tAverage Loss Per Batch: 6.026201599784896 \t Current Batch Loss:  6.202009\n",
      "Done Batch: 2250 \tAverage Loss Per Batch: 6.025190878316912 \t Current Batch Loss:  5.7468195\n",
      "Done Batch: 2300 \tAverage Loss Per Batch: 6.024236372168921 \t Current Batch Loss:  6.0411515\n",
      "Done Batch: 2350 \tAverage Loss Per Batch: 6.022455647872996 \t Current Batch Loss:  5.8472114\n",
      "Done Batch: 2400 \tAverage Loss Per Batch: 6.02219482681643 \t Current Batch Loss:  5.711481\n",
      "Done Batch: 2450 \tAverage Loss Per Batch: 6.021558161805183 \t Current Batch Loss:  6.008732\n",
      "Done Batch: 2500 \tAverage Loss Per Batch: 6.020369179484273 \t Current Batch Loss:  6.4107466\n",
      "Done Batch: 2550 \tAverage Loss Per Batch: 6.019344134500848 \t Current Batch Loss:  6.0176005\n",
      "Done Batch: 2600 \tAverage Loss Per Batch: 6.017780049678593 \t Current Batch Loss:  5.6483474\n",
      "Done Batch: 2650 \tAverage Loss Per Batch: 6.016967935141237 \t Current Batch Loss:  5.721045\n",
      "Done Batch: 2700 \tAverage Loss Per Batch: 6.01597392916547 \t Current Batch Loss:  5.985945\n",
      "Done Batch: 2750 \tAverage Loss Per Batch: 6.014592881551096 \t Current Batch Loss:  5.833283\n",
      "Done Batch: 2800 \tAverage Loss Per Batch: 6.013342608983327 \t Current Batch Loss:  5.739037\n",
      "Done Batch: 2850 \tAverage Loss Per Batch: 6.0117796219177135 \t Current Batch Loss:  5.7676167\n",
      "Done Batch: 2900 \tAverage Loss Per Batch: 6.010954191338724 \t Current Batch Loss:  5.653117\n",
      "Done Batch: 2950 \tAverage Loss Per Batch: 6.009808731983977 \t Current Batch Loss:  5.843824\n",
      "Done Batch: 3000 \tAverage Loss Per Batch: 6.008026975665399 \t Current Batch Loss:  5.8198957\n",
      "Done Batch: 3050 \tAverage Loss Per Batch: 6.006701886399931 \t Current Batch Loss:  5.832068\n",
      "Done Batch: 3100 \tAverage Loss Per Batch: 6.00501833712428 \t Current Batch Loss:  6.1535516\n",
      "Done Batch: 3150 \tAverage Loss Per Batch: 6.00383440103655 \t Current Batch Loss:  5.894571\n",
      "Done Batch: 3200 \tAverage Loss Per Batch: 6.00252474162475 \t Current Batch Loss:  5.882681\n",
      "Done Batch: 3250 \tAverage Loss Per Batch: 6.001609526939005 \t Current Batch Loss:  5.677148\n",
      "Done Batch: 3300 \tAverage Loss Per Batch: 6.0006144022948815 \t Current Batch Loss:  6.0762925\n",
      "Done Batch: 3350 \tAverage Loss Per Batch: 5.9991891786113705 \t Current Batch Loss:  5.7917285\n",
      "Epoch: 2 \tTime: 4403.188914775848 \tAverage Loss Per Batch:: 5.998492488600502\n",
      "Done Batch: 0 \tAverage Loss Per Batch: 6.097047328948975 \t Current Batch Loss:  6.0970473\n",
      "Done Batch: 50 \tAverage Loss Per Batch: 5.916318201551251 \t Current Batch Loss:  6.203905\n",
      "Done Batch: 100 \tAverage Loss Per Batch: 5.930362158482618 \t Current Batch Loss:  6.022977\n",
      "Done Batch: 150 \tAverage Loss Per Batch: 5.918577762629023 \t Current Batch Loss:  5.8592544\n",
      "Done Batch: 200 \tAverage Loss Per Batch: 5.91618775135249 \t Current Batch Loss:  6.029355\n",
      "Done Batch: 250 \tAverage Loss Per Batch: 5.919582059183918 \t Current Batch Loss:  6.0409784\n",
      "Done Batch: 300 \tAverage Loss Per Batch: 5.912721446978294 \t Current Batch Loss:  5.8525286\n",
      "Done Batch: 350 \tAverage Loss Per Batch: 5.909736540922072 \t Current Batch Loss:  5.6217976\n",
      "Done Batch: 400 \tAverage Loss Per Batch: 5.909777816097041 \t Current Batch Loss:  6.0196447\n",
      "Done Batch: 450 \tAverage Loss Per Batch: 5.902833951286096 \t Current Batch Loss:  5.962561\n",
      "Done Batch: 500 \tAverage Loss Per Batch: 5.8990367053749555 \t Current Batch Loss:  5.777813\n",
      "Done Batch: 550 \tAverage Loss Per Batch: 5.8954567182302045 \t Current Batch Loss:  5.715276\n",
      "Done Batch: 600 \tAverage Loss Per Batch: 5.895046378530798 \t Current Batch Loss:  5.6865034\n",
      "Done Batch: 650 \tAverage Loss Per Batch: 5.894648612003356 \t Current Batch Loss:  5.8000374\n",
      "Done Batch: 700 \tAverage Loss Per Batch: 5.894307835125889 \t Current Batch Loss:  5.791811\n",
      "Done Batch: 750 \tAverage Loss Per Batch: 5.8933896326352055 \t Current Batch Loss:  6.2624836\n",
      "Done Batch: 800 \tAverage Loss Per Batch: 5.893653030847938 \t Current Batch Loss:  5.8985195\n",
      "Done Batch: 850 \tAverage Loss Per Batch: 5.893640064324671 \t Current Batch Loss:  5.8619237\n",
      "Done Batch: 900 \tAverage Loss Per Batch: 5.892982116682283 \t Current Batch Loss:  5.8086634\n",
      "Done Batch: 950 \tAverage Loss Per Batch: 5.893630674584807 \t Current Batch Loss:  5.753579\n",
      "Done Batch: 1000 \tAverage Loss Per Batch: 5.8915907124301174 \t Current Batch Loss:  5.8996983\n",
      "Done Batch: 1050 \tAverage Loss Per Batch: 5.88996830993783 \t Current Batch Loss:  5.756694\n",
      "Done Batch: 1100 \tAverage Loss Per Batch: 5.887308440784451 \t Current Batch Loss:  5.745499\n",
      "Done Batch: 1150 \tAverage Loss Per Batch: 5.8875745943999105 \t Current Batch Loss:  5.8930535\n",
      "Done Batch: 1200 \tAverage Loss Per Batch: 5.885896996395673 \t Current Batch Loss:  5.8923955\n",
      "Done Batch: 1250 \tAverage Loss Per Batch: 5.885051808864188 \t Current Batch Loss:  5.6936994\n",
      "Done Batch: 1300 \tAverage Loss Per Batch: 5.883591269273927 \t Current Batch Loss:  5.7077885\n",
      "Done Batch: 1350 \tAverage Loss Per Batch: 5.883316482639948 \t Current Batch Loss:  5.6041937\n",
      "Done Batch: 1400 \tAverage Loss Per Batch: 5.882134533541105 \t Current Batch Loss:  5.8271427\n",
      "Done Batch: 1450 \tAverage Loss Per Batch: 5.881464679186957 \t Current Batch Loss:  5.686211\n",
      "Done Batch: 1500 \tAverage Loss Per Batch: 5.8801647185643935 \t Current Batch Loss:  5.634121\n",
      "Done Batch: 1550 \tAverage Loss Per Batch: 5.877891814147635 \t Current Batch Loss:  5.914198\n",
      "Done Batch: 1600 \tAverage Loss Per Batch: 5.875753840232625 \t Current Batch Loss:  5.817151\n",
      "Done Batch: 1650 \tAverage Loss Per Batch: 5.874023856573723 \t Current Batch Loss:  5.912957\n",
      "Done Batch: 1700 \tAverage Loss Per Batch: 5.871948514665876 \t Current Batch Loss:  6.0118313\n",
      "Done Batch: 1750 \tAverage Loss Per Batch: 5.871200984985334 \t Current Batch Loss:  5.9144382\n",
      "Done Batch: 1800 \tAverage Loss Per Batch: 5.870529015417698 \t Current Batch Loss:  5.863614\n",
      "Done Batch: 1850 \tAverage Loss Per Batch: 5.870077517018455 \t Current Batch Loss:  6.0430427\n",
      "Done Batch: 1900 \tAverage Loss Per Batch: 5.867720034296798 \t Current Batch Loss:  5.7814226\n",
      "Done Batch: 1950 \tAverage Loss Per Batch: 5.865219971633825 \t Current Batch Loss:  5.8566113\n",
      "Done Batch: 2000 \tAverage Loss Per Batch: 5.86352738256993 \t Current Batch Loss:  5.723186\n",
      "Done Batch: 2050 \tAverage Loss Per Batch: 5.861950012720369 \t Current Batch Loss:  5.7515187\n",
      "Done Batch: 2100 \tAverage Loss Per Batch: 5.860292180046361 \t Current Batch Loss:  5.756814\n",
      "Done Batch: 2150 \tAverage Loss Per Batch: 5.858589997018896 \t Current Batch Loss:  6.003553\n",
      "Done Batch: 2200 \tAverage Loss Per Batch: 5.857536526930868 \t Current Batch Loss:  6.1059\n",
      "Done Batch: 2250 \tAverage Loss Per Batch: 5.856647784208945 \t Current Batch Loss:  5.608486\n",
      "Done Batch: 2300 \tAverage Loss Per Batch: 5.855803015749126 \t Current Batch Loss:  5.931194\n",
      "Done Batch: 2350 \tAverage Loss Per Batch: 5.853902084175346 \t Current Batch Loss:  5.7274537\n",
      "Done Batch: 2400 \tAverage Loss Per Batch: 5.853852641825774 \t Current Batch Loss:  5.5782733\n",
      "Done Batch: 2450 \tAverage Loss Per Batch: 5.853368407996126 \t Current Batch Loss:  5.873742\n",
      "Done Batch: 2500 \tAverage Loss Per Batch: 5.85259007835617 \t Current Batch Loss:  6.17896\n",
      "Done Batch: 2550 \tAverage Loss Per Batch: 5.8515867231781 \t Current Batch Loss:  5.8702345\n",
      "Done Batch: 2600 \tAverage Loss Per Batch: 5.8503249198461855 \t Current Batch Loss:  5.4877343\n",
      "Done Batch: 2650 \tAverage Loss Per Batch: 5.849832283870448 \t Current Batch Loss:  5.5600166\n",
      "Done Batch: 2700 \tAverage Loss Per Batch: 5.849083063824007 \t Current Batch Loss:  5.8073583\n",
      "Done Batch: 2750 \tAverage Loss Per Batch: 5.84798093797163 \t Current Batch Loss:  5.670721\n",
      "Done Batch: 2800 \tAverage Loss Per Batch: 5.847079455788669 \t Current Batch Loss:  5.622836\n",
      "Done Batch: 2850 \tAverage Loss Per Batch: 5.845849847174745 \t Current Batch Loss:  5.604697\n",
      "Done Batch: 2900 \tAverage Loss Per Batch: 5.845435203005223 \t Current Batch Loss:  5.5754333\n",
      "Done Batch: 2950 \tAverage Loss Per Batch: 5.844354828912418 \t Current Batch Loss:  5.7159557\n",
      "Done Batch: 3000 \tAverage Loss Per Batch: 5.8427032412230595 \t Current Batch Loss:  5.6572666\n",
      "Done Batch: 3050 \tAverage Loss Per Batch: 5.8415243354479784 \t Current Batch Loss:  5.7258687\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done Batch: 3100 \tAverage Loss Per Batch: 5.839994774676645 \t Current Batch Loss:  6.018997\n",
      "Done Batch: 3150 \tAverage Loss Per Batch: 5.839056444485881 \t Current Batch Loss:  5.6961446\n",
      "Done Batch: 3200 \tAverage Loss Per Batch: 5.837871516059987 \t Current Batch Loss:  5.7245784\n",
      "Done Batch: 3250 \tAverage Loss Per Batch: 5.8370295252956925 \t Current Batch Loss:  5.507179\n",
      "Done Batch: 3300 \tAverage Loss Per Batch: 5.8362800210434465 \t Current Batch Loss:  5.859494\n",
      "Done Batch: 3350 \tAverage Loss Per Batch: 5.834877953462478 \t Current Batch Loss:  5.584882\n",
      "Epoch: 3 \tTime: 4398.199915647507 \tAverage Loss Per Batch:: 5.834192319903417\n",
      "Done Batch: 0 \tAverage Loss Per Batch: 5.916414260864258 \t Current Batch Loss:  5.9164143\n",
      "Done Batch: 50 \tAverage Loss Per Batch: 5.76361515007767 \t Current Batch Loss:  6.001273\n",
      "Done Batch: 100 \tAverage Loss Per Batch: 5.775376673972253 \t Current Batch Loss:  5.8668504\n",
      "Done Batch: 150 \tAverage Loss Per Batch: 5.761416700502105 \t Current Batch Loss:  5.6983857\n",
      "Done Batch: 200 \tAverage Loss Per Batch: 5.7598819922451945 \t Current Batch Loss:  5.827566\n",
      "Done Batch: 250 \tAverage Loss Per Batch: 5.763716881968586 \t Current Batch Loss:  5.825967\n",
      "Done Batch: 300 \tAverage Loss Per Batch: 5.757967123557563 \t Current Batch Loss:  5.7312145\n",
      "Done Batch: 350 \tAverage Loss Per Batch: 5.755324255027662 \t Current Batch Loss:  5.4144993\n",
      "Done Batch: 400 \tAverage Loss Per Batch: 5.754903503189658 \t Current Batch Loss:  5.739628\n",
      "Done Batch: 450 \tAverage Loss Per Batch: 5.747577405028755 \t Current Batch Loss:  5.746948\n",
      "Done Batch: 500 \tAverage Loss Per Batch: 5.743550839300403 \t Current Batch Loss:  5.6203833\n",
      "Done Batch: 550 \tAverage Loss Per Batch: 5.7385523652424615 \t Current Batch Loss:  5.515962\n",
      "Done Batch: 600 \tAverage Loss Per Batch: 5.737185088647979 \t Current Batch Loss:  5.5414076\n",
      "Done Batch: 650 \tAverage Loss Per Batch: 5.736657523522911 \t Current Batch Loss:  5.6387353\n",
      "Done Batch: 700 \tAverage Loss Per Batch: 5.736297789041734 \t Current Batch Loss:  5.6866407\n",
      "Done Batch: 750 \tAverage Loss Per Batch: 5.735787344677312 \t Current Batch Loss:  6.0917554\n",
      "Done Batch: 800 \tAverage Loss Per Batch: 5.737429433696428 \t Current Batch Loss:  5.7939916\n",
      "Done Batch: 850 \tAverage Loss Per Batch: 5.737937995045502 \t Current Batch Loss:  5.700943\n",
      "Done Batch: 900 \tAverage Loss Per Batch: 5.739042502793832 \t Current Batch Loss:  5.642997\n",
      "Done Batch: 950 \tAverage Loss Per Batch: 5.7407457851085 \t Current Batch Loss:  5.693398\n",
      "Done Batch: 1000 \tAverage Loss Per Batch: 5.738887077087647 \t Current Batch Loss:  5.6539783\n",
      "Done Batch: 1050 \tAverage Loss Per Batch: 5.738075580288409 \t Current Batch Loss:  5.649944\n",
      "Done Batch: 1100 \tAverage Loss Per Batch: 5.736173938557195 \t Current Batch Loss:  5.601092\n",
      "Done Batch: 1150 \tAverage Loss Per Batch: 5.736750995874612 \t Current Batch Loss:  5.7793508\n",
      "Done Batch: 1200 \tAverage Loss Per Batch: 5.735739669434534 \t Current Batch Loss:  5.7640734\n",
      "Done Batch: 1250 \tAverage Loss Per Batch: 5.7356294545052435 \t Current Batch Loss:  5.556287\n",
      "Done Batch: 1300 \tAverage Loss Per Batch: 5.734014312456792 \t Current Batch Loss:  5.540852\n",
      "Done Batch: 1350 \tAverage Loss Per Batch: 5.734165766255049 \t Current Batch Loss:  5.409167\n",
      "Done Batch: 1400 \tAverage Loss Per Batch: 5.732765397201854 \t Current Batch Loss:  5.653103\n",
      "Done Batch: 1450 \tAverage Loss Per Batch: 5.73237016790904 \t Current Batch Loss:  5.5066905\n",
      "Done Batch: 1500 \tAverage Loss Per Batch: 5.731126956189973 \t Current Batch Loss:  5.4662886\n",
      "Done Batch: 1550 \tAverage Loss Per Batch: 5.729434407195455 \t Current Batch Loss:  5.8104115\n",
      "Done Batch: 1600 \tAverage Loss Per Batch: 5.728278696201951 \t Current Batch Loss:  5.673137\n",
      "Done Batch: 1650 \tAverage Loss Per Batch: 5.726840093163271 \t Current Batch Loss:  5.8219194\n",
      "Done Batch: 1700 \tAverage Loss Per Batch: 5.725605237490987 \t Current Batch Loss:  5.879984\n",
      "Done Batch: 1750 \tAverage Loss Per Batch: 5.726310235714654 \t Current Batch Loss:  5.7792444\n",
      "Done Batch: 1800 \tAverage Loss Per Batch: 5.726330728282007 \t Current Batch Loss:  5.706261\n",
      "Done Batch: 1850 \tAverage Loss Per Batch: 5.726384230396801 \t Current Batch Loss:  5.925022\n",
      "Done Batch: 1900 \tAverage Loss Per Batch: 5.724199468120031 \t Current Batch Loss:  5.6354628\n",
      "Done Batch: 1950 \tAverage Loss Per Batch: 5.721705653983957 \t Current Batch Loss:  5.7444263\n",
      "Done Batch: 2000 \tAverage Loss Per Batch: 5.719908138563012 \t Current Batch Loss:  5.569043\n",
      "Done Batch: 2050 \tAverage Loss Per Batch: 5.718471588709481 \t Current Batch Loss:  5.5187635\n",
      "Done Batch: 2100 \tAverage Loss Per Batch: 5.7165127765105375 \t Current Batch Loss:  5.542725\n",
      "Done Batch: 2150 \tAverage Loss Per Batch: 5.714748685828812 \t Current Batch Loss:  5.876033\n",
      "Done Batch: 2200 \tAverage Loss Per Batch: 5.71392859290373 \t Current Batch Loss:  6.0275636\n",
      "Done Batch: 2250 \tAverage Loss Per Batch: 5.712976273511686 \t Current Batch Loss:  5.426409\n",
      "Done Batch: 2300 \tAverage Loss Per Batch: 5.71209544194672 \t Current Batch Loss:  5.7733746\n",
      "Done Batch: 2350 \tAverage Loss Per Batch: 5.71005130595017 \t Current Batch Loss:  5.590609\n",
      "Done Batch: 2400 \tAverage Loss Per Batch: 5.7099560023049225 \t Current Batch Loss:  5.489984\n",
      "Done Batch: 2450 \tAverage Loss Per Batch: 5.709213606049605 \t Current Batch Loss:  5.7308874\n",
      "Done Batch: 2500 \tAverage Loss Per Batch: 5.708349644112997 \t Current Batch Loss:  6.0390544\n",
      "Done Batch: 2550 \tAverage Loss Per Batch: 5.707250118255615 \t Current Batch Loss:  5.686818\n",
      "Done Batch: 2600 \tAverage Loss Per Batch: 5.705919582171882 \t Current Batch Loss:  5.281748\n",
      "Done Batch: 2650 \tAverage Loss Per Batch: 5.705256499150527 \t Current Batch Loss:  5.3693976\n",
      "Done Batch: 2700 \tAverage Loss Per Batch: 5.704588326203828 \t Current Batch Loss:  5.6666107\n",
      "Done Batch: 2750 \tAverage Loss Per Batch: 5.703398609369375 \t Current Batch Loss:  5.550856\n",
      "Done Batch: 2800 \tAverage Loss Per Batch: 5.70248313292313 \t Current Batch Loss:  5.4396605\n",
      "Done Batch: 2850 \tAverage Loss Per Batch: 5.701217788848823 \t Current Batch Loss:  5.4478755\n",
      "Done Batch: 2900 \tAverage Loss Per Batch: 5.700772355483014 \t Current Batch Loss:  5.408737\n",
      "Done Batch: 2950 \tAverage Loss Per Batch: 5.699774559697068 \t Current Batch Loss:  5.5547266\n",
      "Done Batch: 3000 \tAverage Loss Per Batch: 5.69808275435059 \t Current Batch Loss:  5.586358\n",
      "Done Batch: 3050 \tAverage Loss Per Batch: 5.697038690991497 \t Current Batch Loss:  5.6389093\n",
      "Done Batch: 3100 \tAverage Loss Per Batch: 5.695441526660839 \t Current Batch Loss:  5.8770084\n",
      "Done Batch: 3150 \tAverage Loss Per Batch: 5.694457884025513 \t Current Batch Loss:  5.5432825\n",
      "Done Batch: 3200 \tAverage Loss Per Batch: 5.693323737939646 \t Current Batch Loss:  5.574965\n",
      "Done Batch: 3250 \tAverage Loss Per Batch: 5.692483300320811 \t Current Batch Loss:  5.3593197\n",
      "Done Batch: 3300 \tAverage Loss Per Batch: 5.691850864320262 \t Current Batch Loss:  5.730924\n",
      "Done Batch: 3350 \tAverage Loss Per Batch: 5.690580526418524 \t Current Batch Loss:  5.4061146\n",
      "Epoch: 4 \tTime: 4405.009969234467 \tAverage Loss Per Batch:: 5.689813490823146\n",
      "Done Batch: 0 \tAverage Loss Per Batch: 5.720174789428711 \t Current Batch Loss:  5.720175\n",
      "Done Batch: 50 \tAverage Loss Per Batch: 5.615600875779694 \t Current Batch Loss:  5.859188\n",
      "Done Batch: 100 \tAverage Loss Per Batch: 5.626096498848188 \t Current Batch Loss:  5.642262\n",
      "Done Batch: 150 \tAverage Loss Per Batch: 5.610410314522042 \t Current Batch Loss:  5.546553\n",
      "Done Batch: 200 \tAverage Loss Per Batch: 5.610403618409266 \t Current Batch Loss:  5.6604357\n",
      "Done Batch: 250 \tAverage Loss Per Batch: 5.61230120905842 \t Current Batch Loss:  5.6709414\n",
      "Done Batch: 300 \tAverage Loss Per Batch: 5.606553819092405 \t Current Batch Loss:  5.556357\n",
      "Done Batch: 350 \tAverage Loss Per Batch: 5.604392686121145 \t Current Batch Loss:  5.24857\n",
      "Done Batch: 400 \tAverage Loss Per Batch: 5.603998083128894 \t Current Batch Loss:  5.559084\n",
      "Done Batch: 450 \tAverage Loss Per Batch: 5.598428892190599 \t Current Batch Loss:  5.655158\n",
      "Done Batch: 500 \tAverage Loss Per Batch: 5.595086425126432 \t Current Batch Loss:  5.48581\n",
      "Done Batch: 550 \tAverage Loss Per Batch: 5.589594995045186 \t Current Batch Loss:  5.2779427\n",
      "Done Batch: 600 \tAverage Loss Per Batch: 5.588190990359137 \t Current Batch Loss:  5.4578457\n",
      "Done Batch: 650 \tAverage Loss Per Batch: 5.587813059488933 \t Current Batch Loss:  5.5205364\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done Batch: 700 \tAverage Loss Per Batch: 5.587189920618599 \t Current Batch Loss:  5.5619273\n",
      "Done Batch: 750 \tAverage Loss Per Batch: 5.586388014604185 \t Current Batch Loss:  5.9697566\n",
      "Done Batch: 800 \tAverage Loss Per Batch: 5.58714642209209 \t Current Batch Loss:  5.6977587\n",
      "Done Batch: 850 \tAverage Loss Per Batch: 5.587752211668237 \t Current Batch Loss:  5.6113567\n",
      "Done Batch: 900 \tAverage Loss Per Batch: 5.587942937370411 \t Current Batch Loss:  5.5395536\n",
      "Done Batch: 950 \tAverage Loss Per Batch: 5.590093440688622 \t Current Batch Loss:  5.5603814\n",
      "Done Batch: 1000 \tAverage Loss Per Batch: 5.587761831807566 \t Current Batch Loss:  5.4966087\n",
      "Done Batch: 1050 \tAverage Loss Per Batch: 5.586335158143919 \t Current Batch Loss:  5.4824853\n",
      "Done Batch: 1100 \tAverage Loss Per Batch: 5.584060575396879 \t Current Batch Loss:  5.486878\n",
      "Done Batch: 1150 \tAverage Loss Per Batch: 5.584007882947615 \t Current Batch Loss:  5.6090107\n",
      "Done Batch: 1200 \tAverage Loss Per Batch: 5.5827368070044985 \t Current Batch Loss:  5.5628343\n",
      "Done Batch: 1250 \tAverage Loss Per Batch: 5.582327195113416 \t Current Batch Loss:  5.412294\n",
      "Done Batch: 1300 \tAverage Loss Per Batch: 5.580503018061808 \t Current Batch Loss:  5.364158\n",
      "Done Batch: 1350 \tAverage Loss Per Batch: 5.580436243823861 \t Current Batch Loss:  5.247517\n",
      "Done Batch: 1400 \tAverage Loss Per Batch: 5.578749468801363 \t Current Batch Loss:  5.497298\n",
      "Done Batch: 1450 \tAverage Loss Per Batch: 5.578172893050947 \t Current Batch Loss:  5.3651576\n",
      "Done Batch: 1500 \tAverage Loss Per Batch: 5.576889358307027 \t Current Batch Loss:  5.347987\n",
      "Done Batch: 1550 \tAverage Loss Per Batch: 5.574397914721688 \t Current Batch Loss:  5.6409407\n",
      "Done Batch: 1600 \tAverage Loss Per Batch: 5.573053860351639 \t Current Batch Loss:  5.546323\n",
      "Done Batch: 1650 \tAverage Loss Per Batch: 5.571112245736448 \t Current Batch Loss:  5.592662\n",
      "Done Batch: 1700 \tAverage Loss Per Batch: 5.569525274930458 \t Current Batch Loss:  5.713614\n",
      "Done Batch: 1750 \tAverage Loss Per Batch: 5.569645739500349 \t Current Batch Loss:  5.61305\n",
      "Done Batch: 1800 \tAverage Loss Per Batch: 5.569368072776646 \t Current Batch Loss:  5.5789332\n",
      "Done Batch: 1850 \tAverage Loss Per Batch: 5.5692846266274065 \t Current Batch Loss:  5.7956758\n",
      "Done Batch: 1900 \tAverage Loss Per Batch: 5.566843100311504 \t Current Batch Loss:  5.4431443\n",
      "Done Batch: 1950 \tAverage Loss Per Batch: 5.564344965818905 \t Current Batch Loss:  5.568669\n",
      "Done Batch: 2000 \tAverage Loss Per Batch: 5.5623809699116205 \t Current Batch Loss:  5.4968343\n",
      "Done Batch: 2050 \tAverage Loss Per Batch: 5.560988711241337 \t Current Batch Loss:  5.3836937\n",
      "Done Batch: 2100 \tAverage Loss Per Batch: 5.558879515717337 \t Current Batch Loss:  5.363133\n",
      "Done Batch: 2150 \tAverage Loss Per Batch: 5.556942699898791 \t Current Batch Loss:  5.711511\n",
      "Done Batch: 2200 \tAverage Loss Per Batch: 5.556042321320828 \t Current Batch Loss:  5.867508\n",
      "Done Batch: 2250 \tAverage Loss Per Batch: 5.555285693168217 \t Current Batch Loss:  5.25459\n",
      "Done Batch: 2300 \tAverage Loss Per Batch: 5.55430072476065 \t Current Batch Loss:  5.6431317\n",
      "Done Batch: 2350 \tAverage Loss Per Batch: 5.5523381724047285 \t Current Batch Loss:  5.4712977\n",
      "Done Batch: 2400 \tAverage Loss Per Batch: 5.5520168675427035 \t Current Batch Loss:  5.3391814\n",
      "Done Batch: 2450 \tAverage Loss Per Batch: 5.551094330850304 \t Current Batch Loss:  5.585102\n",
      "Done Batch: 2500 \tAverage Loss Per Batch: 5.550433211496285 \t Current Batch Loss:  5.891084\n",
      "Done Batch: 2550 \tAverage Loss Per Batch: 5.549386042232656 \t Current Batch Loss:  5.5507703\n",
      "Done Batch: 2600 \tAverage Loss Per Batch: 5.548089723502705 \t Current Batch Loss:  5.172453\n",
      "Done Batch: 2650 \tAverage Loss Per Batch: 5.547283972492312 \t Current Batch Loss:  5.200921\n",
      "Done Batch: 2700 \tAverage Loss Per Batch: 5.546656363719219 \t Current Batch Loss:  5.515277\n",
      "Done Batch: 2750 \tAverage Loss Per Batch: 5.545010673484123 \t Current Batch Loss:  5.3618183\n",
      "Done Batch: 2800 \tAverage Loss Per Batch: 5.543832809573537 \t Current Batch Loss:  5.296433\n",
      "Done Batch: 2850 \tAverage Loss Per Batch: 5.542644323444668 \t Current Batch Loss:  5.2485933\n",
      "Done Batch: 2900 \tAverage Loss Per Batch: 5.54213983377314 \t Current Batch Loss:  5.248128\n",
      "Done Batch: 2950 \tAverage Loss Per Batch: 5.540725846647691 \t Current Batch Loss:  5.4169784\n",
      "Done Batch: 3000 \tAverage Loss Per Batch: 5.538854790782579 \t Current Batch Loss:  5.4174647\n",
      "Done Batch: 3050 \tAverage Loss Per Batch: 5.53771054897102 \t Current Batch Loss:  5.4881444\n",
      "Done Batch: 3100 \tAverage Loss Per Batch: 5.535803744117124 \t Current Batch Loss:  5.674731\n",
      "Done Batch: 3150 \tAverage Loss Per Batch: 5.534631855031567 \t Current Batch Loss:  5.357723\n",
      "Done Batch: 3200 \tAverage Loss Per Batch: 5.53342478203051 \t Current Batch Loss:  5.4075146\n",
      "Done Batch: 3250 \tAverage Loss Per Batch: 5.532333123064232 \t Current Batch Loss:  5.195692\n",
      "Done Batch: 3300 \tAverage Loss Per Batch: 5.531395572418518 \t Current Batch Loss:  5.5030427\n",
      "Done Batch: 3350 \tAverage Loss Per Batch: 5.529871297715067 \t Current Batch Loss:  5.195081\n",
      "Epoch: 5 \tTime: 4390.305835485458 \tAverage Loss Per Batch:: 5.528886837955041\n",
      "Done Batch: 0 \tAverage Loss Per Batch: 5.520071029663086 \t Current Batch Loss:  5.520071\n",
      "Done Batch: 50 \tAverage Loss Per Batch: 5.431188489876542 \t Current Batch Loss:  5.7571044\n",
      "Done Batch: 100 \tAverage Loss Per Batch: 5.453091319244687 \t Current Batch Loss:  5.5049214\n",
      "Done Batch: 150 \tAverage Loss Per Batch: 5.43626623280001 \t Current Batch Loss:  5.3809233\n",
      "Done Batch: 200 \tAverage Loss Per Batch: 5.4364502963735095 \t Current Batch Loss:  5.4181604\n",
      "Done Batch: 250 \tAverage Loss Per Batch: 5.438844967648327 \t Current Batch Loss:  5.5014887\n",
      "Done Batch: 300 \tAverage Loss Per Batch: 5.432534569521679 \t Current Batch Loss:  5.3776894\n",
      "Done Batch: 350 \tAverage Loss Per Batch: 5.43106003432532 \t Current Batch Loss:  5.0234222\n",
      "Done Batch: 400 \tAverage Loss Per Batch: 5.42811506823114 \t Current Batch Loss:  5.2552767\n",
      "Done Batch: 450 \tAverage Loss Per Batch: 5.420414319852504 \t Current Batch Loss:  5.464256\n",
      "Done Batch: 500 \tAverage Loss Per Batch: 5.4163806500311145 \t Current Batch Loss:  5.294597\n",
      "Done Batch: 550 \tAverage Loss Per Batch: 5.410435662728256 \t Current Batch Loss:  5.0643578\n",
      "Done Batch: 600 \tAverage Loss Per Batch: 5.408197885345103 \t Current Batch Loss:  5.253268\n",
      "Done Batch: 650 \tAverage Loss Per Batch: 5.408743834165933 \t Current Batch Loss:  5.389546\n",
      "Done Batch: 700 \tAverage Loss Per Batch: 5.4088215657885845 \t Current Batch Loss:  5.404211\n",
      "Done Batch: 750 \tAverage Loss Per Batch: 5.407871060301556 \t Current Batch Loss:  5.8058324\n",
      "Done Batch: 800 \tAverage Loss Per Batch: 5.407634749394678 \t Current Batch Loss:  5.6050434\n",
      "Done Batch: 850 \tAverage Loss Per Batch: 5.4081398390154725 \t Current Batch Loss:  5.4225025\n",
      "Done Batch: 900 \tAverage Loss Per Batch: 5.408014418679257 \t Current Batch Loss:  5.406049\n",
      "Done Batch: 950 \tAverage Loss Per Batch: 5.41057952947045 \t Current Batch Loss:  5.4240894\n",
      "Done Batch: 1000 \tAverage Loss Per Batch: 5.40758563469459 \t Current Batch Loss:  5.3766284\n",
      "Done Batch: 1050 \tAverage Loss Per Batch: 5.405531961275893 \t Current Batch Loss:  5.308577\n",
      "Done Batch: 1100 \tAverage Loss Per Batch: 5.402827536161112 \t Current Batch Loss:  5.2822323\n",
      "Done Batch: 1150 \tAverage Loss Per Batch: 5.402161902287647 \t Current Batch Loss:  5.3641744\n",
      "Done Batch: 1200 \tAverage Loss Per Batch: 5.400444689440191 \t Current Batch Loss:  5.3451524\n",
      "Done Batch: 1250 \tAverage Loss Per Batch: 5.399585075896802 \t Current Batch Loss:  5.20074\n",
      "Done Batch: 1300 \tAverage Loss Per Batch: 5.397579642830218 \t Current Batch Loss:  5.1982594\n",
      "Done Batch: 1350 \tAverage Loss Per Batch: 5.397026739678147 \t Current Batch Loss:  5.044956\n",
      "Done Batch: 1400 \tAverage Loss Per Batch: 5.394655638469448 \t Current Batch Loss:  5.2194285\n",
      "Done Batch: 1450 \tAverage Loss Per Batch: 5.393563021469905 \t Current Batch Loss:  5.165675\n",
      "Done Batch: 1500 \tAverage Loss Per Batch: 5.391819772841055 \t Current Batch Loss:  5.1789303\n",
      "Done Batch: 1550 \tAverage Loss Per Batch: 5.3900558636466736 \t Current Batch Loss:  5.422146\n",
      "Done Batch: 1600 \tAverage Loss Per Batch: 5.389214321198425 \t Current Batch Loss:  5.3940234\n",
      "Done Batch: 1650 \tAverage Loss Per Batch: 5.38694277710802 \t Current Batch Loss:  5.389405\n",
      "Done Batch: 1700 \tAverage Loss Per Batch: 5.385017345121228 \t Current Batch Loss:  5.578962\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done Batch: 1750 \tAverage Loss Per Batch: 5.385182830281152 \t Current Batch Loss:  5.389167\n",
      "Done Batch: 1800 \tAverage Loss Per Batch: 5.38515687902261 \t Current Batch Loss:  5.3936267\n",
      "Done Batch: 1850 \tAverage Loss Per Batch: 5.385335028847767 \t Current Batch Loss:  5.7184463\n",
      "Done Batch: 1900 \tAverage Loss Per Batch: 5.382867391958543 \t Current Batch Loss:  5.229538\n",
      "Done Batch: 1950 \tAverage Loss Per Batch: 5.38001173963796 \t Current Batch Loss:  5.350504\n",
      "Done Batch: 2000 \tAverage Loss Per Batch: 5.377775047374689 \t Current Batch Loss:  5.314693\n",
      "Done Batch: 2050 \tAverage Loss Per Batch: 5.3765620680799255 \t Current Batch Loss:  5.2559485\n",
      "Done Batch: 2100 \tAverage Loss Per Batch: 5.3752946939881445 \t Current Batch Loss:  5.169223\n",
      "Done Batch: 2150 \tAverage Loss Per Batch: 5.373664171404198 \t Current Batch Loss:  5.5708575\n",
      "Done Batch: 2200 \tAverage Loss Per Batch: 5.372705290177799 \t Current Batch Loss:  5.6480947\n",
      "Done Batch: 2250 \tAverage Loss Per Batch: 5.372012588089173 \t Current Batch Loss:  5.0638647\n",
      "Done Batch: 2300 \tAverage Loss Per Batch: 5.370778142861934 \t Current Batch Loss:  5.4018364\n",
      "Done Batch: 2350 \tAverage Loss Per Batch: 5.368353432566498 \t Current Batch Loss:  5.2681923\n",
      "Done Batch: 2400 \tAverage Loss Per Batch: 5.367400019628612 \t Current Batch Loss:  5.0993905\n",
      "Done Batch: 2450 \tAverage Loss Per Batch: 5.365957839495793 \t Current Batch Loss:  5.4460726\n",
      "Done Batch: 2500 \tAverage Loss Per Batch: 5.3642770028600495 \t Current Batch Loss:  5.6354156\n",
      "Done Batch: 2550 \tAverage Loss Per Batch: 5.362677182743756 \t Current Batch Loss:  5.382185\n",
      "Done Batch: 2600 \tAverage Loss Per Batch: 5.361103877889244 \t Current Batch Loss:  4.9930916\n",
      "Done Batch: 2650 \tAverage Loss Per Batch: 5.360028244962516 \t Current Batch Loss:  4.9886847\n",
      "Done Batch: 2700 \tAverage Loss Per Batch: 5.359574463225346 \t Current Batch Loss:  5.3357234\n",
      "Done Batch: 2750 \tAverage Loss Per Batch: 5.357556266465736 \t Current Batch Loss:  5.162983\n",
      "Done Batch: 2800 \tAverage Loss Per Batch: 5.355998761395989 \t Current Batch Loss:  5.156804\n",
      "Done Batch: 2850 \tAverage Loss Per Batch: 5.354237965724628 \t Current Batch Loss:  5.029333\n",
      "Done Batch: 2900 \tAverage Loss Per Batch: 5.353228159914013 \t Current Batch Loss:  5.0356402\n",
      "Done Batch: 2950 \tAverage Loss Per Batch: 5.3511649251024584 \t Current Batch Loss:  5.1757536\n",
      "Done Batch: 3000 \tAverage Loss Per Batch: 5.3488218340227025 \t Current Batch Loss:  5.232358\n",
      "Done Batch: 3050 \tAverage Loss Per Batch: 5.347087039903749 \t Current Batch Loss:  5.2915716\n",
      "Done Batch: 3100 \tAverage Loss Per Batch: 5.344621746434738 \t Current Batch Loss:  5.413269\n",
      "Done Batch: 3150 \tAverage Loss Per Batch: 5.342561934267442 \t Current Batch Loss:  5.1355987\n",
      "Done Batch: 3200 \tAverage Loss Per Batch: 5.341083716094885 \t Current Batch Loss:  5.1759505\n",
      "Done Batch: 3250 \tAverage Loss Per Batch: 5.339514159158867 \t Current Batch Loss:  4.9419703\n",
      "Done Batch: 3300 \tAverage Loss Per Batch: 5.338102648959525 \t Current Batch Loss:  5.192653\n",
      "Done Batch: 3350 \tAverage Loss Per Batch: 5.336287916400332 \t Current Batch Loss:  4.915318\n",
      "Epoch: 6 \tTime: 4395.811999797821 \tAverage Loss Per Batch:: 5.335021903332879\n",
      "Done Batch: 0 \tAverage Loss Per Batch: 5.170819282531738 \t Current Batch Loss:  5.1708193\n",
      "Done Batch: 50 \tAverage Loss Per Batch: 5.205241923238717 \t Current Batch Loss:  5.556194\n",
      "Done Batch: 100 \tAverage Loss Per Batch: 5.220876367965547 \t Current Batch Loss:  5.268692\n",
      "Done Batch: 150 \tAverage Loss Per Batch: 5.20590049225763 \t Current Batch Loss:  5.1581855\n",
      "Done Batch: 200 \tAverage Loss Per Batch: 5.202459695920422 \t Current Batch Loss:  5.1664677\n",
      "Done Batch: 250 \tAverage Loss Per Batch: 5.205838045750956 \t Current Batch Loss:  5.303689\n",
      "Done Batch: 300 \tAverage Loss Per Batch: 5.199605156020865 \t Current Batch Loss:  5.135711\n",
      "Done Batch: 350 \tAverage Loss Per Batch: 5.198892327115746 \t Current Batch Loss:  4.696937\n",
      "Done Batch: 400 \tAverage Loss Per Batch: 5.195594446320189 \t Current Batch Loss:  4.9885836\n",
      "Done Batch: 450 \tAverage Loss Per Batch: 5.186113524595544 \t Current Batch Loss:  5.1922655\n",
      "Done Batch: 500 \tAverage Loss Per Batch: 5.181115759584956 \t Current Batch Loss:  5.128911\n",
      "Done Batch: 550 \tAverage Loss Per Batch: 5.177228279425314 \t Current Batch Loss:  4.815037\n",
      "Done Batch: 600 \tAverage Loss Per Batch: 5.175010568488656 \t Current Batch Loss:  5.0614758\n",
      "Done Batch: 650 \tAverage Loss Per Batch: 5.1769781163943716 \t Current Batch Loss:  5.08091\n",
      "Done Batch: 700 \tAverage Loss Per Batch: 5.177817216103155 \t Current Batch Loss:  5.1050916\n",
      "Done Batch: 750 \tAverage Loss Per Batch: 5.176622475828534 \t Current Batch Loss:  5.4884577\n",
      "Done Batch: 800 \tAverage Loss Per Batch: 5.175206253442276 \t Current Batch Loss:  5.4164357\n",
      "Done Batch: 850 \tAverage Loss Per Batch: 5.177918292940994 \t Current Batch Loss:  5.1876163\n",
      "Done Batch: 900 \tAverage Loss Per Batch: 5.178651465692213 \t Current Batch Loss:  5.2018003\n",
      "Done Batch: 950 \tAverage Loss Per Batch: 5.182660614278414 \t Current Batch Loss:  5.24734\n",
      "Done Batch: 1000 \tAverage Loss Per Batch: 5.179923814017099 \t Current Batch Loss:  5.0085955\n",
      "Done Batch: 1050 \tAverage Loss Per Batch: 5.178054335455346 \t Current Batch Loss:  5.08211\n",
      "Done Batch: 1100 \tAverage Loss Per Batch: 5.174902878708454 \t Current Batch Loss:  4.9837465\n",
      "Done Batch: 1150 \tAverage Loss Per Batch: 5.1736974401333145 \t Current Batch Loss:  5.1003423\n",
      "Done Batch: 1200 \tAverage Loss Per Batch: 5.1716904322571 \t Current Batch Loss:  5.0102077\n",
      "Done Batch: 1250 \tAverage Loss Per Batch: 5.170560854706737 \t Current Batch Loss:  5.10103\n",
      "Done Batch: 1300 \tAverage Loss Per Batch: 5.168622012875064 \t Current Batch Loss:  4.9995832\n",
      "Done Batch: 1350 \tAverage Loss Per Batch: 5.167995883128451 \t Current Batch Loss:  4.7794456\n",
      "Done Batch: 1400 \tAverage Loss Per Batch: 5.165066881064089 \t Current Batch Loss:  5.011181\n",
      "Done Batch: 1450 \tAverage Loss Per Batch: 5.163249757518282 \t Current Batch Loss:  5.014869\n",
      "Done Batch: 1500 \tAverage Loss Per Batch: 5.160491852185315 \t Current Batch Loss:  4.9993453\n",
      "Done Batch: 1550 \tAverage Loss Per Batch: 5.156725252620026 \t Current Batch Loss:  5.1867456\n",
      "Done Batch: 1600 \tAverage Loss Per Batch: 5.154917650562313 \t Current Batch Loss:  5.198987\n",
      "Done Batch: 1650 \tAverage Loss Per Batch: 5.152127333513395 \t Current Batch Loss:  5.168497\n",
      "Done Batch: 1700 \tAverage Loss Per Batch: 5.149411018983818 \t Current Batch Loss:  5.4145937\n",
      "Done Batch: 1750 \tAverage Loss Per Batch: 5.148847152409316 \t Current Batch Loss:  5.1211543\n",
      "Done Batch: 1800 \tAverage Loss Per Batch: 5.148800142469305 \t Current Batch Loss:  5.1907496\n",
      "Done Batch: 1850 \tAverage Loss Per Batch: 5.148534413095039 \t Current Batch Loss:  5.534314\n",
      "Done Batch: 1900 \tAverage Loss Per Batch: 5.145398683763692 \t Current Batch Loss:  4.9277244\n",
      "Done Batch: 1950 \tAverage Loss Per Batch: 5.14237020591294 \t Current Batch Loss:  5.080369\n",
      "Done Batch: 2000 \tAverage Loss Per Batch: 5.139252235626114 \t Current Batch Loss:  5.106181\n",
      "Done Batch: 2050 \tAverage Loss Per Batch: 5.137316496880214 \t Current Batch Loss:  5.012207\n",
      "Done Batch: 2100 \tAverage Loss Per Batch: 5.135959824739554 \t Current Batch Loss:  4.903509\n",
      "Done Batch: 2150 \tAverage Loss Per Batch: 5.1332877758545745 \t Current Batch Loss:  5.262296\n",
      "Done Batch: 2200 \tAverage Loss Per Batch: 5.1320346477409755 \t Current Batch Loss:  5.3621144\n",
      "Done Batch: 2250 \tAverage Loss Per Batch: 5.131046118920562 \t Current Batch Loss:  4.83383\n",
      "Done Batch: 2300 \tAverage Loss Per Batch: 5.129548269683824 \t Current Batch Loss:  5.115679\n",
      "Done Batch: 2350 \tAverage Loss Per Batch: 5.126582188283773 \t Current Batch Loss:  4.898952\n",
      "Done Batch: 2400 \tAverage Loss Per Batch: 5.124749544708494 \t Current Batch Loss:  4.783446\n",
      "Done Batch: 2450 \tAverage Loss Per Batch: 5.122551983787011 \t Current Batch Loss:  5.1073513\n",
      "Done Batch: 2500 \tAverage Loss Per Batch: 5.1197688729226325 \t Current Batch Loss:  5.288542\n",
      "Done Batch: 2550 \tAverage Loss Per Batch: 5.117155470599478 \t Current Batch Loss:  5.159912\n",
      "Done Batch: 2600 \tAverage Loss Per Batch: 5.115208375916853 \t Current Batch Loss:  4.743908\n",
      "Done Batch: 2650 \tAverage Loss Per Batch: 5.114217563738512 \t Current Batch Loss:  4.766348\n",
      "Done Batch: 2700 \tAverage Loss Per Batch: 5.114079773403988 \t Current Batch Loss:  5.1753635\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done Batch: 2750 \tAverage Loss Per Batch: 5.112200354281793 \t Current Batch Loss:  4.928212\n",
      "Done Batch: 2800 \tAverage Loss Per Batch: 5.1096277107557455 \t Current Batch Loss:  5.0783787\n",
      "Done Batch: 2850 \tAverage Loss Per Batch: 5.1073211071492075 \t Current Batch Loss:  4.7449136\n",
      "Done Batch: 2900 \tAverage Loss Per Batch: 5.105879451603118 \t Current Batch Loss:  4.8449874\n",
      "Done Batch: 2950 \tAverage Loss Per Batch: 5.103134347559516 \t Current Batch Loss:  4.9000664\n",
      "Done Batch: 3000 \tAverage Loss Per Batch: 5.100104916854447 \t Current Batch Loss:  4.950322\n",
      "Done Batch: 3050 \tAverage Loss Per Batch: 5.097839063677543 \t Current Batch Loss:  5.064925\n",
      "Done Batch: 3100 \tAverage Loss Per Batch: 5.094728686508922 \t Current Batch Loss:  5.101854\n",
      "Done Batch: 3150 \tAverage Loss Per Batch: 5.0917880644839135 \t Current Batch Loss:  4.7219114\n",
      "Done Batch: 3200 \tAverage Loss Per Batch: 5.089696929142722 \t Current Batch Loss:  4.858559\n",
      "Done Batch: 3250 \tAverage Loss Per Batch: 5.087564183249542 \t Current Batch Loss:  4.694724\n",
      "Done Batch: 3300 \tAverage Loss Per Batch: 5.085626209840309 \t Current Batch Loss:  4.914132\n",
      "Done Batch: 3350 \tAverage Loss Per Batch: 5.083586877653827 \t Current Batch Loss:  4.6513486\n",
      "Epoch: 7 \tTime: 4392.55396771431 \tAverage Loss Per Batch:: 5.082236082361967\n",
      "Done Batch: 0 \tAverage Loss Per Batch: 4.930756092071533 \t Current Batch Loss:  4.930756\n",
      "Done Batch: 50 \tAverage Loss Per Batch: 4.8974488950243185 \t Current Batch Loss:  5.1097775\n",
      "Done Batch: 100 \tAverage Loss Per Batch: 4.91750429644443 \t Current Batch Loss:  5.0195165\n",
      "Done Batch: 150 \tAverage Loss Per Batch: 4.905712598206981 \t Current Batch Loss:  4.8363132\n",
      "Done Batch: 200 \tAverage Loss Per Batch: 4.907402209381559 \t Current Batch Loss:  4.9505653\n",
      "Done Batch: 250 \tAverage Loss Per Batch: 4.908334618070686 \t Current Batch Loss:  5.064188\n",
      "Done Batch: 300 \tAverage Loss Per Batch: 4.902809613566858 \t Current Batch Loss:  5.009149\n",
      "Done Batch: 350 \tAverage Loss Per Batch: 4.902964966928857 \t Current Batch Loss:  4.441962\n",
      "Done Batch: 400 \tAverage Loss Per Batch: 4.896675698477728 \t Current Batch Loss:  4.586206\n",
      "Done Batch: 450 \tAverage Loss Per Batch: 4.886907114422242 \t Current Batch Loss:  4.8579164\n",
      "Done Batch: 500 \tAverage Loss Per Batch: 4.88265534836851 \t Current Batch Loss:  4.8607006\n",
      "Done Batch: 550 \tAverage Loss Per Batch: 4.880182120847615 \t Current Batch Loss:  4.5914016\n",
      "Done Batch: 600 \tAverage Loss Per Batch: 4.878374617032322 \t Current Batch Loss:  4.7365723\n",
      "Done Batch: 650 \tAverage Loss Per Batch: 4.880842665922806 \t Current Batch Loss:  4.869566\n",
      "Done Batch: 700 \tAverage Loss Per Batch: 4.882744717699995 \t Current Batch Loss:  4.8049755\n",
      "Done Batch: 750 \tAverage Loss Per Batch: 4.881830285297094 \t Current Batch Loss:  5.171842\n",
      "Done Batch: 800 \tAverage Loss Per Batch: 4.87863456473666 \t Current Batch Loss:  5.1888885\n",
      "Done Batch: 850 \tAverage Loss Per Batch: 4.878774202528347 \t Current Batch Loss:  4.890634\n",
      "Done Batch: 900 \tAverage Loss Per Batch: 4.879964505131052 \t Current Batch Loss:  4.8285275\n",
      "Done Batch: 950 \tAverage Loss Per Batch: 4.884511337922073 \t Current Batch Loss:  4.9921103\n",
      "Done Batch: 1000 \tAverage Loss Per Batch: 4.883362869163612 \t Current Batch Loss:  4.625851\n",
      "Done Batch: 1050 \tAverage Loss Per Batch: 4.882657584182202 \t Current Batch Loss:  4.7453976\n",
      "Done Batch: 1100 \tAverage Loss Per Batch: 4.880695845840412 \t Current Batch Loss:  4.760091\n",
      "Done Batch: 1150 \tAverage Loss Per Batch: 4.879761579034641 \t Current Batch Loss:  4.8533616\n",
      "Done Batch: 1200 \tAverage Loss Per Batch: 4.878309644132133 \t Current Batch Loss:  4.744437\n",
      "Done Batch: 1250 \tAverage Loss Per Batch: 4.877324775540286 \t Current Batch Loss:  4.8361626\n",
      "Done Batch: 1300 \tAverage Loss Per Batch: 4.875306892541626 \t Current Batch Loss:  4.600223\n",
      "Done Batch: 1350 \tAverage Loss Per Batch: 4.874272422734055 \t Current Batch Loss:  4.5224614\n",
      "Done Batch: 1400 \tAverage Loss Per Batch: 4.870998836601742 \t Current Batch Loss:  4.7733526\n",
      "Done Batch: 1450 \tAverage Loss Per Batch: 4.86880525435192 \t Current Batch Loss:  4.678481\n",
      "Done Batch: 1500 \tAverage Loss Per Batch: 4.865801628870141 \t Current Batch Loss:  4.7183228\n",
      "Done Batch: 1550 \tAverage Loss Per Batch: 4.8604078219061435 \t Current Batch Loss:  4.836916\n",
      "Done Batch: 1600 \tAverage Loss Per Batch: 4.857948654372568 \t Current Batch Loss:  4.849391\n",
      "Done Batch: 1650 \tAverage Loss Per Batch: 4.854615161810262 \t Current Batch Loss:  4.743378\n",
      "Done Batch: 1700 \tAverage Loss Per Batch: 4.850957907206308 \t Current Batch Loss:  5.125013\n",
      "Done Batch: 1750 \tAverage Loss Per Batch: 4.849408991741222 \t Current Batch Loss:  4.788629\n",
      "Done Batch: 1800 \tAverage Loss Per Batch: 4.849070691982949 \t Current Batch Loss:  4.8309546\n",
      "Done Batch: 1850 \tAverage Loss Per Batch: 4.847784564535016 \t Current Batch Loss:  5.2248554\n",
      "Done Batch: 1900 \tAverage Loss Per Batch: 4.843866152615374 \t Current Batch Loss:  4.629701\n",
      "Done Batch: 1950 \tAverage Loss Per Batch: 4.839740852158476 \t Current Batch Loss:  4.6192946\n",
      "Done Batch: 2000 \tAverage Loss Per Batch: 4.8356513753049315 \t Current Batch Loss:  4.7049584\n",
      "Done Batch: 2050 \tAverage Loss Per Batch: 4.833767836992477 \t Current Batch Loss:  4.7655525\n",
      "Done Batch: 2100 \tAverage Loss Per Batch: 4.831420119520257 \t Current Batch Loss:  4.711216\n",
      "Done Batch: 2150 \tAverage Loss Per Batch: 4.828863796329897 \t Current Batch Loss:  4.9063487\n",
      "Done Batch: 2200 \tAverage Loss Per Batch: 4.827796932135534 \t Current Batch Loss:  4.957019\n",
      "Done Batch: 2250 \tAverage Loss Per Batch: 4.826332291620458 \t Current Batch Loss:  4.525572\n",
      "Done Batch: 2300 \tAverage Loss Per Batch: 4.824831333226714 \t Current Batch Loss:  4.8446836\n",
      "Done Batch: 2350 \tAverage Loss Per Batch: 4.822209618234574 \t Current Batch Loss:  4.662063\n",
      "Done Batch: 2400 \tAverage Loss Per Batch: 4.819867789075058 \t Current Batch Loss:  4.4788284\n",
      "Done Batch: 2450 \tAverage Loss Per Batch: 4.817741713490792 \t Current Batch Loss:  4.830523\n",
      "Done Batch: 2500 \tAverage Loss Per Batch: 4.813870871748652 \t Current Batch Loss:  4.743191\n",
      "Done Batch: 2550 \tAverage Loss Per Batch: 4.810451663834868 \t Current Batch Loss:  4.7087584\n",
      "Done Batch: 2600 \tAverage Loss Per Batch: 4.807622900929464 \t Current Batch Loss:  4.6499224\n",
      "Done Batch: 2650 \tAverage Loss Per Batch: 4.8066734584130595 \t Current Batch Loss:  4.507946\n",
      "Done Batch: 2700 \tAverage Loss Per Batch: 4.806377998240477 \t Current Batch Loss:  4.805925\n",
      "Done Batch: 2750 \tAverage Loss Per Batch: 4.803903417472881 \t Current Batch Loss:  4.5516515\n",
      "Done Batch: 2800 \tAverage Loss Per Batch: 4.801075558286188 \t Current Batch Loss:  4.7597075\n",
      "Done Batch: 2850 \tAverage Loss Per Batch: 4.798708308081759 \t Current Batch Loss:  4.425546\n",
      "Done Batch: 2900 \tAverage Loss Per Batch: 4.797315881729784 \t Current Batch Loss:  4.6101713\n",
      "Done Batch: 2950 \tAverage Loss Per Batch: 4.794435405925103 \t Current Batch Loss:  4.652118\n",
      "Done Batch: 3000 \tAverage Loss Per Batch: 4.790381596828055 \t Current Batch Loss:  4.6558557\n",
      "Done Batch: 3050 \tAverage Loss Per Batch: 4.789589891118013 \t Current Batch Loss:  4.7388663\n",
      "Done Batch: 3100 \tAverage Loss Per Batch: 4.788054232749582 \t Current Batch Loss:  4.789899\n",
      "Done Batch: 3150 \tAverage Loss Per Batch: 4.785533855469104 \t Current Batch Loss:  4.336326\n",
      "Done Batch: 3200 \tAverage Loss Per Batch: 4.7832862892883785 \t Current Batch Loss:  4.410406\n",
      "Done Batch: 3250 \tAverage Loss Per Batch: 4.780407905945299 \t Current Batch Loss:  4.3310575\n",
      "Done Batch: 3300 \tAverage Loss Per Batch: 4.778346441977315 \t Current Batch Loss:  4.6073365\n",
      "Done Batch: 3350 \tAverage Loss Per Batch: 4.776336264588946 \t Current Batch Loss:  4.3127136\n",
      "Epoch: 8 \tTime: 4387.437413692474 \tAverage Loss Per Batch:: 4.774797479795496\n",
      "Done Batch: 0 \tAverage Loss Per Batch: 4.645386695861816 \t Current Batch Loss:  4.6453867\n",
      "Done Batch: 50 \tAverage Loss Per Batch: 4.58528564490524 \t Current Batch Loss:  4.8696837\n",
      "Done Batch: 100 \tAverage Loss Per Batch: 4.592610718000053 \t Current Batch Loss:  4.691033\n",
      "Done Batch: 150 \tAverage Loss Per Batch: 4.582286967347 \t Current Batch Loss:  4.4117684\n",
      "Done Batch: 200 \tAverage Loss Per Batch: 4.589367690964124 \t Current Batch Loss:  4.62486\n",
      "Done Batch: 250 \tAverage Loss Per Batch: 4.586888890817346 \t Current Batch Loss:  4.637309\n",
      "Done Batch: 300 \tAverage Loss Per Batch: 4.576959823849193 \t Current Batch Loss:  4.5181007\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done Batch: 350 \tAverage Loss Per Batch: 4.576333961595497 \t Current Batch Loss:  4.0876145\n",
      "Done Batch: 400 \tAverage Loss Per Batch: 4.565685900072207 \t Current Batch Loss:  3.9997845\n",
      "Done Batch: 450 \tAverage Loss Per Batch: 4.5501855736032555 \t Current Batch Loss:  4.595671\n",
      "Done Batch: 500 \tAverage Loss Per Batch: 4.54506088302521 \t Current Batch Loss:  4.550986\n",
      "Done Batch: 550 \tAverage Loss Per Batch: 4.540461335987015 \t Current Batch Loss:  4.2890186\n",
      "Done Batch: 600 \tAverage Loss Per Batch: 4.542407027100168 \t Current Batch Loss:  4.534943\n",
      "Done Batch: 650 \tAverage Loss Per Batch: 4.54712948454873 \t Current Batch Loss:  4.6340957\n",
      "Done Batch: 700 \tAverage Loss Per Batch: 4.552110576085459 \t Current Batch Loss:  4.4450355\n",
      "Done Batch: 750 \tAverage Loss Per Batch: 4.551687348539756 \t Current Batch Loss:  4.792299\n",
      "Done Batch: 800 \tAverage Loss Per Batch: 4.5470791961369885 \t Current Batch Loss:  4.7554865\n",
      "Done Batch: 850 \tAverage Loss Per Batch: 4.5439021105491735 \t Current Batch Loss:  4.540053\n",
      "Done Batch: 900 \tAverage Loss Per Batch: 4.543296087595254 \t Current Batch Loss:  4.4378576\n",
      "Done Batch: 950 \tAverage Loss Per Batch: 4.54890795059886 \t Current Batch Loss:  4.736506\n",
      "Done Batch: 1000 \tAverage Loss Per Batch: 4.548889282819156 \t Current Batch Loss:  4.261626\n",
      "Done Batch: 1050 \tAverage Loss Per Batch: 4.548456561554056 \t Current Batch Loss:  4.368095\n",
      "Done Batch: 1100 \tAverage Loss Per Batch: 4.546830333870828 \t Current Batch Loss:  4.412374\n",
      "Done Batch: 1150 \tAverage Loss Per Batch: 4.545770998730233 \t Current Batch Loss:  4.551798\n",
      "Done Batch: 1200 \tAverage Loss Per Batch: 4.545553325117875 \t Current Batch Loss:  4.4284706\n",
      "Done Batch: 1250 \tAverage Loss Per Batch: 4.544366178085669 \t Current Batch Loss:  4.6101356\n",
      "Done Batch: 1300 \tAverage Loss Per Batch: 4.541587411945 \t Current Batch Loss:  4.265342\n",
      "Done Batch: 1350 \tAverage Loss Per Batch: 4.54042015456871 \t Current Batch Loss:  3.9757574\n",
      "Done Batch: 1400 \tAverage Loss Per Batch: 4.53661704693072 \t Current Batch Loss:  4.383554\n",
      "Done Batch: 1450 \tAverage Loss Per Batch: 4.533057283155677 \t Current Batch Loss:  4.3733854\n",
      "Done Batch: 1500 \tAverage Loss Per Batch: 4.5302489964982655 \t Current Batch Loss:  4.4378967\n",
      "Done Batch: 1550 \tAverage Loss Per Batch: 4.5241934804283055 \t Current Batch Loss:  4.494772\n",
      "Done Batch: 1600 \tAverage Loss Per Batch: 4.520559468468899 \t Current Batch Loss:  4.4074907\n",
      "Done Batch: 1650 \tAverage Loss Per Batch: 4.516047366671963 \t Current Batch Loss:  4.57123\n",
      "Done Batch: 1700 \tAverage Loss Per Batch: 4.5118497783194424 \t Current Batch Loss:  4.6819954\n",
      "Done Batch: 1750 \tAverage Loss Per Batch: 4.5084458052397185 \t Current Batch Loss:  4.4340043\n",
      "Done Batch: 1800 \tAverage Loss Per Batch: 4.507303175563484 \t Current Batch Loss:  4.577635\n",
      "Done Batch: 1850 \tAverage Loss Per Batch: 4.5052257068475345 \t Current Batch Loss:  4.725436\n",
      "Done Batch: 1900 \tAverage Loss Per Batch: 4.500480744413549 \t Current Batch Loss:  4.1939526\n",
      "Done Batch: 1950 \tAverage Loss Per Batch: 4.495305954035217 \t Current Batch Loss:  4.37334\n",
      "Done Batch: 2000 \tAverage Loss Per Batch: 4.489694307352053 \t Current Batch Loss:  4.5100713\n",
      "Done Batch: 2050 \tAverage Loss Per Batch: 4.486955171559044 \t Current Batch Loss:  4.487062\n",
      "Done Batch: 2100 \tAverage Loss Per Batch: 4.484398485648525 \t Current Batch Loss:  4.408568\n",
      "Done Batch: 2150 \tAverage Loss Per Batch: 4.480849212959499 \t Current Batch Loss:  4.3832717\n",
      "Done Batch: 2200 \tAverage Loss Per Batch: 4.479644912743991 \t Current Batch Loss:  4.50243\n",
      "Done Batch: 2250 \tAverage Loss Per Batch: 4.478261716733345 \t Current Batch Loss:  4.2513213\n",
      "Done Batch: 2300 \tAverage Loss Per Batch: 4.476885328495933 \t Current Batch Loss:  4.460006\n",
      "Done Batch: 2350 \tAverage Loss Per Batch: 4.474646091258358 \t Current Batch Loss:  4.236239\n",
      "Done Batch: 2400 \tAverage Loss Per Batch: 4.472348330071547 \t Current Batch Loss:  4.121043\n",
      "Done Batch: 2450 \tAverage Loss Per Batch: 4.470660949229999 \t Current Batch Loss:  4.481741\n",
      "Done Batch: 2500 \tAverage Loss Per Batch: 4.465989937738436 \t Current Batch Loss:  4.542429\n",
      "Done Batch: 2550 \tAverage Loss Per Batch: 4.462261909412337 \t Current Batch Loss:  4.1966796\n",
      "Done Batch: 2600 \tAverage Loss Per Batch: 4.458829391648887 \t Current Batch Loss:  4.324107\n",
      "Done Batch: 2650 \tAverage Loss Per Batch: 4.456867822652851 \t Current Batch Loss:  4.118222\n",
      "Done Batch: 2700 \tAverage Loss Per Batch: 4.4561449314478985 \t Current Batch Loss:  4.5505147\n",
      "Done Batch: 2750 \tAverage Loss Per Batch: 4.452807537558641 \t Current Batch Loss:  4.1349034\n",
      "Done Batch: 2800 \tAverage Loss Per Batch: 4.44937886275891 \t Current Batch Loss:  4.331407\n",
      "Done Batch: 2850 \tAverage Loss Per Batch: 4.44585184321743 \t Current Batch Loss:  4.058527\n",
      "Done Batch: 2900 \tAverage Loss Per Batch: 4.443968393933317 \t Current Batch Loss:  4.2791595\n",
      "Done Batch: 2950 \tAverage Loss Per Batch: 4.440908593349398 \t Current Batch Loss:  4.294373\n",
      "Done Batch: 3000 \tAverage Loss Per Batch: 4.4365977077871825 \t Current Batch Loss:  4.1759014\n",
      "Done Batch: 3050 \tAverage Loss Per Batch: 4.434756444970415 \t Current Batch Loss:  4.2271867\n",
      "Done Batch: 3100 \tAverage Loss Per Batch: 4.432032508567163 \t Current Batch Loss:  4.367159\n",
      "Done Batch: 3150 \tAverage Loss Per Batch: 4.4283048348440435 \t Current Batch Loss:  3.9220228\n",
      "Done Batch: 3200 \tAverage Loss Per Batch: 4.42487168967519 \t Current Batch Loss:  4.0171485\n",
      "Done Batch: 3250 \tAverage Loss Per Batch: 4.42078138255809 \t Current Batch Loss:  3.9782007\n",
      "Done Batch: 3300 \tAverage Loss Per Batch: 4.417642428852579 \t Current Batch Loss:  4.180636\n",
      "Done Batch: 3350 \tAverage Loss Per Batch: 4.4152662751924385 \t Current Batch Loss:  4.140666\n",
      "Epoch: 9 \tTime: 4399.160917520523 \tAverage Loss Per Batch:: 4.413661626562861\n"
     ]
    }
   ],
   "source": [
    "USE_CUDA = True # switch to true when training on GPU(s)\n",
    "\n",
    "def train_pass(image_input, target_output, model, optimizer, criterion):\n",
    "    \"\"\"\n",
    "    Given batch of images, completes one pass of training on the model,\n",
    "    using the given optimizer and criterion.\n",
    "    \"\"\"\n",
    "\n",
    "    if USE_CUDA:\n",
    "        image_input = image_input.cuda()\n",
    "        target_output = target_output.cuda()\n",
    "        model = model.cuda()\n",
    "        criterion = criterion.cuda()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    model_output = model(image_input)\n",
    "    \n",
    "    loss = criterion(model_output, target_output)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.data.cpu().numpy()\n",
    "\n",
    "def load_batch(image_addresses, volatile=False):\n",
    "    \n",
    "    img_tensor = load_image(image_addresses[0])\n",
    "    for i in range(1, len(image_addresses)):\n",
    "        img_tensor = torch.cat((img_tensor, load_image(image_addresses[i])))\n",
    "        \n",
    "    target_tensor = torch.from_numpy(anp_tag_to_vector[train_image_to_anp_tag[image_addresses[0]]]).unsqueeze(0)\n",
    "    for i in range(1, len(image_addresses)):\n",
    "        target_tensor = torch.cat((target_tensor, torch.from_numpy(anp_tag_to_vector[train_image_to_anp_tag[image_addresses[i]]]).unsqueeze(0)))\n",
    "        \n",
    "    target_indices = torch.argmax(target_tensor, 1)\n",
    "        \n",
    "    return img_tensor, target_indices\n",
    "\n",
    "def train(model, learning_rate=0.0001, batch_size=100, epochs=10):\n",
    "    \n",
    "    # defining criterion and optimizer\n",
    "    criterion = nn.NLLLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr = learning_rate)\n",
    "    \n",
    "    number_of_batches = math.ceil(len(train_image_addresses)/batch_size)\n",
    "    indexes = np.arange(len(train_image_addresses))\n",
    "    \n",
    "    loss_arr = []\n",
    "    for epoch in range(epochs):\n",
    "        avgloss = 0.0\n",
    "        start = time.time()\n",
    "        avg_loss_arr = []\n",
    "        for batch in range(number_of_batches):\n",
    "            train_indexes = [train_image_addresses[i] for i in indexes[batch*batch_size:(batch+1)*batch_size]]\n",
    "            image_batch, target_batch = load_batch(train_indexes)\n",
    "            loss = train_pass(image_batch, target_batch, model, optimizer, criterion)\n",
    "            avgloss += loss\n",
    "            if batch%50 == 0:\n",
    "                print (\"Done Batch:\", batch, \"\\tAverage Loss Per Batch:\", avgloss/(batch+1), \"\\t Current Batch Loss: \", loss)\n",
    "        loss_arr.append(avgloss/(batch+1))\n",
    "        print (\"Epoch:\",epoch, \"\\tTime:\", time.time() - start, \"\\tAverage Loss Per Batch::\", avgloss/(batch+1))\n",
    "        torch.save({'epoch': epoch ,'state_dict': model.state_dict(),'optimizer': optimizer.state_dict()}, open(\"outputs/anp_log_softmax_classifier_batch_\"+str(epoch), \"wb+\"))\n",
    "    loss_arr = np.array(loss_arr)\n",
    "    np.save(open('outputs/loss_log_softmax_anp_classifier', 'wb+'), loss_arr)\n",
    "\n",
    "train(main_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done Batch: 0 \tAverage Loss Per Batch: 4.089768886566162 \t Current Batch Loss:  4.089769\n",
      "Done Batch: 50 \tAverage Loss Per Batch: 4.21828854785246 \t Current Batch Loss:  4.589336\n",
      "Done Batch: 100 \tAverage Loss Per Batch: 4.2734852804995995 \t Current Batch Loss:  4.286379\n",
      "Done Batch: 150 \tAverage Loss Per Batch: 4.2595590203013645 \t Current Batch Loss:  4.080468\n",
      "Done Batch: 200 \tAverage Loss Per Batch: 4.269356094189544 \t Current Batch Loss:  4.068374\n",
      "Done Batch: 250 \tAverage Loss Per Batch: 4.2775371008185274 \t Current Batch Loss:  4.3298035\n",
      "Done Batch: 300 \tAverage Loss Per Batch: 4.265339554346281 \t Current Batch Loss:  4.147882\n",
      "Done Batch: 350 \tAverage Loss Per Batch: 4.262606840867263 \t Current Batch Loss:  3.652676\n",
      "Done Batch: 400 \tAverage Loss Per Batch: 4.253583904513694 \t Current Batch Loss:  3.8766806\n",
      "Done Batch: 450 \tAverage Loss Per Batch: 4.233575204523598 \t Current Batch Loss:  4.172745\n",
      "Done Batch: 500 \tAverage Loss Per Batch: 4.222772908067989 \t Current Batch Loss:  4.0697517\n",
      "Done Batch: 550 \tAverage Loss Per Batch: 4.21398466079075 \t Current Batch Loss:  3.9384544\n",
      "Done Batch: 600 \tAverage Loss Per Batch: 4.212840726887327 \t Current Batch Loss:  4.313322\n",
      "Done Batch: 650 \tAverage Loss Per Batch: 4.213898176421767 \t Current Batch Loss:  4.2540483\n",
      "Done Batch: 700 \tAverage Loss Per Batch: 4.2153409588524005 \t Current Batch Loss:  4.1965714\n",
      "Done Batch: 750 \tAverage Loss Per Batch: 4.212090208432011 \t Current Batch Loss:  4.3611894\n",
      "Done Batch: 800 \tAverage Loss Per Batch: 4.203817307130525 \t Current Batch Loss:  4.2085557\n",
      "Done Batch: 850 \tAverage Loss Per Batch: 4.197394576392078 \t Current Batch Loss:  4.2005153\n",
      "Done Batch: 900 \tAverage Loss Per Batch: 4.193703698794399 \t Current Batch Loss:  4.183042\n",
      "Done Batch: 950 \tAverage Loss Per Batch: 4.200145289724934 \t Current Batch Loss:  4.321293\n",
      "Done Batch: 1000 \tAverage Loss Per Batch: 4.199516111082368 \t Current Batch Loss:  3.8232775\n",
      "Done Batch: 1050 \tAverage Loss Per Batch: 4.199391827596924 \t Current Batch Loss:  4.105614\n",
      "Done Batch: 1100 \tAverage Loss Per Batch: 4.196945650592704 \t Current Batch Loss:  3.8510873\n",
      "Done Batch: 1150 \tAverage Loss Per Batch: 4.195241829916045 \t Current Batch Loss:  4.193653\n",
      "Done Batch: 1200 \tAverage Loss Per Batch: 4.19391258332652 \t Current Batch Loss:  4.1485734\n",
      "Done Batch: 1250 \tAverage Loss Per Batch: 4.192619793897243 \t Current Batch Loss:  4.3025312\n",
      "Done Batch: 1300 \tAverage Loss Per Batch: 4.186396937659848 \t Current Batch Loss:  4.0624757\n",
      "Done Batch: 1350 \tAverage Loss Per Batch: 4.183183562040859 \t Current Batch Loss:  3.6582308\n",
      "Done Batch: 1400 \tAverage Loss Per Batch: 4.178419850197628 \t Current Batch Loss:  4.0200443\n",
      "Done Batch: 1450 \tAverage Loss Per Batch: 4.1739448142166715 \t Current Batch Loss:  3.9647639\n",
      "Done Batch: 1500 \tAverage Loss Per Batch: 4.169861497281791 \t Current Batch Loss:  4.081534\n",
      "Done Batch: 1550 \tAverage Loss Per Batch: 4.1628630362657635 \t Current Batch Loss:  4.0149508\n",
      "Done Batch: 1600 \tAverage Loss Per Batch: 4.157662131204075 \t Current Batch Loss:  4.170514\n",
      "Done Batch: 1650 \tAverage Loss Per Batch: 4.152689867204496 \t Current Batch Loss:  4.1312256\n",
      "Done Batch: 1700 \tAverage Loss Per Batch: 4.147482928074787 \t Current Batch Loss:  4.1626225\n",
      "Done Batch: 1750 \tAverage Loss Per Batch: 4.142392401556367 \t Current Batch Loss:  3.9982395\n",
      "Done Batch: 1800 \tAverage Loss Per Batch: 4.1383467068743665 \t Current Batch Loss:  3.995599\n",
      "Done Batch: 1850 \tAverage Loss Per Batch: 4.133856871139675 \t Current Batch Loss:  4.228162\n",
      "Done Batch: 1900 \tAverage Loss Per Batch: 4.128309671029738 \t Current Batch Loss:  3.8156292\n",
      "Done Batch: 1950 \tAverage Loss Per Batch: 4.12143655448984 \t Current Batch Loss:  3.8511014\n",
      "Done Batch: 2000 \tAverage Loss Per Batch: 4.114527970895 \t Current Batch Loss:  4.0378175\n",
      "Done Batch: 2050 \tAverage Loss Per Batch: 4.111074059722134 \t Current Batch Loss:  3.9736707\n",
      "Done Batch: 2100 \tAverage Loss Per Batch: 4.107393066296403 \t Current Batch Loss:  3.9054706\n",
      "Done Batch: 2150 \tAverage Loss Per Batch: 4.102695721019318 \t Current Batch Loss:  3.9784522\n",
      "Done Batch: 2200 \tAverage Loss Per Batch: 4.100455312174269 \t Current Batch Loss:  4.0318675\n",
      "Done Batch: 2250 \tAverage Loss Per Batch: 4.099279013489258 \t Current Batch Loss:  3.87347\n",
      "Done Batch: 2300 \tAverage Loss Per Batch: 4.0963355050507655 \t Current Batch Loss:  4.141103\n",
      "Done Batch: 2350 \tAverage Loss Per Batch: 4.094166933146501 \t Current Batch Loss:  4.003594\n",
      "Done Batch: 2400 \tAverage Loss Per Batch: 4.090589563829707 \t Current Batch Loss:  3.6227677\n",
      "Done Batch: 2450 \tAverage Loss Per Batch: 4.087117436563467 \t Current Batch Loss:  3.9414756\n",
      "Done Batch: 2500 \tAverage Loss Per Batch: 4.082242611645222 \t Current Batch Loss:  3.9360628\n",
      "Done Batch: 2550 \tAverage Loss Per Batch: 4.077358282486255 \t Current Batch Loss:  3.74428\n",
      "Done Batch: 2600 \tAverage Loss Per Batch: 4.072606590700718 \t Current Batch Loss:  3.74053\n",
      "Done Batch: 2650 \tAverage Loss Per Batch: 4.0694162413921955 \t Current Batch Loss:  3.7923555\n",
      "Done Batch: 2700 \tAverage Loss Per Batch: 4.067264288895574 \t Current Batch Loss:  4.1407304\n",
      "Done Batch: 2750 \tAverage Loss Per Batch: 4.0626669468250505 \t Current Batch Loss:  3.7478695\n",
      "Done Batch: 2800 \tAverage Loss Per Batch: 4.058999453813253 \t Current Batch Loss:  3.9282062\n",
      "Done Batch: 2850 \tAverage Loss Per Batch: 4.055131044775844 \t Current Batch Loss:  3.7884126\n",
      "Done Batch: 2900 \tAverage Loss Per Batch: 4.052577418978565 \t Current Batch Loss:  3.805057\n",
      "Done Batch: 2950 \tAverage Loss Per Batch: 4.049227944958699 \t Current Batch Loss:  3.8403277\n",
      "Done Batch: 3000 \tAverage Loss Per Batch: 4.044820860281503 \t Current Batch Loss:  3.5480325\n",
      "Done Batch: 3050 \tAverage Loss Per Batch: 4.041305313029081 \t Current Batch Loss:  3.8105156\n",
      "Done Batch: 3100 \tAverage Loss Per Batch: 4.038250152774873 \t Current Batch Loss:  3.7836235\n",
      "Done Batch: 3150 \tAverage Loss Per Batch: 4.033934572014647 \t Current Batch Loss:  3.675726\n",
      "Done Batch: 3200 \tAverage Loss Per Batch: 4.029701922767351 \t Current Batch Loss:  3.6418786\n",
      "Done Batch: 3250 \tAverage Loss Per Batch: 4.024574692965801 \t Current Batch Loss:  3.5678375\n",
      "Done Batch: 3300 \tAverage Loss Per Batch: 4.020554846324765 \t Current Batch Loss:  4.0292945\n",
      "Done Batch: 3350 \tAverage Loss Per Batch: 4.017763561506621 \t Current Batch Loss:  3.6931405\n",
      "Epoch: 10 \tTime: 4384.520453929901 \tAverage Loss Per Batch:: 4.017584524347267\n",
      "Done Batch: 0 \tAverage Loss Per Batch: 3.06427001953125 \t Current Batch Loss:  3.06427\n",
      "Done Batch: 50 \tAverage Loss Per Batch: 3.673975776223575 \t Current Batch Loss:  3.7690144\n",
      "Done Batch: 100 \tAverage Loss Per Batch: 3.7333725513798175 \t Current Batch Loss:  3.8194788\n",
      "Done Batch: 150 \tAverage Loss Per Batch: 3.741098361299527 \t Current Batch Loss:  3.6024454\n",
      "Done Batch: 200 \tAverage Loss Per Batch: 3.758684534338576 \t Current Batch Loss:  3.8701947\n",
      "Done Batch: 250 \tAverage Loss Per Batch: 3.7829936272594558 \t Current Batch Loss:  3.8550951\n",
      "Done Batch: 300 \tAverage Loss Per Batch: 3.7865906799354425 \t Current Batch Loss:  3.7164066\n",
      "Done Batch: 350 \tAverage Loss Per Batch: 3.793357078166429 \t Current Batch Loss:  3.3255024\n",
      "Done Batch: 400 \tAverage Loss Per Batch: 3.792193540016612 \t Current Batch Loss:  3.4054675\n",
      "Done Batch: 450 \tAverage Loss Per Batch: 3.7839921530493084 \t Current Batch Loss:  3.8887079\n",
      "Done Batch: 500 \tAverage Loss Per Batch: 3.781778337950716 \t Current Batch Loss:  3.7850494\n",
      "Done Batch: 550 \tAverage Loss Per Batch: 3.779754762424531 \t Current Batch Loss:  3.4374812\n",
      "Done Batch: 600 \tAverage Loss Per Batch: 3.780590362041048 \t Current Batch Loss:  3.77382\n",
      "Done Batch: 650 \tAverage Loss Per Batch: 3.783759259225403 \t Current Batch Loss:  3.8005984\n",
      "Done Batch: 700 \tAverage Loss Per Batch: 3.7861510768596522 \t Current Batch Loss:  3.60044\n",
      "Done Batch: 750 \tAverage Loss Per Batch: 3.7831963556266817 \t Current Batch Loss:  4.0455284\n",
      "Done Batch: 800 \tAverage Loss Per Batch: 3.776925698052929 \t Current Batch Loss:  3.6625051\n",
      "Done Batch: 850 \tAverage Loss Per Batch: 3.773472766618471 \t Current Batch Loss:  3.7991247\n",
      "Done Batch: 900 \tAverage Loss Per Batch: 3.7704958928941226 \t Current Batch Loss:  3.9400816\n",
      "Done Batch: 950 \tAverage Loss Per Batch: 3.77916733022744 \t Current Batch Loss:  4.002383\n",
      "Done Batch: 1000 \tAverage Loss Per Batch: 3.7833076752387322 \t Current Batch Loss:  3.5761046\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done Batch: 1050 \tAverage Loss Per Batch: 3.7846177688221156 \t Current Batch Loss:  3.752888\n",
      "Done Batch: 1100 \tAverage Loss Per Batch: 3.7854131164602753 \t Current Batch Loss:  3.7153907\n",
      "Done Batch: 1150 \tAverage Loss Per Batch: 3.7848398496957576 \t Current Batch Loss:  3.8671362\n",
      "Done Batch: 1200 \tAverage Loss Per Batch: 3.7861049488918868 \t Current Batch Loss:  3.7577143\n",
      "Done Batch: 1250 \tAverage Loss Per Batch: 3.7864953471030547 \t Current Batch Loss:  3.9099884\n",
      "Done Batch: 1300 \tAverage Loss Per Batch: 3.7810020373474167 \t Current Batch Loss:  3.5217474\n",
      "Done Batch: 1350 \tAverage Loss Per Batch: 3.777339997245681 \t Current Batch Loss:  3.2638113\n",
      "Done Batch: 1400 \tAverage Loss Per Batch: 3.773348097797805 \t Current Batch Loss:  3.614864\n",
      "Done Batch: 1450 \tAverage Loss Per Batch: 3.768926837376937 \t Current Batch Loss:  3.5935192\n",
      "Done Batch: 1500 \tAverage Loss Per Batch: 3.7649024352480933 \t Current Batch Loss:  3.6733246\n",
      "Done Batch: 1550 \tAverage Loss Per Batch: 3.758615951704103 \t Current Batch Loss:  3.5640574\n",
      "Done Batch: 1600 \tAverage Loss Per Batch: 3.753063179268679 \t Current Batch Loss:  3.7962754\n",
      "Done Batch: 1650 \tAverage Loss Per Batch: 3.748172214578528 \t Current Batch Loss:  3.6435266\n",
      "Done Batch: 1700 \tAverage Loss Per Batch: 3.7427083806806563 \t Current Batch Loss:  3.7827299\n",
      "Done Batch: 1750 \tAverage Loss Per Batch: 3.738000765723681 \t Current Batch Loss:  3.5002968\n",
      "Done Batch: 1800 \tAverage Loss Per Batch: 3.7341745192312255 \t Current Batch Loss:  3.5117903\n",
      "Done Batch: 1850 \tAverage Loss Per Batch: 3.7308275725376534 \t Current Batch Loss:  3.7936532\n",
      "Done Batch: 1900 \tAverage Loss Per Batch: 3.7253861724546495 \t Current Batch Loss:  3.238801\n",
      "Done Batch: 1950 \tAverage Loss Per Batch: 3.7186090353511654 \t Current Batch Loss:  3.4003713\n",
      "Done Batch: 2000 \tAverage Loss Per Batch: 3.710736449869319 \t Current Batch Loss:  3.5578895\n",
      "Done Batch: 2050 \tAverage Loss Per Batch: 3.707459922071668 \t Current Batch Loss:  3.6389475\n",
      "Done Batch: 2100 \tAverage Loss Per Batch: 3.7037933084977235 \t Current Batch Loss:  3.438558\n",
      "Done Batch: 2150 \tAverage Loss Per Batch: 3.6986679792515016 \t Current Batch Loss:  3.4415743\n",
      "Done Batch: 2200 \tAverage Loss Per Batch: 3.695932437592558 \t Current Batch Loss:  3.6886196\n",
      "Done Batch: 2250 \tAverage Loss Per Batch: 3.6944557061358485 \t Current Batch Loss:  3.4999726\n",
      "Done Batch: 2300 \tAverage Loss Per Batch: 3.691540509293982 \t Current Batch Loss:  3.4722226\n",
      "Done Batch: 2350 \tAverage Loss Per Batch: 3.688653109987357 \t Current Batch Loss:  3.604768\n",
      "Done Batch: 2400 \tAverage Loss Per Batch: 3.68522521934128 \t Current Batch Loss:  3.3357105\n",
      "Done Batch: 2450 \tAverage Loss Per Batch: 3.6818285521464853 \t Current Batch Loss:  3.4723349\n",
      "Done Batch: 2500 \tAverage Loss Per Batch: 3.6773522028871555 \t Current Batch Loss:  3.4106355\n",
      "Done Batch: 2550 \tAverage Loss Per Batch: 3.672463231907317 \t Current Batch Loss:  3.2997346\n",
      "Done Batch: 2600 \tAverage Loss Per Batch: 3.6675480755876 \t Current Batch Loss:  3.3047843\n",
      "Done Batch: 2650 \tAverage Loss Per Batch: 3.6632269725670055 \t Current Batch Loss:  3.3370287\n",
      "Done Batch: 2700 \tAverage Loss Per Batch: 3.6612263044486526 \t Current Batch Loss:  3.6611285\n",
      "Done Batch: 2750 \tAverage Loss Per Batch: 3.6569948400076586 \t Current Batch Loss:  3.3977082\n",
      "Done Batch: 2800 \tAverage Loss Per Batch: 3.653243891727239 \t Current Batch Loss:  3.45931\n",
      "Done Batch: 2850 \tAverage Loss Per Batch: 3.649411867727777 \t Current Batch Loss:  3.4977267\n",
      "Done Batch: 2900 \tAverage Loss Per Batch: 3.646203790932267 \t Current Batch Loss:  3.3311875\n",
      "Done Batch: 2950 \tAverage Loss Per Batch: 3.642248108200363 \t Current Batch Loss:  3.2973502\n",
      "Done Batch: 3000 \tAverage Loss Per Batch: 3.638733372375275 \t Current Batch Loss:  3.361548\n",
      "Done Batch: 3050 \tAverage Loss Per Batch: 3.634761589622935 \t Current Batch Loss:  3.4590247\n",
      "Done Batch: 3100 \tAverage Loss Per Batch: 3.630821914912731 \t Current Batch Loss:  3.3390195\n",
      "Done Batch: 3150 \tAverage Loss Per Batch: 3.626145373371358 \t Current Batch Loss:  3.0442796\n",
      "Done Batch: 3200 \tAverage Loss Per Batch: 3.621265753326249 \t Current Batch Loss:  2.9818335\n",
      "Done Batch: 3250 \tAverage Loss Per Batch: 3.6156664085329515 \t Current Batch Loss:  3.071388\n",
      "Done Batch: 3300 \tAverage Loss Per Batch: 3.611025986545774 \t Current Batch Loss:  3.3174036\n",
      "Done Batch: 3350 \tAverage Loss Per Batch: 3.6079279262960506 \t Current Batch Loss:  3.3556669\n",
      "Epoch: 11 \tTime: 4398.797075510025 \tAverage Loss Per Batch:: 3.6072031359733443\n",
      "Done Batch: 0 \tAverage Loss Per Batch: 2.934379816055298 \t Current Batch Loss:  2.9343798\n",
      "Done Batch: 50 \tAverage Loss Per Batch: 3.2942507126752067 \t Current Batch Loss:  3.3397322\n",
      "Done Batch: 100 \tAverage Loss Per Batch: 3.3195110736507 \t Current Batch Loss:  3.691327\n",
      "Done Batch: 150 \tAverage Loss Per Batch: 3.3281193916371326 \t Current Batch Loss:  3.3738909\n",
      "Done Batch: 200 \tAverage Loss Per Batch: 3.3404458956931955 \t Current Batch Loss:  3.4138765\n",
      "Done Batch: 250 \tAverage Loss Per Batch: 3.3619313534512463 \t Current Batch Loss:  3.5019958\n",
      "Done Batch: 300 \tAverage Loss Per Batch: 3.3708599405827315 \t Current Batch Loss:  3.1960514\n",
      "Done Batch: 350 \tAverage Loss Per Batch: 3.3820085885517956 \t Current Batch Loss:  3.0354552\n",
      "Done Batch: 400 \tAverage Loss Per Batch: 3.387559974580037 \t Current Batch Loss:  3.1000586\n",
      "Done Batch: 450 \tAverage Loss Per Batch: 3.379188556100207 \t Current Batch Loss:  3.7685568\n",
      "Done Batch: 500 \tAverage Loss Per Batch: 3.3806978371328937 \t Current Batch Loss:  3.4199848\n",
      "Done Batch: 550 \tAverage Loss Per Batch: 3.3771563978247117 \t Current Batch Loss:  3.2426941\n",
      "Done Batch: 600 \tAverage Loss Per Batch: 3.379573884303875 \t Current Batch Loss:  3.386499\n",
      "Done Batch: 650 \tAverage Loss Per Batch: 3.384308793394613 \t Current Batch Loss:  3.399272\n",
      "Done Batch: 700 \tAverage Loss Per Batch: 3.38500271578148 \t Current Batch Loss:  3.1786797\n",
      "Done Batch: 750 \tAverage Loss Per Batch: 3.3803528437760475 \t Current Batch Loss:  3.564636\n",
      "Done Batch: 800 \tAverage Loss Per Batch: 3.374043642656038 \t Current Batch Loss:  3.2958164\n",
      "Done Batch: 850 \tAverage Loss Per Batch: 3.3706888849950984 \t Current Batch Loss:  3.538519\n",
      "Done Batch: 900 \tAverage Loss Per Batch: 3.366520864717439 \t Current Batch Loss:  3.4552119\n",
      "Done Batch: 950 \tAverage Loss Per Batch: 3.3774565993548693 \t Current Batch Loss:  3.6195886\n",
      "Done Batch: 1000 \tAverage Loss Per Batch: 3.384179942972296 \t Current Batch Loss:  3.112034\n",
      "Done Batch: 1050 \tAverage Loss Per Batch: 3.3858057885483035 \t Current Batch Loss:  3.3406396\n",
      "Done Batch: 1100 \tAverage Loss Per Batch: 3.387974974461624 \t Current Batch Loss:  3.1867988\n",
      "Done Batch: 1150 \tAverage Loss Per Batch: 3.389429134042443 \t Current Batch Loss:  3.3293464\n",
      "Done Batch: 1200 \tAverage Loss Per Batch: 3.388566568034773 \t Current Batch Loss:  3.4107132\n",
      "Done Batch: 1250 \tAverage Loss Per Batch: 3.3888847502015476 \t Current Batch Loss:  3.4287188\n",
      "Done Batch: 1300 \tAverage Loss Per Batch: 3.384717448320323 \t Current Batch Loss:  3.121389\n",
      "Done Batch: 1350 \tAverage Loss Per Batch: 3.3799116103230893 \t Current Batch Loss:  2.8764434\n",
      "Done Batch: 1400 \tAverage Loss Per Batch: 3.374210518653864 \t Current Batch Loss:  3.2034311\n",
      "Done Batch: 1450 \tAverage Loss Per Batch: 3.369653185181253 \t Current Batch Loss:  3.231771\n",
      "Done Batch: 1500 \tAverage Loss Per Batch: 3.3635616356495777 \t Current Batch Loss:  3.2744956\n",
      "Done Batch: 1550 \tAverage Loss Per Batch: 3.3583427756006374 \t Current Batch Loss:  3.4854834\n",
      "Done Batch: 1600 \tAverage Loss Per Batch: 3.352178821557764 \t Current Batch Loss:  3.2281759\n",
      "Done Batch: 1650 \tAverage Loss Per Batch: 3.346776554326012 \t Current Batch Loss:  3.264701\n",
      "Done Batch: 1700 \tAverage Loss Per Batch: 3.3398919315775446 \t Current Batch Loss:  3.415296\n",
      "Done Batch: 1750 \tAverage Loss Per Batch: 3.335707305568344 \t Current Batch Loss:  3.3393805\n",
      "Done Batch: 1800 \tAverage Loss Per Batch: 3.331804867521516 \t Current Batch Loss:  3.2342007\n",
      "Done Batch: 1850 \tAverage Loss Per Batch: 3.326469792603545 \t Current Batch Loss:  3.1943696\n",
      "Done Batch: 1900 \tAverage Loss Per Batch: 3.3202687961562316 \t Current Batch Loss:  3.052851\n",
      "Done Batch: 1950 \tAverage Loss Per Batch: 3.3125958550226864 \t Current Batch Loss:  3.1595354\n",
      "Done Batch: 2000 \tAverage Loss Per Batch: 3.3043608204356913 \t Current Batch Loss:  3.3164651\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done Batch: 2050 \tAverage Loss Per Batch: 3.300300653825906 \t Current Batch Loss:  3.5370343\n",
      "Done Batch: 2100 \tAverage Loss Per Batch: 3.2964939123559485 \t Current Batch Loss:  3.0834434\n",
      "Done Batch: 2150 \tAverage Loss Per Batch: 3.2910718689514615 \t Current Batch Loss:  2.9331481\n",
      "Done Batch: 2200 \tAverage Loss Per Batch: 3.28678814631492 \t Current Batch Loss:  3.0527804\n",
      "Done Batch: 2250 \tAverage Loss Per Batch: 3.2857840852385785 \t Current Batch Loss:  3.0491037\n",
      "Done Batch: 2300 \tAverage Loss Per Batch: 3.2830487343084807 \t Current Batch Loss:  3.142734\n",
      "Done Batch: 2350 \tAverage Loss Per Batch: 3.2798529562672267 \t Current Batch Loss:  3.211615\n",
      "Done Batch: 2400 \tAverage Loss Per Batch: 3.2753590549443574 \t Current Batch Loss:  3.0449734\n",
      "Done Batch: 2450 \tAverage Loss Per Batch: 3.272822058672615 \t Current Batch Loss:  2.9658227\n",
      "Done Batch: 2500 \tAverage Loss Per Batch: 3.26876667040627 \t Current Batch Loss:  3.2200894\n",
      "Done Batch: 2550 \tAverage Loss Per Batch: 3.2640687850165584 \t Current Batch Loss:  3.1140242\n",
      "Done Batch: 2600 \tAverage Loss Per Batch: 3.2594948657335387 \t Current Batch Loss:  2.9212496\n",
      "Done Batch: 2650 \tAverage Loss Per Batch: 3.255595033279413 \t Current Batch Loss:  2.778428\n",
      "Done Batch: 2700 \tAverage Loss Per Batch: 3.2539289623487884 \t Current Batch Loss:  3.2520614\n",
      "Done Batch: 2750 \tAverage Loss Per Batch: 3.2489785061277767 \t Current Batch Loss:  3.1527753\n",
      "Done Batch: 2800 \tAverage Loss Per Batch: 3.243807841692172 \t Current Batch Loss:  2.942931\n",
      "Done Batch: 2850 \tAverage Loss Per Batch: 3.2398710561944326 \t Current Batch Loss:  2.7829604\n",
      "Done Batch: 2900 \tAverage Loss Per Batch: 3.2372965093564674 \t Current Batch Loss:  3.1710136\n",
      "Done Batch: 2950 \tAverage Loss Per Batch: 3.2328646851975567 \t Current Batch Loss:  3.060939\n",
      "Done Batch: 3000 \tAverage Loss Per Batch: 3.2293310904256587 \t Current Batch Loss:  2.9848962\n",
      "Done Batch: 3050 \tAverage Loss Per Batch: 3.225250977682153 \t Current Batch Loss:  3.0011742\n",
      "Done Batch: 3100 \tAverage Loss Per Batch: 3.2214348722911197 \t Current Batch Loss:  2.863657\n",
      "Done Batch: 3150 \tAverage Loss Per Batch: 3.216509020165843 \t Current Batch Loss:  2.7389948\n",
      "Done Batch: 3200 \tAverage Loss Per Batch: 3.212000323399571 \t Current Batch Loss:  2.7456446\n",
      "Done Batch: 3250 \tAverage Loss Per Batch: 3.207060051272371 \t Current Batch Loss:  2.6126351\n",
      "Done Batch: 3300 \tAverage Loss Per Batch: 3.2033662640011986 \t Current Batch Loss:  2.866477\n",
      "Done Batch: 3350 \tAverage Loss Per Batch: 3.1999472939267295 \t Current Batch Loss:  2.9218903\n",
      "Epoch: 12 \tTime: 4386.032041549683 \tAverage Loss Per Batch:: 3.198785851679395\n",
      "Done Batch: 0 \tAverage Loss Per Batch: 2.6067380905151367 \t Current Batch Loss:  2.606738\n",
      "Done Batch: 50 \tAverage Loss Per Batch: 2.9162477324990665 \t Current Batch Loss:  3.1018226\n",
      "Done Batch: 100 \tAverage Loss Per Batch: 2.9228609151179246 \t Current Batch Loss:  3.0454524\n",
      "Done Batch: 150 \tAverage Loss Per Batch: 2.920046188973433 \t Current Batch Loss:  2.8701818\n",
      "Done Batch: 200 \tAverage Loss Per Batch: 2.931932865683712 \t Current Batch Loss:  2.7602339\n",
      "Done Batch: 250 \tAverage Loss Per Batch: 2.947430198411068 \t Current Batch Loss:  3.3327248\n",
      "Done Batch: 300 \tAverage Loss Per Batch: 2.955745444345316 \t Current Batch Loss:  2.9265754\n",
      "Done Batch: 350 \tAverage Loss Per Batch: 2.963908766069983 \t Current Batch Loss:  2.7912402\n",
      "Done Batch: 400 \tAverage Loss Per Batch: 2.974527392898712 \t Current Batch Loss:  2.9598107\n",
      "Done Batch: 450 \tAverage Loss Per Batch: 2.9668926061389187 \t Current Batch Loss:  3.4483612\n",
      "Done Batch: 500 \tAverage Loss Per Batch: 2.970312651997793 \t Current Batch Loss:  2.7308512\n",
      "Done Batch: 550 \tAverage Loss Per Batch: 2.965134465759333 \t Current Batch Loss:  2.683806\n",
      "Done Batch: 600 \tAverage Loss Per Batch: 2.966858053366079 \t Current Batch Loss:  3.0797014\n",
      "Done Batch: 650 \tAverage Loss Per Batch: 2.9686056428607523 \t Current Batch Loss:  3.2040107\n",
      "Done Batch: 700 \tAverage Loss Per Batch: 2.9710068988391916 \t Current Batch Loss:  2.8086646\n",
      "Done Batch: 750 \tAverage Loss Per Batch: 2.9643816141567916 \t Current Batch Loss:  2.878456\n",
      "Done Batch: 800 \tAverage Loss Per Batch: 2.95711315794384 \t Current Batch Loss:  2.7769952\n",
      "Done Batch: 850 \tAverage Loss Per Batch: 2.9560792014685697 \t Current Batch Loss:  3.0276594\n",
      "Done Batch: 900 \tAverage Loss Per Batch: 2.9531526589367156 \t Current Batch Loss:  2.8489316\n",
      "Done Batch: 950 \tAverage Loss Per Batch: 2.9657685894570016 \t Current Batch Loss:  3.204615\n",
      "Done Batch: 1000 \tAverage Loss Per Batch: 2.9718194284162798 \t Current Batch Loss:  2.8631384\n",
      "Done Batch: 1050 \tAverage Loss Per Batch: 2.9747917155330232 \t Current Batch Loss:  2.7026258\n",
      "Done Batch: 1100 \tAverage Loss Per Batch: 2.9782591849213618 \t Current Batch Loss:  2.6160882\n",
      "Done Batch: 1150 \tAverage Loss Per Batch: 2.9791607645467297 \t Current Batch Loss:  2.8672538\n",
      "Done Batch: 1200 \tAverage Loss Per Batch: 2.9785590368345516 \t Current Batch Loss:  2.9354794\n",
      "Done Batch: 1250 \tAverage Loss Per Batch: 2.97870385961281 \t Current Batch Loss:  2.9864287\n",
      "Done Batch: 1300 \tAverage Loss Per Batch: 2.9747219490693406 \t Current Batch Loss:  2.8607655\n",
      "Done Batch: 1350 \tAverage Loss Per Batch: 2.9701043488271672 \t Current Batch Loss:  2.4484258\n",
      "Done Batch: 1400 \tAverage Loss Per Batch: 2.9652641534294766 \t Current Batch Loss:  2.9505486\n",
      "Done Batch: 1450 \tAverage Loss Per Batch: 2.960317346328378 \t Current Batch Loss:  2.7133863\n",
      "Done Batch: 1500 \tAverage Loss Per Batch: 2.9554824978411 \t Current Batch Loss:  3.2068238\n",
      "Done Batch: 1550 \tAverage Loss Per Batch: 2.9510661971561376 \t Current Batch Loss:  2.780823\n",
      "Done Batch: 1600 \tAverage Loss Per Batch: 2.9443681856604536 \t Current Batch Loss:  2.5952501\n",
      "Done Batch: 1650 \tAverage Loss Per Batch: 2.9382855489281434 \t Current Batch Loss:  2.9497378\n",
      "Done Batch: 1700 \tAverage Loss Per Batch: 2.930720300410931 \t Current Batch Loss:  2.816431\n",
      "Done Batch: 1750 \tAverage Loss Per Batch: 2.9244005236198123 \t Current Batch Loss:  2.855167\n",
      "Done Batch: 1800 \tAverage Loss Per Batch: 2.91934443539477 \t Current Batch Loss:  2.708243\n",
      "Done Batch: 1850 \tAverage Loss Per Batch: 2.913803601252073 \t Current Batch Loss:  3.009309\n",
      "Done Batch: 1900 \tAverage Loss Per Batch: 2.9067180573093707 \t Current Batch Loss:  2.4359448\n",
      "Done Batch: 1950 \tAverage Loss Per Batch: 2.8984162251439725 \t Current Batch Loss:  2.739604\n",
      "Done Batch: 2000 \tAverage Loss Per Batch: 2.88963112719115 \t Current Batch Loss:  2.81659\n",
      "Done Batch: 2050 \tAverage Loss Per Batch: 2.8854670826834274 \t Current Batch Loss:  3.1345131\n",
      "Done Batch: 2100 \tAverage Loss Per Batch: 2.8813017307719067 \t Current Batch Loss:  2.6303878\n",
      "Done Batch: 2150 \tAverage Loss Per Batch: 2.8761850435309273 \t Current Batch Loss:  2.6612885\n",
      "Done Batch: 2200 \tAverage Loss Per Batch: 2.871440022926556 \t Current Batch Loss:  2.7857733\n",
      "Done Batch: 2250 \tAverage Loss Per Batch: 2.869487200774706 \t Current Batch Loss:  2.5627267\n",
      "Done Batch: 2300 \tAverage Loss Per Batch: 2.8671418500433585 \t Current Batch Loss:  2.62587\n",
      "Done Batch: 2350 \tAverage Loss Per Batch: 2.864301867507357 \t Current Batch Loss:  2.6618972\n",
      "Done Batch: 2400 \tAverage Loss Per Batch: 2.8593453475208594 \t Current Batch Loss:  2.3588488\n",
      "Done Batch: 2450 \tAverage Loss Per Batch: 2.856716838674806 \t Current Batch Loss:  2.7390795\n",
      "Done Batch: 2500 \tAverage Loss Per Batch: 2.8529150982658082 \t Current Batch Loss:  2.933782\n",
      "Done Batch: 2550 \tAverage Loss Per Batch: 2.848241405457153 \t Current Batch Loss:  2.4709208\n",
      "Done Batch: 2600 \tAverage Loss Per Batch: 2.844046150982632 \t Current Batch Loss:  2.6064575\n",
      "Done Batch: 2650 \tAverage Loss Per Batch: 2.8411509857587838 \t Current Batch Loss:  2.6145399\n",
      "Done Batch: 2700 \tAverage Loss Per Batch: 2.8391327175640346 \t Current Batch Loss:  2.695965\n",
      "Done Batch: 2750 \tAverage Loss Per Batch: 2.8345305162185066 \t Current Batch Loss:  2.6927614\n",
      "Done Batch: 2800 \tAverage Loss Per Batch: 2.829967086121935 \t Current Batch Loss:  2.6319776\n",
      "Done Batch: 2850 \tAverage Loss Per Batch: 2.8255005426383444 \t Current Batch Loss:  2.524543\n",
      "Done Batch: 2900 \tAverage Loss Per Batch: 2.822242888447664 \t Current Batch Loss:  2.4955523\n",
      "Done Batch: 2950 \tAverage Loss Per Batch: 2.818803078358313 \t Current Batch Loss:  2.8308747\n",
      "Done Batch: 3000 \tAverage Loss Per Batch: 2.8159641452885915 \t Current Batch Loss:  2.5943255\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done Batch: 3050 \tAverage Loss Per Batch: 2.813434443056525 \t Current Batch Loss:  2.8334942\n",
      "Done Batch: 3100 \tAverage Loss Per Batch: 2.809581152011179 \t Current Batch Loss:  2.323589\n",
      "Done Batch: 3150 \tAverage Loss Per Batch: 2.8052961343207157 \t Current Batch Loss:  2.3209198\n",
      "Done Batch: 3200 \tAverage Loss Per Batch: 2.80128329454158 \t Current Batch Loss:  2.3472877\n",
      "Done Batch: 3250 \tAverage Loss Per Batch: 2.797041037272615 \t Current Batch Loss:  2.6247506\n",
      "Done Batch: 3300 \tAverage Loss Per Batch: 2.7938191857131387 \t Current Batch Loss:  2.6098654\n",
      "Done Batch: 3350 \tAverage Loss Per Batch: 2.79076944735825 \t Current Batch Loss:  2.6089673\n",
      "Epoch: 13 \tTime: 4394.65519452095 \tAverage Loss Per Batch:: 2.7896592189871243\n",
      "Done Batch: 0 \tAverage Loss Per Batch: 2.3154306411743164 \t Current Batch Loss:  2.3154306\n",
      "Done Batch: 50 \tAverage Loss Per Batch: 2.5303442150938746 \t Current Batch Loss:  2.5699573\n",
      "Done Batch: 100 \tAverage Loss Per Batch: 2.523738157631147 \t Current Batch Loss:  2.8122451\n",
      "Done Batch: 150 \tAverage Loss Per Batch: 2.515557353859706 \t Current Batch Loss:  2.3097527\n",
      "Done Batch: 200 \tAverage Loss Per Batch: 2.524267304596023 \t Current Batch Loss:  2.6761215\n",
      "Done Batch: 250 \tAverage Loss Per Batch: 2.5387104326985273 \t Current Batch Loss:  2.7064703\n",
      "Done Batch: 300 \tAverage Loss Per Batch: 2.5485837942738074 \t Current Batch Loss:  2.6037652\n",
      "Done Batch: 350 \tAverage Loss Per Batch: 2.559995454940361 \t Current Batch Loss:  2.30193\n",
      "Done Batch: 400 \tAverage Loss Per Batch: 2.5733684714595575 \t Current Batch Loss:  2.2117414\n",
      "Done Batch: 450 \tAverage Loss Per Batch: 2.5608843530625305 \t Current Batch Loss:  2.7267656\n",
      "Done Batch: 500 \tAverage Loss Per Batch: 2.569393135116486 \t Current Batch Loss:  2.5621827\n",
      "Done Batch: 550 \tAverage Loss Per Batch: 2.5666635927833794 \t Current Batch Loss:  2.457331\n",
      "Done Batch: 600 \tAverage Loss Per Batch: 2.5713908430343855 \t Current Batch Loss:  2.6036713\n",
      "Done Batch: 650 \tAverage Loss Per Batch: 2.57120411187273 \t Current Batch Loss:  2.5284007\n",
      "Done Batch: 700 \tAverage Loss Per Batch: 2.5721721771610278 \t Current Batch Loss:  2.5779634\n",
      "Done Batch: 750 \tAverage Loss Per Batch: 2.568189340647305 \t Current Batch Loss:  2.5169497\n",
      "Done Batch: 800 \tAverage Loss Per Batch: 2.5610994444357766 \t Current Batch Loss:  2.4013152\n",
      "Done Batch: 850 \tAverage Loss Per Batch: 2.562204954066652 \t Current Batch Loss:  2.5855038\n",
      "Done Batch: 900 \tAverage Loss Per Batch: 2.56054151944659 \t Current Batch Loss:  2.76095\n",
      "Done Batch: 950 \tAverage Loss Per Batch: 2.572600507836487 \t Current Batch Loss:  2.8359556\n",
      "Done Batch: 1000 \tAverage Loss Per Batch: 2.5808260786187995 \t Current Batch Loss:  2.3529115\n",
      "Done Batch: 1050 \tAverage Loss Per Batch: 2.5864348790398335 \t Current Batch Loss:  2.5248427\n",
      "Done Batch: 1100 \tAverage Loss Per Batch: 2.5908873971216684 \t Current Batch Loss:  2.3168015\n",
      "Done Batch: 1150 \tAverage Loss Per Batch: 2.593593350086494 \t Current Batch Loss:  2.6108005\n",
      "Done Batch: 1200 \tAverage Loss Per Batch: 2.594402977667879 \t Current Batch Loss:  2.5954342\n",
      "Done Batch: 1250 \tAverage Loss Per Batch: 2.5942071905906063 \t Current Batch Loss:  2.6609313\n",
      "Done Batch: 1300 \tAverage Loss Per Batch: 2.590719028585787 \t Current Batch Loss:  2.3061037\n",
      "Done Batch: 1350 \tAverage Loss Per Batch: 2.5877398391020554 \t Current Batch Loss:  2.2852592\n",
      "Done Batch: 1400 \tAverage Loss Per Batch: 2.581822049949613 \t Current Batch Loss:  2.506833\n",
      "Done Batch: 1450 \tAverage Loss Per Batch: 2.5770809094220666 \t Current Batch Loss:  2.3356879\n",
      "Done Batch: 1500 \tAverage Loss Per Batch: 2.570668072084202 \t Current Batch Loss:  2.5823786\n",
      "Done Batch: 1550 \tAverage Loss Per Batch: 2.564733210953492 \t Current Batch Loss:  2.4940953\n",
      "Done Batch: 1600 \tAverage Loss Per Batch: 2.5583351729141035 \t Current Batch Loss:  2.4968576\n",
      "Done Batch: 1650 \tAverage Loss Per Batch: 2.5524866970866325 \t Current Batch Loss:  2.591158\n",
      "Done Batch: 1700 \tAverage Loss Per Batch: 2.546670975718759 \t Current Batch Loss:  2.5967622\n",
      "Done Batch: 1750 \tAverage Loss Per Batch: 2.5405421366628955 \t Current Batch Loss:  2.5411277\n",
      "Done Batch: 1800 \tAverage Loss Per Batch: 2.5357184375014192 \t Current Batch Loss:  2.3446589\n",
      "Done Batch: 1850 \tAverage Loss Per Batch: 2.530742449028823 \t Current Batch Loss:  2.6172354\n",
      "Done Batch: 1900 \tAverage Loss Per Batch: 2.5244577063942257 \t Current Batch Loss:  1.9803656\n",
      "Done Batch: 1950 \tAverage Loss Per Batch: 2.5164212593966298 \t Current Batch Loss:  2.379284\n",
      "Done Batch: 2000 \tAverage Loss Per Batch: 2.507985939686445 \t Current Batch Loss:  2.5393875\n",
      "Done Batch: 2050 \tAverage Loss Per Batch: 2.5035565466139156 \t Current Batch Loss:  2.461068\n",
      "Done Batch: 2100 \tAverage Loss Per Batch: 2.499308111609759 \t Current Batch Loss:  2.1864076\n",
      "Done Batch: 2150 \tAverage Loss Per Batch: 2.4936152980361967 \t Current Batch Loss:  2.0513678\n",
      "Done Batch: 2200 \tAverage Loss Per Batch: 2.48882463860111 \t Current Batch Loss:  2.2589002\n",
      "Done Batch: 2250 \tAverage Loss Per Batch: 2.485957832719844 \t Current Batch Loss:  2.1710563\n",
      "Done Batch: 2300 \tAverage Loss Per Batch: 2.4832708181271808 \t Current Batch Loss:  2.3685627\n",
      "Done Batch: 2350 \tAverage Loss Per Batch: 2.4812145131235677 \t Current Batch Loss:  2.210087\n",
      "Done Batch: 2400 \tAverage Loss Per Batch: 2.4760651620712344 \t Current Batch Loss:  2.0188677\n",
      "Done Batch: 2450 \tAverage Loss Per Batch: 2.473216734385792 \t Current Batch Loss:  2.1560907\n",
      "Done Batch: 2500 \tAverage Loss Per Batch: 2.4690189805806804 \t Current Batch Loss:  2.3003824\n",
      "Done Batch: 2550 \tAverage Loss Per Batch: 2.4637340606310376 \t Current Batch Loss:  2.0958874\n",
      "Done Batch: 2600 \tAverage Loss Per Batch: 2.4596390869469884 \t Current Batch Loss:  2.2129002\n",
      "Done Batch: 2650 \tAverage Loss Per Batch: 2.4561406847037537 \t Current Batch Loss:  2.2397783\n",
      "Done Batch: 2700 \tAverage Loss Per Batch: 2.45382234208101 \t Current Batch Loss:  2.407592\n",
      "Done Batch: 2750 \tAverage Loss Per Batch: 2.449836549564779 \t Current Batch Loss:  2.3393152\n",
      "Done Batch: 2800 \tAverage Loss Per Batch: 2.445370043868978 \t Current Batch Loss:  2.1843553\n",
      "Done Batch: 2850 \tAverage Loss Per Batch: 2.4413864073607847 \t Current Batch Loss:  2.2405608\n",
      "Done Batch: 2900 \tAverage Loss Per Batch: 2.4378085243254026 \t Current Batch Loss:  2.0286703\n",
      "Done Batch: 2950 \tAverage Loss Per Batch: 2.4336098654316225 \t Current Batch Loss:  2.128183\n",
      "Done Batch: 3000 \tAverage Loss Per Batch: 2.4313141343912177 \t Current Batch Loss:  2.3622563\n",
      "Done Batch: 3050 \tAverage Loss Per Batch: 2.4298104625653143 \t Current Batch Loss:  2.4485734\n",
      "Done Batch: 3100 \tAverage Loss Per Batch: 2.426715558707733 \t Current Batch Loss:  2.054703\n",
      "Done Batch: 3150 \tAverage Loss Per Batch: 2.4224144825667513 \t Current Batch Loss:  1.9837799\n",
      "Done Batch: 3200 \tAverage Loss Per Batch: 2.4185846341993242 \t Current Batch Loss:  2.0010817\n",
      "Done Batch: 3250 \tAverage Loss Per Batch: 2.4146236247629136 \t Current Batch Loss:  2.1961596\n",
      "Done Batch: 3300 \tAverage Loss Per Batch: 2.411274092409041 \t Current Batch Loss:  2.3572361\n",
      "Done Batch: 3350 \tAverage Loss Per Batch: 2.408444725624023 \t Current Batch Loss:  2.1549668\n",
      "Epoch: 14 \tTime: 4403.981489658356 \tAverage Loss Per Batch:: 2.4071397602487328\n"
     ]
    }
   ],
   "source": [
    "def train(model, learning_rate=0.0001, batch_size=100, epochs=10):\n",
    "    \n",
    "    # defining criterion and optimizer\n",
    "    criterion = nn.NLLLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr = learning_rate)\n",
    "    \n",
    "    number_of_batches = math.ceil(len(train_image_addresses)/batch_size)\n",
    "    indexes = np.arange(len(train_image_addresses))\n",
    "    \n",
    "    loss_arr = []\n",
    "    for epoch in range(10, 10 + epochs):\n",
    "        avgloss = 0.0\n",
    "        start = time.time()\n",
    "        avg_loss_arr = []\n",
    "        for batch in range(number_of_batches):\n",
    "            train_indexes = [train_image_addresses[i] for i in indexes[batch*batch_size:(batch+1)*batch_size]]\n",
    "            image_batch, target_batch = load_batch(train_indexes)\n",
    "            loss = train_pass(image_batch, target_batch, model, optimizer, criterion)\n",
    "            avgloss += loss\n",
    "            if batch%50 == 0:\n",
    "                print (\"Done Batch:\", batch, \"\\tAverage Loss Per Batch:\", avgloss/(batch+1), \"\\t Current Batch Loss: \", loss)\n",
    "        loss_arr.append(avgloss/(batch+1))\n",
    "        print (\"Epoch:\",epoch, \"\\tTime:\", time.time() - start, \"\\tAverage Loss Per Batch::\", avgloss/(batch+1))\n",
    "        torch.save({'epoch': epoch ,'state_dict': model.state_dict(),'optimizer': optimizer.state_dict()}, open(\"outputs/anp_log_softmax_classifier_batch_\"+str(epoch), \"wb+\"))\n",
    "    loss_arr = np.array(loss_arr)\n",
    "    np.save(open('outputs/loss_log_softmax_anp_classifier', 'wb+'), loss_arr)\n",
    "    \n",
    "train(main_model, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done Batch: 0 \tAverage Loss Per Batch: 1.9522595405578613 \t Current Batch Loss:  1.9522595\n",
      "Done Batch: 50 \tAverage Loss Per Batch: 2.2286404651754044 \t Current Batch Loss:  2.3930936\n",
      "Done Batch: 100 \tAverage Loss Per Batch: 2.2371898075141528 \t Current Batch Loss:  2.2350478\n",
      "Done Batch: 150 \tAverage Loss Per Batch: 2.2297276937408954 \t Current Batch Loss:  2.1392846\n",
      "Done Batch: 200 \tAverage Loss Per Batch: 2.226523348941139 \t Current Batch Loss:  2.26777\n",
      "Done Batch: 250 \tAverage Loss Per Batch: 2.2351838274306037 \t Current Batch Loss:  2.3810325\n",
      "Done Batch: 300 \tAverage Loss Per Batch: 2.23554566967923 \t Current Batch Loss:  2.206349\n",
      "Done Batch: 350 \tAverage Loss Per Batch: 2.2408747367369823 \t Current Batch Loss:  1.9449487\n",
      "Done Batch: 400 \tAverage Loss Per Batch: 2.2473033223663483 \t Current Batch Loss:  1.8010334\n",
      "Done Batch: 450 \tAverage Loss Per Batch: 2.231427931732719 \t Current Batch Loss:  2.262139\n",
      "Done Batch: 500 \tAverage Loss Per Batch: 2.234071563579841 \t Current Batch Loss:  2.1867678\n",
      "Done Batch: 550 \tAverage Loss Per Batch: 2.229335220669229 \t Current Batch Loss:  2.1991446\n",
      "Done Batch: 600 \tAverage Loss Per Batch: 2.2330894632863125 \t Current Batch Loss:  2.401075\n",
      "Done Batch: 650 \tAverage Loss Per Batch: 2.2309641471839354 \t Current Batch Loss:  2.1271722\n",
      "Done Batch: 700 \tAverage Loss Per Batch: 2.2301204891585757 \t Current Batch Loss:  2.373422\n",
      "Done Batch: 750 \tAverage Loss Per Batch: 2.2242754361283446 \t Current Batch Loss:  1.9577981\n",
      "Done Batch: 800 \tAverage Loss Per Batch: 2.2159317987837297 \t Current Batch Loss:  1.9904819\n",
      "Done Batch: 850 \tAverage Loss Per Batch: 2.2157574987299435 \t Current Batch Loss:  2.2905223\n",
      "Done Batch: 900 \tAverage Loss Per Batch: 2.2149418419129843 \t Current Batch Loss:  2.3626294\n",
      "Done Batch: 950 \tAverage Loss Per Batch: 2.2258119265990555 \t Current Batch Loss:  2.3727114\n",
      "Done Batch: 1000 \tAverage Loss Per Batch: 2.233080033774857 \t Current Batch Loss:  2.027606\n",
      "Done Batch: 1050 \tAverage Loss Per Batch: 2.2364672521088487 \t Current Batch Loss:  2.3436809\n",
      "Done Batch: 1100 \tAverage Loss Per Batch: 2.241647157205656 \t Current Batch Loss:  2.1222274\n",
      "Done Batch: 1150 \tAverage Loss Per Batch: 2.2438718306303644 \t Current Batch Loss:  2.1310487\n",
      "Done Batch: 1200 \tAverage Loss Per Batch: 2.2445271404260003 \t Current Batch Loss:  2.2551775\n",
      "Done Batch: 1250 \tAverage Loss Per Batch: 2.245026377274645 \t Current Batch Loss:  2.4251049\n",
      "Done Batch: 1300 \tAverage Loss Per Batch: 2.24126403099019 \t Current Batch Loss:  2.1000624\n",
      "Done Batch: 1350 \tAverage Loss Per Batch: 2.237983873471077 \t Current Batch Loss:  1.9573027\n",
      "Done Batch: 1400 \tAverage Loss Per Batch: 2.232542990957474 \t Current Batch Loss:  2.0723424\n",
      "Done Batch: 1450 \tAverage Loss Per Batch: 2.2260638467202427 \t Current Batch Loss:  1.8853889\n",
      "Done Batch: 1500 \tAverage Loss Per Batch: 2.2195846898646296 \t Current Batch Loss:  2.3110414\n",
      "Done Batch: 1550 \tAverage Loss Per Batch: 2.2151456834576653 \t Current Batch Loss:  2.1614618\n",
      "Done Batch: 1600 \tAverage Loss Per Batch: 2.208843503722096 \t Current Batch Loss:  2.0240846\n",
      "Done Batch: 1650 \tAverage Loss Per Batch: 2.2038554983526053 \t Current Batch Loss:  2.0864673\n",
      "Done Batch: 1700 \tAverage Loss Per Batch: 2.1983455018672013 \t Current Batch Loss:  2.1096318\n",
      "Done Batch: 1750 \tAverage Loss Per Batch: 2.1917572002421783 \t Current Batch Loss:  2.2202568\n",
      "Done Batch: 1800 \tAverage Loss Per Batch: 2.1864657162428562 \t Current Batch Loss:  2.1018229\n",
      "Done Batch: 1850 \tAverage Loss Per Batch: 2.180943229081243 \t Current Batch Loss:  2.3846378\n",
      "Done Batch: 1900 \tAverage Loss Per Batch: 2.1754968895653812 \t Current Batch Loss:  1.8739395\n",
      "Done Batch: 1950 \tAverage Loss Per Batch: 2.1680377064943923 \t Current Batch Loss:  2.1520443\n",
      "Done Batch: 2000 \tAverage Loss Per Batch: 2.1602355708246646 \t Current Batch Loss:  1.9432359\n",
      "Done Batch: 2050 \tAverage Loss Per Batch: 2.155492704592351 \t Current Batch Loss:  2.1087\n",
      "Done Batch: 2100 \tAverage Loss Per Batch: 2.1506011541090597 \t Current Batch Loss:  1.717825\n",
      "Done Batch: 2150 \tAverage Loss Per Batch: 2.145120530059758 \t Current Batch Loss:  1.90558\n",
      "Done Batch: 2200 \tAverage Loss Per Batch: 2.140295817006019 \t Current Batch Loss:  1.8145294\n",
      "Done Batch: 2250 \tAverage Loss Per Batch: 2.1369409112070255 \t Current Batch Loss:  1.950453\n",
      "Done Batch: 2300 \tAverage Loss Per Batch: 2.1348446002476944 \t Current Batch Loss:  1.8292917\n",
      "Done Batch: 2350 \tAverage Loss Per Batch: 2.132301009700228 \t Current Batch Loss:  1.9383113\n",
      "Done Batch: 2400 \tAverage Loss Per Batch: 2.1274351785700305 \t Current Batch Loss:  1.6183836\n",
      "Done Batch: 2450 \tAverage Loss Per Batch: 2.1243071322633704 \t Current Batch Loss:  1.6581062\n",
      "Done Batch: 2500 \tAverage Loss Per Batch: 2.120807334357669 \t Current Batch Loss:  1.9779412\n",
      "Done Batch: 2550 \tAverage Loss Per Batch: 2.1159230750572537 \t Current Batch Loss:  1.7546848\n",
      "Done Batch: 2600 \tAverage Loss Per Batch: 2.1116026306738993 \t Current Batch Loss:  1.8432367\n",
      "Done Batch: 2650 \tAverage Loss Per Batch: 2.1075563296332804 \t Current Batch Loss:  1.8186073\n",
      "Done Batch: 2700 \tAverage Loss Per Batch: 2.106050054614785 \t Current Batch Loss:  2.0454655\n",
      "Done Batch: 2750 \tAverage Loss Per Batch: 2.1024682739785434 \t Current Batch Loss:  1.9496123\n",
      "Done Batch: 2800 \tAverage Loss Per Batch: 2.0980002221274656 \t Current Batch Loss:  1.8701009\n",
      "Done Batch: 2850 \tAverage Loss Per Batch: 2.093683221724024 \t Current Batch Loss:  1.66487\n",
      "Done Batch: 2900 \tAverage Loss Per Batch: 2.090468618714452 \t Current Batch Loss:  1.7672218\n",
      "Done Batch: 2950 \tAverage Loss Per Batch: 2.0869746774142173 \t Current Batch Loss:  1.9464074\n",
      "Done Batch: 3000 \tAverage Loss Per Batch: 2.084167210946914 \t Current Batch Loss:  1.906412\n",
      "Done Batch: 3050 \tAverage Loss Per Batch: 2.0828346670435827 \t Current Batch Loss:  2.0450568\n",
      "Done Batch: 3100 \tAverage Loss Per Batch: 2.080778995393669 \t Current Batch Loss:  1.6482749\n",
      "Done Batch: 3150 \tAverage Loss Per Batch: 2.0772433108808275 \t Current Batch Loss:  1.7748592\n",
      "Done Batch: 3200 \tAverage Loss Per Batch: 2.0737516570188075 \t Current Batch Loss:  1.464447\n",
      "Done Batch: 3250 \tAverage Loss Per Batch: 2.06988411223254 \t Current Batch Loss:  1.8746191\n",
      "Done Batch: 3300 \tAverage Loss Per Batch: 2.0678568660978187 \t Current Batch Loss:  2.0026324\n",
      "Done Batch: 3350 \tAverage Loss Per Batch: 2.065713401197925 \t Current Batch Loss:  2.0519633\n",
      "Epoch: 15 \tTime: 4398.222699642181 \tAverage Loss Per Batch:: 2.066209852695465\n",
      "Done Batch: 0 \tAverage Loss Per Batch: 0.9824014902114868 \t Current Batch Loss:  0.9824015\n",
      "Done Batch: 50 \tAverage Loss Per Batch: 1.8214381325478648 \t Current Batch Loss:  2.1023517\n",
      "Done Batch: 100 \tAverage Loss Per Batch: 1.8378851449135507 \t Current Batch Loss:  1.9566483\n",
      "Done Batch: 150 \tAverage Loss Per Batch: 1.8341491482905206 \t Current Batch Loss:  1.744298\n",
      "Done Batch: 200 \tAverage Loss Per Batch: 1.839452917303019 \t Current Batch Loss:  1.799634\n",
      "Done Batch: 250 \tAverage Loss Per Batch: 1.8576832962226109 \t Current Batch Loss:  2.1999798\n",
      "Done Batch: 300 \tAverage Loss Per Batch: 1.8609676543264293 \t Current Batch Loss:  2.1737225\n",
      "Done Batch: 350 \tAverage Loss Per Batch: 1.8704716072462901 \t Current Batch Loss:  1.7532448\n",
      "Done Batch: 400 \tAverage Loss Per Batch: 1.8821416463043328 \t Current Batch Loss:  1.6802173\n",
      "Done Batch: 450 \tAverage Loss Per Batch: 1.872957711473537 \t Current Batch Loss:  1.891917\n",
      "Done Batch: 500 \tAverage Loss Per Batch: 1.878302496350454 \t Current Batch Loss:  1.653873\n",
      "Done Batch: 550 \tAverage Loss Per Batch: 1.8762364606026514 \t Current Batch Loss:  2.158803\n",
      "Done Batch: 600 \tAverage Loss Per Batch: 1.879216597004857 \t Current Batch Loss:  1.8259355\n",
      "Done Batch: 650 \tAverage Loss Per Batch: 1.8787274085980956 \t Current Batch Loss:  1.742048\n",
      "Done Batch: 700 \tAverage Loss Per Batch: 1.8773263014332204 \t Current Batch Loss:  1.9337342\n",
      "Done Batch: 750 \tAverage Loss Per Batch: 1.8763659305166152 \t Current Batch Loss:  1.9150949\n",
      "Done Batch: 800 \tAverage Loss Per Batch: 1.8734889158446542 \t Current Batch Loss:  1.933058\n",
      "Done Batch: 850 \tAverage Loss Per Batch: 1.8759246841860995 \t Current Batch Loss:  2.0551014\n",
      "Done Batch: 900 \tAverage Loss Per Batch: 1.8760111211539638 \t Current Batch Loss:  1.8781631\n",
      "Done Batch: 950 \tAverage Loss Per Batch: 1.885416649869564 \t Current Batch Loss:  2.1168692\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done Batch: 1000 \tAverage Loss Per Batch: 1.895622536733553 \t Current Batch Loss:  2.0128498\n",
      "Done Batch: 1050 \tAverage Loss Per Batch: 1.9023961679465877 \t Current Batch Loss:  2.1210341\n",
      "Done Batch: 1100 \tAverage Loss Per Batch: 1.907587130647914 \t Current Batch Loss:  1.9710221\n",
      "Done Batch: 1150 \tAverage Loss Per Batch: 1.9093878146982732 \t Current Batch Loss:  1.9929384\n",
      "Done Batch: 1200 \tAverage Loss Per Batch: 1.910719232297162 \t Current Batch Loss:  2.122558\n",
      "Done Batch: 1250 \tAverage Loss Per Batch: 1.9095049120729966 \t Current Batch Loss:  1.8713497\n",
      "Done Batch: 1300 \tAverage Loss Per Batch: 1.9072708651068393 \t Current Batch Loss:  1.8457166\n",
      "Done Batch: 1350 \tAverage Loss Per Batch: 1.9034522129464027 \t Current Batch Loss:  1.440107\n",
      "Done Batch: 1400 \tAverage Loss Per Batch: 1.8985741426908995 \t Current Batch Loss:  1.6735169\n",
      "Done Batch: 1450 \tAverage Loss Per Batch: 1.8937389678909071 \t Current Batch Loss:  1.6882232\n",
      "Done Batch: 1500 \tAverage Loss Per Batch: 1.8899097521252668 \t Current Batch Loss:  2.1252131\n",
      "Done Batch: 1550 \tAverage Loss Per Batch: 1.8857540776082427 \t Current Batch Loss:  1.889656\n",
      "Done Batch: 1600 \tAverage Loss Per Batch: 1.8825644055133608 \t Current Batch Loss:  1.6588408\n",
      "Done Batch: 1650 \tAverage Loss Per Batch: 1.8781309267883948 \t Current Batch Loss:  2.0112467\n",
      "Done Batch: 1700 \tAverage Loss Per Batch: 1.8732091528327937 \t Current Batch Loss:  1.639208\n",
      "Done Batch: 1750 \tAverage Loss Per Batch: 1.8685118429052157 \t Current Batch Loss:  1.7914485\n",
      "Done Batch: 1800 \tAverage Loss Per Batch: 1.865824223292794 \t Current Batch Loss:  1.69067\n",
      "Done Batch: 1850 \tAverage Loss Per Batch: 1.8609516099618617 \t Current Batch Loss:  1.8107\n",
      "Done Batch: 1900 \tAverage Loss Per Batch: 1.8567720977711966 \t Current Batch Loss:  1.5592788\n",
      "Done Batch: 1950 \tAverage Loss Per Batch: 1.850751372131551 \t Current Batch Loss:  1.9510276\n",
      "Done Batch: 2000 \tAverage Loss Per Batch: 1.8442975610330783 \t Current Batch Loss:  1.9663752\n",
      "Done Batch: 2050 \tAverage Loss Per Batch: 1.8401105759028746 \t Current Batch Loss:  1.9719996\n",
      "Done Batch: 2100 \tAverage Loss Per Batch: 1.8361637440254324 \t Current Batch Loss:  1.6093094\n",
      "Done Batch: 2150 \tAverage Loss Per Batch: 1.8326624984355484 \t Current Batch Loss:  1.7270708\n",
      "Done Batch: 2200 \tAverage Loss Per Batch: 1.8295364253145086 \t Current Batch Loss:  2.1005225\n",
      "Done Batch: 2250 \tAverage Loss Per Batch: 1.8265765341373614 \t Current Batch Loss:  1.636717\n",
      "Done Batch: 2300 \tAverage Loss Per Batch: 1.825041819945049 \t Current Batch Loss:  1.71895\n",
      "Done Batch: 2350 \tAverage Loss Per Batch: 1.8241916385826684 \t Current Batch Loss:  1.7626555\n",
      "Done Batch: 2400 \tAverage Loss Per Batch: 1.821180708951525 \t Current Batch Loss:  1.3888212\n",
      "Done Batch: 2450 \tAverage Loss Per Batch: 1.8192432427688405 \t Current Batch Loss:  1.6961484\n",
      "Done Batch: 2500 \tAverage Loss Per Batch: 1.8170059456056902 \t Current Batch Loss:  1.6870564\n",
      "Done Batch: 2550 \tAverage Loss Per Batch: 1.812816904414546 \t Current Batch Loss:  1.4901695\n",
      "Done Batch: 2600 \tAverage Loss Per Batch: 1.8086997355373489 \t Current Batch Loss:  1.5987984\n",
      "Done Batch: 2650 \tAverage Loss Per Batch: 1.8054940208135035 \t Current Batch Loss:  1.6202877\n",
      "Done Batch: 2700 \tAverage Loss Per Batch: 1.8034060734018313 \t Current Batch Loss:  1.5843239\n",
      "Done Batch: 2750 \tAverage Loss Per Batch: 1.800009045112094 \t Current Batch Loss:  1.6685985\n",
      "Done Batch: 2800 \tAverage Loss Per Batch: 1.795716553447673 \t Current Batch Loss:  1.6091183\n",
      "Done Batch: 2850 \tAverage Loss Per Batch: 1.7925354157026003 \t Current Batch Loss:  1.6450219\n",
      "Done Batch: 2900 \tAverage Loss Per Batch: 1.7891716161541837 \t Current Batch Loss:  1.4993511\n",
      "Done Batch: 2950 \tAverage Loss Per Batch: 1.7859006633357815 \t Current Batch Loss:  1.4675269\n",
      "Done Batch: 3000 \tAverage Loss Per Batch: 1.7834492760553078 \t Current Batch Loss:  1.5962651\n",
      "Done Batch: 3050 \tAverage Loss Per Batch: 1.7819660648053999 \t Current Batch Loss:  1.7546538\n",
      "Done Batch: 3100 \tAverage Loss Per Batch: 1.7794428616329687 \t Current Batch Loss:  1.4791234\n",
      "Done Batch: 3150 \tAverage Loss Per Batch: 1.7763012563262959 \t Current Batch Loss:  1.7331853\n",
      "Done Batch: 3200 \tAverage Loss Per Batch: 1.7735293621720467 \t Current Batch Loss:  1.3067245\n",
      "Done Batch: 3250 \tAverage Loss Per Batch: 1.770039075889869 \t Current Batch Loss:  1.692107\n",
      "Done Batch: 3300 \tAverage Loss Per Batch: 1.767887018225981 \t Current Batch Loss:  1.7343116\n",
      "Done Batch: 3350 \tAverage Loss Per Batch: 1.766139759266067 \t Current Batch Loss:  1.8202274\n",
      "Epoch: 16 \tTime: 4409.763114929199 \tAverage Loss Per Batch:: 1.7657069661583997\n",
      "Done Batch: 0 \tAverage Loss Per Batch: 0.9865760803222656 \t Current Batch Loss:  0.9865761\n",
      "Done Batch: 50 \tAverage Loss Per Batch: 1.5268073198842067 \t Current Batch Loss:  1.6832285\n",
      "Done Batch: 100 \tAverage Loss Per Batch: 1.5262907070688683 \t Current Batch Loss:  1.4759723\n",
      "Done Batch: 150 \tAverage Loss Per Batch: 1.5382229816045192 \t Current Batch Loss:  1.4662896\n",
      "Done Batch: 200 \tAverage Loss Per Batch: 1.5468621556438618 \t Current Batch Loss:  1.8883907\n",
      "Done Batch: 250 \tAverage Loss Per Batch: 1.5719444566513912 \t Current Batch Loss:  1.8434863\n",
      "Done Batch: 300 \tAverage Loss Per Batch: 1.5805234386279337 \t Current Batch Loss:  1.5698031\n",
      "Done Batch: 350 \tAverage Loss Per Batch: 1.5970702711333575 \t Current Batch Loss:  1.3698708\n",
      "Done Batch: 400 \tAverage Loss Per Batch: 1.6061019986644944 \t Current Batch Loss:  1.4938833\n",
      "Done Batch: 450 \tAverage Loss Per Batch: 1.6014108380298657 \t Current Batch Loss:  1.8582042\n",
      "Done Batch: 500 \tAverage Loss Per Batch: 1.6054623193607598 \t Current Batch Loss:  1.5261314\n",
      "Done Batch: 550 \tAverage Loss Per Batch: 1.6022064181724174 \t Current Batch Loss:  1.449948\n",
      "Done Batch: 600 \tAverage Loss Per Batch: 1.606287510145127 \t Current Batch Loss:  1.7211703\n",
      "Done Batch: 650 \tAverage Loss Per Batch: 1.6078727752565423 \t Current Batch Loss:  1.6241008\n",
      "Done Batch: 700 \tAverage Loss Per Batch: 1.609536847272375 \t Current Batch Loss:  1.6713299\n",
      "Done Batch: 750 \tAverage Loss Per Batch: 1.607492908180633 \t Current Batch Loss:  1.5828284\n",
      "Done Batch: 800 \tAverage Loss Per Batch: 1.6047584848308682 \t Current Batch Loss:  1.6579155\n",
      "Done Batch: 850 \tAverage Loss Per Batch: 1.6060018882628193 \t Current Batch Loss:  1.5748923\n",
      "Done Batch: 900 \tAverage Loss Per Batch: 1.6066263493369608 \t Current Batch Loss:  1.5049661\n",
      "Done Batch: 950 \tAverage Loss Per Batch: 1.6152551760808902 \t Current Batch Loss:  1.8211399\n",
      "Done Batch: 1000 \tAverage Loss Per Batch: 1.6225857213065102 \t Current Batch Loss:  1.6003305\n",
      "Done Batch: 1050 \tAverage Loss Per Batch: 1.629520473312356 \t Current Batch Loss:  1.831311\n",
      "Done Batch: 1100 \tAverage Loss Per Batch: 1.6318464283505751 \t Current Batch Loss:  1.717233\n",
      "Done Batch: 1150 \tAverage Loss Per Batch: 1.6328987259123244 \t Current Batch Loss:  1.6073503\n",
      "Done Batch: 1200 \tAverage Loss Per Batch: 1.634329559304732 \t Current Batch Loss:  1.6523678\n",
      "Done Batch: 1250 \tAverage Loss Per Batch: 1.6335812363026143 \t Current Batch Loss:  1.8021092\n",
      "Done Batch: 1300 \tAverage Loss Per Batch: 1.6309523219607776 \t Current Batch Loss:  1.49756\n",
      "Done Batch: 1350 \tAverage Loss Per Batch: 1.6299257014611312 \t Current Batch Loss:  1.5236101\n",
      "Done Batch: 1400 \tAverage Loss Per Batch: 1.6270139953394094 \t Current Batch Loss:  1.4798877\n",
      "Done Batch: 1450 \tAverage Loss Per Batch: 1.6212631507547537 \t Current Batch Loss:  1.4782239\n",
      "Done Batch: 1500 \tAverage Loss Per Batch: 1.6175959892546155 \t Current Batch Loss:  1.6043662\n",
      "Done Batch: 1550 \tAverage Loss Per Batch: 1.6138048746138522 \t Current Batch Loss:  1.632097\n",
      "Done Batch: 1600 \tAverage Loss Per Batch: 1.6102319055613243 \t Current Batch Loss:  1.388068\n",
      "Done Batch: 1650 \tAverage Loss Per Batch: 1.6062617915677417 \t Current Batch Loss:  1.7668706\n",
      "Done Batch: 1700 \tAverage Loss Per Batch: 1.6020877468522894 \t Current Batch Loss:  1.6761072\n",
      "Done Batch: 1750 \tAverage Loss Per Batch: 1.5962489510453952 \t Current Batch Loss:  1.6980199\n",
      "Done Batch: 1800 \tAverage Loss Per Batch: 1.592549248181205 \t Current Batch Loss:  1.5275161\n",
      "Done Batch: 1850 \tAverage Loss Per Batch: 1.5889007517094873 \t Current Batch Loss:  1.3772938\n",
      "Done Batch: 1900 \tAverage Loss Per Batch: 1.5848011586742612 \t Current Batch Loss:  1.1485512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done Batch: 1950 \tAverage Loss Per Batch: 1.5786858459181323 \t Current Batch Loss:  1.3120024\n",
      "Done Batch: 2000 \tAverage Loss Per Batch: 1.5722679256022662 \t Current Batch Loss:  1.3054825\n",
      "Done Batch: 2050 \tAverage Loss Per Batch: 1.568597690353505 \t Current Batch Loss:  1.8655125\n",
      "Done Batch: 2100 \tAverage Loss Per Batch: 1.5642176773478904 \t Current Batch Loss:  1.1769087\n",
      "Done Batch: 2150 \tAverage Loss Per Batch: 1.5593825512739294 \t Current Batch Loss:  1.2930037\n",
      "Done Batch: 2200 \tAverage Loss Per Batch: 1.5560595002947804 \t Current Batch Loss:  1.405282\n",
      "Done Batch: 2250 \tAverage Loss Per Batch: 1.5540884969394402 \t Current Batch Loss:  1.54202\n",
      "Done Batch: 2300 \tAverage Loss Per Batch: 1.5522172413717814 \t Current Batch Loss:  1.4418522\n",
      "Done Batch: 2350 \tAverage Loss Per Batch: 1.55061412085781 \t Current Batch Loss:  1.686506\n",
      "Done Batch: 2400 \tAverage Loss Per Batch: 1.5480591755814972 \t Current Batch Loss:  1.3022581\n",
      "Done Batch: 2450 \tAverage Loss Per Batch: 1.5457573329025946 \t Current Batch Loss:  1.3965572\n",
      "Done Batch: 2500 \tAverage Loss Per Batch: 1.5430523801069171 \t Current Batch Loss:  1.3611808\n",
      "Done Batch: 2550 \tAverage Loss Per Batch: 1.5395998939351911 \t Current Batch Loss:  1.4349359\n",
      "Done Batch: 2600 \tAverage Loss Per Batch: 1.536655505360754 \t Current Batch Loss:  1.5458016\n",
      "Done Batch: 2650 \tAverage Loss Per Batch: 1.5345265612157952 \t Current Batch Loss:  1.6424251\n",
      "Done Batch: 2700 \tAverage Loss Per Batch: 1.533265106989604 \t Current Batch Loss:  1.6657987\n",
      "Done Batch: 2750 \tAverage Loss Per Batch: 1.5311535395348734 \t Current Batch Loss:  1.5524895\n",
      "Done Batch: 2800 \tAverage Loss Per Batch: 1.5278818780973271 \t Current Batch Loss:  1.5518391\n",
      "Done Batch: 2850 \tAverage Loss Per Batch: 1.5251579493315586 \t Current Batch Loss:  1.3523904\n",
      "Done Batch: 2900 \tAverage Loss Per Batch: 1.5238217572350619 \t Current Batch Loss:  1.528033\n",
      "Done Batch: 2950 \tAverage Loss Per Batch: 1.5219139444226129 \t Current Batch Loss:  1.519365\n",
      "Done Batch: 3000 \tAverage Loss Per Batch: 1.5199560664447693 \t Current Batch Loss:  1.3470662\n",
      "Done Batch: 3050 \tAverage Loss Per Batch: 1.5185053629665757 \t Current Batch Loss:  1.2845596\n",
      "Done Batch: 3100 \tAverage Loss Per Batch: 1.517460326558734 \t Current Batch Loss:  1.1695303\n",
      "Done Batch: 3150 \tAverage Loss Per Batch: 1.5143404671102432 \t Current Batch Loss:  1.2753273\n",
      "Done Batch: 3200 \tAverage Loss Per Batch: 1.5118276125674917 \t Current Batch Loss:  1.0639353\n",
      "Done Batch: 3250 \tAverage Loss Per Batch: 1.5083112904600862 \t Current Batch Loss:  1.5115747\n",
      "Done Batch: 3300 \tAverage Loss Per Batch: 1.5068283553594388 \t Current Batch Loss:  1.4941498\n",
      "Done Batch: 3350 \tAverage Loss Per Batch: 1.5050501236628149 \t Current Batch Loss:  1.1551528\n",
      "Epoch: 17 \tTime: 4400.788722276688 \tAverage Loss Per Batch:: 1.504787223449545\n",
      "Done Batch: 0 \tAverage Loss Per Batch: 1.0350440740585327 \t Current Batch Loss:  1.0350441\n",
      "Done Batch: 50 \tAverage Loss Per Batch: 1.3548875977011288 \t Current Batch Loss:  1.2501204\n",
      "Done Batch: 100 \tAverage Loss Per Batch: 1.3178875104035481 \t Current Batch Loss:  1.3162314\n",
      "Done Batch: 150 \tAverage Loss Per Batch: 1.3321721230121637 \t Current Batch Loss:  1.2764041\n",
      "Done Batch: 200 \tAverage Loss Per Batch: 1.3429445704417442 \t Current Batch Loss:  1.4334924\n",
      "Done Batch: 250 \tAverage Loss Per Batch: 1.3594348810583472 \t Current Batch Loss:  1.3708719\n",
      "Done Batch: 300 \tAverage Loss Per Batch: 1.3646724925088725 \t Current Batch Loss:  1.352118\n",
      "Done Batch: 350 \tAverage Loss Per Batch: 1.3771121678528961 \t Current Batch Loss:  1.1637871\n",
      "Done Batch: 400 \tAverage Loss Per Batch: 1.3844286899019655 \t Current Batch Loss:  1.0683221\n",
      "Done Batch: 450 \tAverage Loss Per Batch: 1.3800566310100175 \t Current Batch Loss:  1.5631005\n",
      "Done Batch: 500 \tAverage Loss Per Batch: 1.3754206469910826 \t Current Batch Loss:  1.3367555\n",
      "Done Batch: 550 \tAverage Loss Per Batch: 1.3771064281463623 \t Current Batch Loss:  1.378196\n",
      "Done Batch: 600 \tAverage Loss Per Batch: 1.3805693663693903 \t Current Batch Loss:  1.3445463\n",
      "Done Batch: 650 \tAverage Loss Per Batch: 1.3829189546280376 \t Current Batch Loss:  1.313265\n",
      "Done Batch: 700 \tAverage Loss Per Batch: 1.3810927094134386 \t Current Batch Loss:  1.5105636\n",
      "Done Batch: 750 \tAverage Loss Per Batch: 1.380576246746688 \t Current Batch Loss:  1.2242053\n",
      "Done Batch: 800 \tAverage Loss Per Batch: 1.3792902383018522 \t Current Batch Loss:  1.2685971\n",
      "Done Batch: 850 \tAverage Loss Per Batch: 1.3794809448172707 \t Current Batch Loss:  1.2339623\n",
      "Done Batch: 900 \tAverage Loss Per Batch: 1.3823825090495119 \t Current Batch Loss:  1.4289191\n",
      "Done Batch: 950 \tAverage Loss Per Batch: 1.38933610941208 \t Current Batch Loss:  1.5064294\n",
      "Done Batch: 1000 \tAverage Loss Per Batch: 1.3951114503058282 \t Current Batch Loss:  1.4064937\n",
      "Done Batch: 1050 \tAverage Loss Per Batch: 1.3999029806293157 \t Current Batch Loss:  1.436809\n",
      "Done Batch: 1100 \tAverage Loss Per Batch: 1.4040876192791043 \t Current Batch Loss:  1.277465\n",
      "Done Batch: 1150 \tAverage Loss Per Batch: 1.4059876209751199 \t Current Batch Loss:  1.2917833\n",
      "Done Batch: 1200 \tAverage Loss Per Batch: 1.4058355453310163 \t Current Batch Loss:  1.481949\n",
      "Done Batch: 1250 \tAverage Loss Per Batch: 1.40483768804849 \t Current Batch Loss:  1.587932\n",
      "Done Batch: 1300 \tAverage Loss Per Batch: 1.4016555202273018 \t Current Batch Loss:  1.2615476\n",
      "Done Batch: 1350 \tAverage Loss Per Batch: 1.4001359935163304 \t Current Batch Loss:  1.2060761\n",
      "Done Batch: 1400 \tAverage Loss Per Batch: 1.397254171837746 \t Current Batch Loss:  1.6332924\n",
      "Done Batch: 1450 \tAverage Loss Per Batch: 1.3927800832412556 \t Current Batch Loss:  1.3203653\n",
      "Done Batch: 1500 \tAverage Loss Per Batch: 1.39044539444769 \t Current Batch Loss:  1.5285394\n",
      "Done Batch: 1550 \tAverage Loss Per Batch: 1.386267312221724 \t Current Batch Loss:  1.1879332\n",
      "Done Batch: 1600 \tAverage Loss Per Batch: 1.3838330048609346 \t Current Batch Loss:  1.63139\n",
      "Done Batch: 1650 \tAverage Loss Per Batch: 1.3797385232655082 \t Current Batch Loss:  1.3689057\n",
      "Done Batch: 1700 \tAverage Loss Per Batch: 1.3761915138088767 \t Current Batch Loss:  1.4321344\n",
      "Done Batch: 1750 \tAverage Loss Per Batch: 1.3718948758785416 \t Current Batch Loss:  1.3257499\n",
      "Done Batch: 1800 \tAverage Loss Per Batch: 1.3687319813470984 \t Current Batch Loss:  1.3565124\n",
      "Done Batch: 1850 \tAverage Loss Per Batch: 1.3644289170195127 \t Current Batch Loss:  1.3978473\n",
      "Done Batch: 1900 \tAverage Loss Per Batch: 1.3606617408699264 \t Current Batch Loss:  1.0895385\n",
      "Done Batch: 1950 \tAverage Loss Per Batch: 1.354843271078054 \t Current Batch Loss:  1.2498189\n",
      "Done Batch: 2000 \tAverage Loss Per Batch: 1.3498231482648777 \t Current Batch Loss:  1.1125453\n",
      "Done Batch: 2050 \tAverage Loss Per Batch: 1.346221627461393 \t Current Batch Loss:  1.4964964\n",
      "Done Batch: 2100 \tAverage Loss Per Batch: 1.3427246474811203 \t Current Batch Loss:  1.0053685\n",
      "Done Batch: 2150 \tAverage Loss Per Batch: 1.3387607691843306 \t Current Batch Loss:  1.1270906\n",
      "Done Batch: 2200 \tAverage Loss Per Batch: 1.3352930213255754 \t Current Batch Loss:  1.3010106\n",
      "Done Batch: 2250 \tAverage Loss Per Batch: 1.3334588368221794 \t Current Batch Loss:  1.1997571\n",
      "Done Batch: 2300 \tAverage Loss Per Batch: 1.332480246711948 \t Current Batch Loss:  1.1299535\n",
      "Done Batch: 2350 \tAverage Loss Per Batch: 1.330832259673253 \t Current Batch Loss:  1.263377\n",
      "Done Batch: 2400 \tAverage Loss Per Batch: 1.3281394968128164 \t Current Batch Loss:  0.9813557\n",
      "Done Batch: 2450 \tAverage Loss Per Batch: 1.3263067513619184 \t Current Batch Loss:  1.3055582\n",
      "Done Batch: 2500 \tAverage Loss Per Batch: 1.3236204629800454 \t Current Batch Loss:  1.2032129\n",
      "Done Batch: 2550 \tAverage Loss Per Batch: 1.320475323089942 \t Current Batch Loss:  0.92282397\n",
      "Done Batch: 2600 \tAverage Loss Per Batch: 1.3181823972141409 \t Current Batch Loss:  1.3788985\n",
      "Done Batch: 2650 \tAverage Loss Per Batch: 1.3161833122378248 \t Current Batch Loss:  1.2493911\n",
      "Done Batch: 2700 \tAverage Loss Per Batch: 1.3153557415275121 \t Current Batch Loss:  1.2725706\n",
      "Done Batch: 2750 \tAverage Loss Per Batch: 1.3140868144830935 \t Current Batch Loss:  1.4146171\n",
      "Done Batch: 2800 \tAverage Loss Per Batch: 1.311171633224324 \t Current Batch Loss:  1.100699\n",
      "Done Batch: 2850 \tAverage Loss Per Batch: 1.3089525209147232 \t Current Batch Loss:  1.2532666\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done Batch: 2900 \tAverage Loss Per Batch: 1.307455121731191 \t Current Batch Loss:  1.256136\n",
      "Done Batch: 2950 \tAverage Loss Per Batch: 1.3060262433677075 \t Current Batch Loss:  1.160245\n",
      "Done Batch: 3000 \tAverage Loss Per Batch: 1.304287694843957 \t Current Batch Loss:  1.3959215\n",
      "Done Batch: 3050 \tAverage Loss Per Batch: 1.3032471949215678 \t Current Batch Loss:  1.3556195\n",
      "Done Batch: 3100 \tAverage Loss Per Batch: 1.302669477593472 \t Current Batch Loss:  1.0248339\n",
      "Done Batch: 3150 \tAverage Loss Per Batch: 1.3007424171861488 \t Current Batch Loss:  0.9999575\n",
      "Done Batch: 3200 \tAverage Loss Per Batch: 1.2987999974917859 \t Current Batch Loss:  1.2730085\n",
      "Done Batch: 3250 \tAverage Loss Per Batch: 1.2967635283137204 \t Current Batch Loss:  0.97930586\n",
      "Done Batch: 3300 \tAverage Loss Per Batch: 1.2956291869964356 \t Current Batch Loss:  1.3175579\n",
      "Done Batch: 3350 \tAverage Loss Per Batch: 1.2949928584400485 \t Current Batch Loss:  1.0890974\n",
      "Epoch: 18 \tTime: 4391.272900342941 \tAverage Loss Per Batch:: 1.2949442109687768\n",
      "Done Batch: 0 \tAverage Loss Per Batch: 0.737069308757782 \t Current Batch Loss:  0.7370693\n",
      "Done Batch: 50 \tAverage Loss Per Batch: 1.2160653553757013 \t Current Batch Loss:  1.3346393\n",
      "Done Batch: 100 \tAverage Loss Per Batch: 1.1990216693075577 \t Current Batch Loss:  1.1902889\n",
      "Done Batch: 150 \tAverage Loss Per Batch: 1.2062156729350817 \t Current Batch Loss:  1.1770972\n",
      "Done Batch: 200 \tAverage Loss Per Batch: 1.1965320694505872 \t Current Batch Loss:  1.1975048\n",
      "Done Batch: 250 \tAverage Loss Per Batch: 1.2046752933012062 \t Current Batch Loss:  1.535795\n",
      "Done Batch: 300 \tAverage Loss Per Batch: 1.2078566339324874 \t Current Batch Loss:  1.2703556\n",
      "Done Batch: 350 \tAverage Loss Per Batch: 1.2147494681880004 \t Current Batch Loss:  1.128714\n",
      "Done Batch: 400 \tAverage Loss Per Batch: 1.2228580957041715 \t Current Batch Loss:  0.9283392\n",
      "Done Batch: 450 \tAverage Loss Per Batch: 1.218935812259725 \t Current Batch Loss:  1.2774719\n",
      "Done Batch: 500 \tAverage Loss Per Batch: 1.2153606410036069 \t Current Batch Loss:  0.9687947\n",
      "Done Batch: 550 \tAverage Loss Per Batch: 1.2134163988696685 \t Current Batch Loss:  1.2501614\n",
      "Done Batch: 600 \tAverage Loss Per Batch: 1.217009441031394 \t Current Batch Loss:  1.2567397\n",
      "Done Batch: 650 \tAverage Loss Per Batch: 1.218004352669196 \t Current Batch Loss:  1.3395635\n",
      "Done Batch: 700 \tAverage Loss Per Batch: 1.2170489624291445 \t Current Batch Loss:  0.9869652\n",
      "Done Batch: 750 \tAverage Loss Per Batch: 1.2155252435553408 \t Current Batch Loss:  1.304506\n",
      "Done Batch: 800 \tAverage Loss Per Batch: 1.2147576112425729 \t Current Batch Loss:  1.3424201\n",
      "Done Batch: 850 \tAverage Loss Per Batch: 1.2133520020721382 \t Current Batch Loss:  1.184071\n",
      "Done Batch: 900 \tAverage Loss Per Batch: 1.2130393625364186 \t Current Batch Loss:  1.4260353\n",
      "Done Batch: 950 \tAverage Loss Per Batch: 1.2191076144560404 \t Current Batch Loss:  1.5614195\n",
      "Done Batch: 1000 \tAverage Loss Per Batch: 1.2240880462792252 \t Current Batch Loss:  1.1386666\n",
      "Done Batch: 1050 \tAverage Loss Per Batch: 1.2278344703786834 \t Current Batch Loss:  1.207236\n",
      "Done Batch: 1100 \tAverage Loss Per Batch: 1.229020359568115 \t Current Batch Loss:  1.3705069\n",
      "Done Batch: 1150 \tAverage Loss Per Batch: 1.2291510710500821 \t Current Batch Loss:  1.1809168\n",
      "Done Batch: 1200 \tAverage Loss Per Batch: 1.228016168400211 \t Current Batch Loss:  1.3527865\n",
      "Done Batch: 1250 \tAverage Loss Per Batch: 1.2269965084813101 \t Current Batch Loss:  1.08082\n",
      "Done Batch: 1300 \tAverage Loss Per Batch: 1.223830950480072 \t Current Batch Loss:  1.1329538\n",
      "Done Batch: 1350 \tAverage Loss Per Batch: 1.2217785011036673 \t Current Batch Loss:  1.208005\n",
      "Done Batch: 1400 \tAverage Loss Per Batch: 1.2185510624144267 \t Current Batch Loss:  1.1012105\n",
      "Done Batch: 1450 \tAverage Loss Per Batch: 1.2151223927181725 \t Current Batch Loss:  1.269364\n",
      "Done Batch: 1500 \tAverage Loss Per Batch: 1.2118298858503433 \t Current Batch Loss:  1.5189424\n",
      "Done Batch: 1550 \tAverage Loss Per Batch: 1.2087879740830623 \t Current Batch Loss:  1.0358019\n",
      "Done Batch: 1600 \tAverage Loss Per Batch: 1.2061014718967702 \t Current Batch Loss:  1.0209602\n",
      "Done Batch: 1650 \tAverage Loss Per Batch: 1.2026259069873086 \t Current Batch Loss:  0.99696517\n",
      "Done Batch: 1700 \tAverage Loss Per Batch: 1.198870917021142 \t Current Batch Loss:  1.0958595\n",
      "Done Batch: 1750 \tAverage Loss Per Batch: 1.1945041611765126 \t Current Batch Loss:  1.1695696\n",
      "Done Batch: 1800 \tAverage Loss Per Batch: 1.1915786974566438 \t Current Batch Loss:  1.2157947\n",
      "Done Batch: 1850 \tAverage Loss Per Batch: 1.1868318641011872 \t Current Batch Loss:  1.1265\n",
      "Done Batch: 1900 \tAverage Loss Per Batch: 1.183245290460742 \t Current Batch Loss:  1.1162486\n",
      "Done Batch: 1950 \tAverage Loss Per Batch: 1.178259379829522 \t Current Batch Loss:  1.1067176\n",
      "Done Batch: 2000 \tAverage Loss Per Batch: 1.1738694064144133 \t Current Batch Loss:  0.99441576\n",
      "Done Batch: 2050 \tAverage Loss Per Batch: 1.1715228254187113 \t Current Batch Loss:  1.247369\n",
      "Done Batch: 2100 \tAverage Loss Per Batch: 1.168251467880211 \t Current Batch Loss:  0.7578071\n",
      "Done Batch: 2150 \tAverage Loss Per Batch: 1.165211997992047 \t Current Batch Loss:  0.9340815\n",
      "Done Batch: 2200 \tAverage Loss Per Batch: 1.162172621973316 \t Current Batch Loss:  1.2410656\n",
      "Done Batch: 2250 \tAverage Loss Per Batch: 1.1605813304406705 \t Current Batch Loss:  0.97068113\n",
      "Done Batch: 2300 \tAverage Loss Per Batch: 1.1588925917425243 \t Current Batch Loss:  1.2225568\n",
      "Done Batch: 2350 \tAverage Loss Per Batch: 1.157256980568031 \t Current Batch Loss:  1.1907187\n",
      "Done Batch: 2400 \tAverage Loss Per Batch: 1.1564369563111063 \t Current Batch Loss:  0.89912117\n",
      "Done Batch: 2450 \tAverage Loss Per Batch: 1.154902432194441 \t Current Batch Loss:  1.0313908\n",
      "Done Batch: 2500 \tAverage Loss Per Batch: 1.1527913846477704 \t Current Batch Loss:  0.99999076\n",
      "Done Batch: 2550 \tAverage Loss Per Batch: 1.150234407674841 \t Current Batch Loss:  0.83927256\n",
      "Done Batch: 2600 \tAverage Loss Per Batch: 1.1488841783537493 \t Current Batch Loss:  0.9822744\n",
      "Done Batch: 2650 \tAverage Loss Per Batch: 1.1472502061440242 \t Current Batch Loss:  1.1117355\n",
      "Done Batch: 2700 \tAverage Loss Per Batch: 1.1458678695723905 \t Current Batch Loss:  1.3464639\n",
      "Done Batch: 2750 \tAverage Loss Per Batch: 1.1455549165709416 \t Current Batch Loss:  1.1976932\n",
      "Done Batch: 2800 \tAverage Loss Per Batch: 1.1434774726725017 \t Current Batch Loss:  0.94921356\n",
      "Done Batch: 2850 \tAverage Loss Per Batch: 1.1415119693388818 \t Current Batch Loss:  0.94495654\n",
      "Done Batch: 2900 \tAverage Loss Per Batch: 1.1402573189995118 \t Current Batch Loss:  1.1654512\n",
      "Done Batch: 2950 \tAverage Loss Per Batch: 1.1388689386202335 \t Current Batch Loss:  0.9946275\n",
      "Done Batch: 3000 \tAverage Loss Per Batch: 1.1378047071826811 \t Current Batch Loss:  1.1772869\n",
      "Done Batch: 3050 \tAverage Loss Per Batch: 1.137327815403277 \t Current Batch Loss:  0.9861895\n",
      "Done Batch: 3100 \tAverage Loss Per Batch: 1.136627461613474 \t Current Batch Loss:  1.0649525\n",
      "Done Batch: 3150 \tAverage Loss Per Batch: 1.1351402384142995 \t Current Batch Loss:  1.2144116\n",
      "Done Batch: 3200 \tAverage Loss Per Batch: 1.1333072721418163 \t Current Batch Loss:  1.1615722\n",
      "Done Batch: 3250 \tAverage Loss Per Batch: 1.1323343115526139 \t Current Batch Loss:  1.188817\n",
      "Done Batch: 3300 \tAverage Loss Per Batch: 1.1318200287295992 \t Current Batch Loss:  1.0128226\n",
      "Done Batch: 3350 \tAverage Loss Per Batch: 1.131558856210649 \t Current Batch Loss:  1.0220559\n",
      "Epoch: 19 \tTime: 4386.839592933655 \tAverage Loss Per Batch:: 1.1312771451855612\n"
     ]
    }
   ],
   "source": [
    "def train(model, learning_rate=0.0001, batch_size=100, epochs=10):\n",
    "    \n",
    "    # defining criterion and optimizer\n",
    "    criterion = nn.NLLLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr = learning_rate)\n",
    "    \n",
    "    number_of_batches = math.ceil(len(train_image_addresses)/batch_size)\n",
    "    indexes = np.arange(len(train_image_addresses))\n",
    "    \n",
    "    loss_arr = []\n",
    "    for epoch in range(15, 15 + epochs):\n",
    "        avgloss = 0.0\n",
    "        start = time.time()\n",
    "        avg_loss_arr = []\n",
    "        for batch in range(number_of_batches):\n",
    "            train_indexes = [train_image_addresses[i] for i in indexes[batch*batch_size:(batch+1)*batch_size]]\n",
    "            image_batch, target_batch = load_batch(train_indexes)\n",
    "            loss = train_pass(image_batch, target_batch, model, optimizer, criterion)\n",
    "            avgloss += loss\n",
    "            if batch%50 == 0:\n",
    "                print (\"Done Batch:\", batch, \"\\tAverage Loss Per Batch:\", avgloss/(batch+1), \"\\t Current Batch Loss: \", loss)\n",
    "        loss_arr.append(avgloss/(batch+1))\n",
    "        print (\"Epoch:\",epoch, \"\\tTime:\", time.time() - start, \"\\tAverage Loss Per Batch::\", avgloss/(batch+1))\n",
    "        torch.save({'epoch': epoch ,'state_dict': model.state_dict(),'optimizer': optimizer.state_dict()}, open(\"outputs/anp_log_softmax_classifier_batch_\"+str(epoch), \"wb+\"))\n",
    "    loss_arr = np.array(loss_arr)\n",
    "    np.save(open('outputs/loss_log_softmax_anp_classifier', 'wb+'), loss_arr)\n",
    "    \n",
    "train(main_model, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done Batch: 0 \tAverage Loss Per Batch: 0.7156417369842529 \t Current Batch Loss:  0.71564174\n",
      "Done Batch: 50 \tAverage Loss Per Batch: 1.106495250673855 \t Current Batch Loss:  1.4382366\n",
      "Done Batch: 100 \tAverage Loss Per Batch: 1.1164612404190668 \t Current Batch Loss:  1.1303102\n",
      "Done Batch: 150 \tAverage Loss Per Batch: 1.103954608077245 \t Current Batch Loss:  1.1451755\n",
      "Done Batch: 200 \tAverage Loss Per Batch: 1.1013731315954407 \t Current Batch Loss:  1.2451254\n",
      "Done Batch: 250 \tAverage Loss Per Batch: 1.0993914295477696 \t Current Batch Loss:  1.1063768\n",
      "Done Batch: 300 \tAverage Loss Per Batch: 1.099318616215969 \t Current Batch Loss:  1.1388297\n",
      "Done Batch: 350 \tAverage Loss Per Batch: 1.1053235136885249 \t Current Batch Loss:  1.1766738\n",
      "Done Batch: 400 \tAverage Loss Per Batch: 1.107397822073273 \t Current Batch Loss:  1.0034796\n",
      "Done Batch: 450 \tAverage Loss Per Batch: 1.0994208763021058 \t Current Batch Loss:  1.316684\n",
      "Done Batch: 500 \tAverage Loss Per Batch: 1.093865593155463 \t Current Batch Loss:  1.0882255\n",
      "Done Batch: 550 \tAverage Loss Per Batch: 1.0887858333258793 \t Current Batch Loss:  0.9284734\n",
      "Done Batch: 600 \tAverage Loss Per Batch: 1.0852210496904053 \t Current Batch Loss:  0.9624217\n",
      "Done Batch: 650 \tAverage Loss Per Batch: 1.0828348669222057 \t Current Batch Loss:  0.94178903\n",
      "Done Batch: 700 \tAverage Loss Per Batch: 1.0808973165279447 \t Current Batch Loss:  0.8657994\n",
      "Done Batch: 750 \tAverage Loss Per Batch: 1.0769931695432702 \t Current Batch Loss:  1.0412993\n",
      "Done Batch: 800 \tAverage Loss Per Batch: 1.0742258652021526 \t Current Batch Loss:  0.97436064\n",
      "Done Batch: 850 \tAverage Loss Per Batch: 1.0711596038730948 \t Current Batch Loss:  1.210575\n",
      "Done Batch: 900 \tAverage Loss Per Batch: 1.0726462020461223 \t Current Batch Loss:  1.3259004\n",
      "Done Batch: 950 \tAverage Loss Per Batch: 1.074776040016038 \t Current Batch Loss:  1.2047881\n",
      "Done Batch: 1000 \tAverage Loss Per Batch: 1.0780051861490523 \t Current Batch Loss:  1.0334877\n",
      "Done Batch: 1050 \tAverage Loss Per Batch: 1.0795892538851948 \t Current Batch Loss:  1.0058722\n",
      "Done Batch: 1100 \tAverage Loss Per Batch: 1.080183946252627 \t Current Batch Loss:  1.2440144\n",
      "Done Batch: 1150 \tAverage Loss Per Batch: 1.08049803827246 \t Current Batch Loss:  1.0303159\n",
      "Done Batch: 1200 \tAverage Loss Per Batch: 1.0787485596242297 \t Current Batch Loss:  1.304112\n",
      "Done Batch: 1250 \tAverage Loss Per Batch: 1.0798539282987825 \t Current Batch Loss:  1.2000588\n",
      "Done Batch: 1300 \tAverage Loss Per Batch: 1.077140704198217 \t Current Batch Loss:  1.3026419\n",
      "Done Batch: 1350 \tAverage Loss Per Batch: 1.0755687612326035 \t Current Batch Loss:  0.90700775\n",
      "Done Batch: 1400 \tAverage Loss Per Batch: 1.0738265352279777 \t Current Batch Loss:  1.0427737\n",
      "Done Batch: 1450 \tAverage Loss Per Batch: 1.0696467834452281 \t Current Batch Loss:  1.0495481\n",
      "Done Batch: 1500 \tAverage Loss Per Batch: 1.066513098493407 \t Current Batch Loss:  1.3880643\n",
      "Done Batch: 1550 \tAverage Loss Per Batch: 1.0637297791330989 \t Current Batch Loss:  1.0347815\n",
      "Done Batch: 1600 \tAverage Loss Per Batch: 1.06242216176945 \t Current Batch Loss:  1.16225\n",
      "Done Batch: 1650 \tAverage Loss Per Batch: 1.0601245301698352 \t Current Batch Loss:  0.8619719\n",
      "Done Batch: 1700 \tAverage Loss Per Batch: 1.058338125196364 \t Current Batch Loss:  0.9497071\n",
      "Done Batch: 1750 \tAverage Loss Per Batch: 1.05543218034802 \t Current Batch Loss:  1.0814018\n",
      "Done Batch: 1800 \tAverage Loss Per Batch: 1.052493021661609 \t Current Batch Loss:  1.0283442\n",
      "Done Batch: 1850 \tAverage Loss Per Batch: 1.0498990679933597 \t Current Batch Loss:  0.78191763\n",
      "Done Batch: 1900 \tAverage Loss Per Batch: 1.046910308901101 \t Current Batch Loss:  0.76693135\n",
      "Done Batch: 1950 \tAverage Loss Per Batch: 1.0439368652295358 \t Current Batch Loss:  1.1132187\n",
      "Done Batch: 2000 \tAverage Loss Per Batch: 1.040532765925854 \t Current Batch Loss:  1.0263486\n",
      "Done Batch: 2050 \tAverage Loss Per Batch: 1.037587276439676 \t Current Batch Loss:  1.3633323\n",
      "Done Batch: 2100 \tAverage Loss Per Batch: 1.0348697371961728 \t Current Batch Loss:  0.8637952\n",
      "Done Batch: 2150 \tAverage Loss Per Batch: 1.0320554687665375 \t Current Batch Loss:  0.8351931\n",
      "Done Batch: 2200 \tAverage Loss Per Batch: 1.029447745079888 \t Current Batch Loss:  0.9094017\n",
      "Done Batch: 2250 \tAverage Loss Per Batch: 1.0280807717595297 \t Current Batch Loss:  0.9524672\n",
      "Done Batch: 2300 \tAverage Loss Per Batch: 1.0273289430768322 \t Current Batch Loss:  1.1140615\n",
      "Done Batch: 2350 \tAverage Loss Per Batch: 1.0266799237666258 \t Current Batch Loss:  1.0988907\n",
      "Done Batch: 2400 \tAverage Loss Per Batch: 1.0251005373861033 \t Current Batch Loss:  0.7864731\n",
      "Done Batch: 2450 \tAverage Loss Per Batch: 1.0237600578915484 \t Current Batch Loss:  0.9687065\n",
      "Done Batch: 2500 \tAverage Loss Per Batch: 1.022432945052036 \t Current Batch Loss:  1.0582677\n",
      "Done Batch: 2550 \tAverage Loss Per Batch: 1.0208084770905088 \t Current Batch Loss:  0.9534558\n",
      "Done Batch: 2600 \tAverage Loss Per Batch: 1.0188174342384984 \t Current Batch Loss:  0.7251871\n",
      "Done Batch: 2650 \tAverage Loss Per Batch: 1.0175066930939234 \t Current Batch Loss:  0.89650184\n",
      "Done Batch: 2700 \tAverage Loss Per Batch: 1.0162367749902683 \t Current Batch Loss:  0.9600841\n",
      "Done Batch: 2750 \tAverage Loss Per Batch: 1.016500724939293 \t Current Batch Loss:  1.1979883\n",
      "Done Batch: 2800 \tAverage Loss Per Batch: 1.015560310149099 \t Current Batch Loss:  0.9002979\n",
      "Done Batch: 2850 \tAverage Loss Per Batch: 1.014454933728138 \t Current Batch Loss:  1.0684546\n",
      "Done Batch: 2900 \tAverage Loss Per Batch: 1.0139526327656532 \t Current Batch Loss:  0.9756325\n",
      "Done Batch: 2950 \tAverage Loss Per Batch: 1.0126918287084936 \t Current Batch Loss:  1.0393229\n",
      "Done Batch: 3000 \tAverage Loss Per Batch: 1.0115514054412804 \t Current Batch Loss:  0.930439\n",
      "Done Batch: 3050 \tAverage Loss Per Batch: 1.0106318745016074 \t Current Batch Loss:  1.0669587\n",
      "Done Batch: 3100 \tAverage Loss Per Batch: 1.0095103078217553 \t Current Batch Loss:  0.73749906\n",
      "Done Batch: 3150 \tAverage Loss Per Batch: 1.0084644365938684 \t Current Batch Loss:  0.79993814\n",
      "Done Batch: 3200 \tAverage Loss Per Batch: 1.0075037953295733 \t Current Batch Loss:  0.84663993\n",
      "Done Batch: 3250 \tAverage Loss Per Batch: 1.006235711466749 \t Current Batch Loss:  0.92073804\n",
      "Done Batch: 3300 \tAverage Loss Per Batch: 1.0057057993623642 \t Current Batch Loss:  1.0139024\n",
      "Done Batch: 3350 \tAverage Loss Per Batch: 1.0056324070049947 \t Current Batch Loss:  0.97106814\n",
      "Epoch: 20 \tTime: 4384.073119401932 \tAverage Loss Per Batch:: 1.0065601611919845\n",
      "Done Batch: 0 \tAverage Loss Per Batch: 0.5654725432395935 \t Current Batch Loss:  0.56547254\n",
      "Done Batch: 50 \tAverage Loss Per Batch: 0.9088921991049075 \t Current Batch Loss:  0.8969609\n",
      "Done Batch: 100 \tAverage Loss Per Batch: 0.9053737243803421 \t Current Batch Loss:  0.88412225\n",
      "Done Batch: 150 \tAverage Loss Per Batch: 0.9033319760632041 \t Current Batch Loss:  0.91624415\n",
      "Done Batch: 200 \tAverage Loss Per Batch: 0.909878824777271 \t Current Batch Loss:  0.9607435\n",
      "Done Batch: 250 \tAverage Loss Per Batch: 0.9242492394143367 \t Current Batch Loss:  0.82056427\n",
      "Done Batch: 300 \tAverage Loss Per Batch: 0.9359972746269252 \t Current Batch Loss:  0.9897494\n",
      "Done Batch: 350 \tAverage Loss Per Batch: 0.9416046202012957 \t Current Batch Loss:  1.020807\n",
      "Done Batch: 400 \tAverage Loss Per Batch: 0.9554719229291502 \t Current Batch Loss:  0.8521505\n",
      "Done Batch: 450 \tAverage Loss Per Batch: 0.949549839264009 \t Current Batch Loss:  1.0433797\n",
      "Done Batch: 500 \tAverage Loss Per Batch: 0.9480631933240833 \t Current Batch Loss:  1.0234224\n",
      "Done Batch: 550 \tAverage Loss Per Batch: 0.9442319966271222 \t Current Batch Loss:  0.8578715\n",
      "Done Batch: 600 \tAverage Loss Per Batch: 0.9423455445222966 \t Current Batch Loss:  0.78424853\n",
      "Done Batch: 650 \tAverage Loss Per Batch: 0.9453288441246372 \t Current Batch Loss:  0.93073463\n",
      "Done Batch: 700 \tAverage Loss Per Batch: 0.9449218554095433 \t Current Batch Loss:  0.7650993\n",
      "Done Batch: 750 \tAverage Loss Per Batch: 0.943818853317342 \t Current Batch Loss:  0.8520932\n",
      "Done Batch: 800 \tAverage Loss Per Batch: 0.9420694443914626 \t Current Batch Loss:  0.69813246\n",
      "Done Batch: 850 \tAverage Loss Per Batch: 0.9394142954806183 \t Current Batch Loss:  1.0764642\n",
      "Done Batch: 900 \tAverage Loss Per Batch: 0.938192775723143 \t Current Batch Loss:  0.74270445\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done Batch: 950 \tAverage Loss Per Batch: 0.9394300142547937 \t Current Batch Loss:  0.9709457\n",
      "Done Batch: 1000 \tAverage Loss Per Batch: 0.9422271569649299 \t Current Batch Loss:  0.9637573\n",
      "Done Batch: 1050 \tAverage Loss Per Batch: 0.9450436627490536 \t Current Batch Loss:  1.0756383\n",
      "Done Batch: 1100 \tAverage Loss Per Batch: 0.944208137562446 \t Current Batch Loss:  0.9330766\n",
      "Done Batch: 1150 \tAverage Loss Per Batch: 0.9453773564302227 \t Current Batch Loss:  1.002827\n",
      "Done Batch: 1200 \tAverage Loss Per Batch: 0.9459983496542874 \t Current Batch Loss:  0.7220034\n",
      "Done Batch: 1250 \tAverage Loss Per Batch: 0.9453236824221652 \t Current Batch Loss:  1.0870068\n",
      "Done Batch: 1300 \tAverage Loss Per Batch: 0.9425873695475426 \t Current Batch Loss:  0.9310077\n",
      "Done Batch: 1350 \tAverage Loss Per Batch: 0.9405015933787179 \t Current Batch Loss:  0.937371\n",
      "Done Batch: 1400 \tAverage Loss Per Batch: 0.9391698091477687 \t Current Batch Loss:  1.3960207\n",
      "Done Batch: 1450 \tAverage Loss Per Batch: 0.9361311711170688 \t Current Batch Loss:  0.9410871\n",
      "Done Batch: 1500 \tAverage Loss Per Batch: 0.9333023623495719 \t Current Batch Loss:  0.9859385\n",
      "Done Batch: 1550 \tAverage Loss Per Batch: 0.9301419039836321 \t Current Batch Loss:  0.80043244\n",
      "Done Batch: 1600 \tAverage Loss Per Batch: 0.928464197278544 \t Current Batch Loss:  0.9963913\n",
      "Done Batch: 1650 \tAverage Loss Per Batch: 0.9265574141750186 \t Current Batch Loss:  1.0774217\n",
      "Done Batch: 1700 \tAverage Loss Per Batch: 0.9253676344477941 \t Current Batch Loss:  0.94513756\n",
      "Done Batch: 1750 \tAverage Loss Per Batch: 0.9222670787406063 \t Current Batch Loss:  0.84825945\n",
      "Done Batch: 1800 \tAverage Loss Per Batch: 0.9206232543391429 \t Current Batch Loss:  0.943304\n",
      "Done Batch: 1850 \tAverage Loss Per Batch: 0.9195558970068546 \t Current Batch Loss:  0.94313693\n",
      "Done Batch: 1900 \tAverage Loss Per Batch: 0.9178405489688294 \t Current Batch Loss:  0.85940117\n",
      "Done Batch: 1950 \tAverage Loss Per Batch: 0.915736987858415 \t Current Batch Loss:  0.7874269\n",
      "Done Batch: 2000 \tAverage Loss Per Batch: 0.9133620113208852 \t Current Batch Loss:  0.9967946\n",
      "Done Batch: 2050 \tAverage Loss Per Batch: 0.91176513783354 \t Current Batch Loss:  0.87018853\n",
      "Done Batch: 2100 \tAverage Loss Per Batch: 0.9105303317861634 \t Current Batch Loss:  0.863667\n",
      "Done Batch: 2150 \tAverage Loss Per Batch: 0.9082194981271309 \t Current Batch Loss:  0.93857145\n",
      "Done Batch: 2200 \tAverage Loss Per Batch: 0.9057679416942683 \t Current Batch Loss:  0.885997\n",
      "Done Batch: 2250 \tAverage Loss Per Batch: 0.9045853650156841 \t Current Batch Loss:  0.8283669\n",
      "Done Batch: 2300 \tAverage Loss Per Batch: 0.9037378725540322 \t Current Batch Loss:  0.91575396\n",
      "Done Batch: 2350 \tAverage Loss Per Batch: 0.9033424871645295 \t Current Batch Loss:  0.9606113\n",
      "Done Batch: 2400 \tAverage Loss Per Batch: 0.902318231454247 \t Current Batch Loss:  0.54130316\n",
      "Done Batch: 2450 \tAverage Loss Per Batch: 0.9013366166765181 \t Current Batch Loss:  0.61412376\n",
      "Done Batch: 2500 \tAverage Loss Per Batch: 0.9004374397058384 \t Current Batch Loss:  0.79469895\n",
      "Done Batch: 2550 \tAverage Loss Per Batch: 0.8996170527067712 \t Current Batch Loss:  0.7095368\n",
      "Done Batch: 2600 \tAverage Loss Per Batch: 0.8979250502352621 \t Current Batch Loss:  0.88252723\n",
      "Done Batch: 2650 \tAverage Loss Per Batch: 0.8965916702518909 \t Current Batch Loss:  0.9354983\n",
      "Done Batch: 2700 \tAverage Loss Per Batch: 0.896115336174084 \t Current Batch Loss:  1.0451449\n",
      "Done Batch: 2750 \tAverage Loss Per Batch: 0.8950667017938787 \t Current Batch Loss:  0.8463712\n",
      "Done Batch: 2800 \tAverage Loss Per Batch: 0.8940252279094695 \t Current Batch Loss:  1.0092791\n",
      "Done Batch: 2850 \tAverage Loss Per Batch: 0.8929510548297919 \t Current Batch Loss:  0.7705915\n",
      "Done Batch: 2900 \tAverage Loss Per Batch: 0.8920062738054992 \t Current Batch Loss:  0.63536155\n",
      "Done Batch: 2950 \tAverage Loss Per Batch: 0.8916229021747166 \t Current Batch Loss:  0.81157076\n",
      "Done Batch: 3000 \tAverage Loss Per Batch: 0.8908254967177561 \t Current Batch Loss:  1.1311666\n",
      "Done Batch: 3050 \tAverage Loss Per Batch: 0.8908682323330467 \t Current Batch Loss:  0.697679\n",
      "Done Batch: 3100 \tAverage Loss Per Batch: 0.8904149568646003 \t Current Batch Loss:  0.55239135\n",
      "Done Batch: 3150 \tAverage Loss Per Batch: 0.8896554165210319 \t Current Batch Loss:  0.9490815\n",
      "Done Batch: 3200 \tAverage Loss Per Batch: 0.8894162268163412 \t Current Batch Loss:  0.8936727\n",
      "Done Batch: 3250 \tAverage Loss Per Batch: 0.8888311830347261 \t Current Batch Loss:  0.9282532\n",
      "Done Batch: 3300 \tAverage Loss Per Batch: 0.8887859157308309 \t Current Batch Loss:  0.92442816\n",
      "Done Batch: 3350 \tAverage Loss Per Batch: 0.8889011553919731 \t Current Batch Loss:  1.0695916\n",
      "Epoch: 21 \tTime: 4378.680444717407 \tAverage Loss Per Batch:: 0.8890510031287947\n",
      "Done Batch: 0 \tAverage Loss Per Batch: 0.5942214131355286 \t Current Batch Loss:  0.5942214\n",
      "Done Batch: 50 \tAverage Loss Per Batch: 0.8156246128035527 \t Current Batch Loss:  1.1271282\n",
      "Done Batch: 100 \tAverage Loss Per Batch: 0.8244415459656479 \t Current Batch Loss:  0.8952012\n",
      "Done Batch: 150 \tAverage Loss Per Batch: 0.8241860607996682 \t Current Batch Loss:  0.89961004\n",
      "Done Batch: 200 \tAverage Loss Per Batch: 0.828021385331652 \t Current Batch Loss:  1.236214\n",
      "Done Batch: 250 \tAverage Loss Per Batch: 0.8368132870748224 \t Current Batch Loss:  0.8696387\n",
      "Done Batch: 300 \tAverage Loss Per Batch: 0.8439213775123077 \t Current Batch Loss:  0.7775876\n",
      "Done Batch: 350 \tAverage Loss Per Batch: 0.8469582421657367 \t Current Batch Loss:  0.6453143\n",
      "Done Batch: 400 \tAverage Loss Per Batch: 0.8560209139772781 \t Current Batch Loss:  0.6535997\n",
      "Done Batch: 450 \tAverage Loss Per Batch: 0.8554713886619407 \t Current Batch Loss:  1.2262473\n",
      "Done Batch: 500 \tAverage Loss Per Batch: 0.8551683579971214 \t Current Batch Loss:  0.74361145\n",
      "Done Batch: 550 \tAverage Loss Per Batch: 0.8541192684437532 \t Current Batch Loss:  0.87831855\n",
      "Done Batch: 600 \tAverage Loss Per Batch: 0.8541554073724096 \t Current Batch Loss:  0.873984\n",
      "Done Batch: 650 \tAverage Loss Per Batch: 0.8565828734279228 \t Current Batch Loss:  0.8844615\n",
      "Done Batch: 700 \tAverage Loss Per Batch: 0.8544046014929975 \t Current Batch Loss:  0.7388328\n",
      "Done Batch: 750 \tAverage Loss Per Batch: 0.8529527361954894 \t Current Batch Loss:  1.0126317\n",
      "Done Batch: 800 \tAverage Loss Per Batch: 0.8522116740544637 \t Current Batch Loss:  0.7121465\n",
      "Done Batch: 850 \tAverage Loss Per Batch: 0.8510766323528894 \t Current Batch Loss:  0.9336544\n",
      "Done Batch: 900 \tAverage Loss Per Batch: 0.8518095008541027 \t Current Batch Loss:  0.719332\n",
      "Done Batch: 950 \tAverage Loss Per Batch: 0.8546065027530763 \t Current Batch Loss:  1.0042074\n",
      "Done Batch: 1000 \tAverage Loss Per Batch: 0.8555779776254019 \t Current Batch Loss:  0.67107534\n",
      "Done Batch: 1050 \tAverage Loss Per Batch: 0.8562085980693007 \t Current Batch Loss:  0.8402287\n",
      "Done Batch: 1100 \tAverage Loss Per Batch: 0.8552414000196743 \t Current Batch Loss:  0.93500257\n",
      "Done Batch: 1150 \tAverage Loss Per Batch: 0.8553482849218864 \t Current Batch Loss:  0.64001566\n",
      "Done Batch: 1200 \tAverage Loss Per Batch: 0.855182453208323 \t Current Batch Loss:  0.89514714\n",
      "Done Batch: 1250 \tAverage Loss Per Batch: 0.8538586945175457 \t Current Batch Loss:  0.9201116\n",
      "Done Batch: 1300 \tAverage Loss Per Batch: 0.8514846040137817 \t Current Batch Loss:  0.6824952\n",
      "Done Batch: 1350 \tAverage Loss Per Batch: 0.8497850216820362 \t Current Batch Loss:  0.83276004\n",
      "Done Batch: 1400 \tAverage Loss Per Batch: 0.8463079407332541 \t Current Batch Loss:  0.7561618\n",
      "Done Batch: 1450 \tAverage Loss Per Batch: 0.8436499796268613 \t Current Batch Loss:  0.79629225\n",
      "Done Batch: 1500 \tAverage Loss Per Batch: 0.8410933817489238 \t Current Batch Loss:  0.9231878\n",
      "Done Batch: 1550 \tAverage Loss Per Batch: 0.8395545594081965 \t Current Batch Loss:  0.7375208\n",
      "Done Batch: 1600 \tAverage Loss Per Batch: 0.8375391374894189 \t Current Batch Loss:  0.74944806\n",
      "Done Batch: 1650 \tAverage Loss Per Batch: 0.836725594526634 \t Current Batch Loss:  0.69241095\n",
      "Done Batch: 1700 \tAverage Loss Per Batch: 0.8347548157379671 \t Current Batch Loss:  0.7666399\n",
      "Done Batch: 1750 \tAverage Loss Per Batch: 0.8323574854536508 \t Current Batch Loss:  0.78213096\n",
      "Done Batch: 1800 \tAverage Loss Per Batch: 0.8303407331031935 \t Current Batch Loss:  0.6577467\n",
      "Done Batch: 1850 \tAverage Loss Per Batch: 0.8285469645297185 \t Current Batch Loss:  0.8782547\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done Batch: 1900 \tAverage Loss Per Batch: 0.8264333355558476 \t Current Batch Loss:  0.8607304\n",
      "Done Batch: 1950 \tAverage Loss Per Batch: 0.8238538335770109 \t Current Batch Loss:  0.8099582\n",
      "Done Batch: 2000 \tAverage Loss Per Batch: 0.8223121874574302 \t Current Batch Loss:  0.87082916\n",
      "Done Batch: 2050 \tAverage Loss Per Batch: 0.8212931563887929 \t Current Batch Loss:  0.82653487\n",
      "Done Batch: 2100 \tAverage Loss Per Batch: 0.8197375545326951 \t Current Batch Loss:  0.570082\n",
      "Done Batch: 2150 \tAverage Loss Per Batch: 0.8183548404886688 \t Current Batch Loss:  0.86093295\n",
      "Done Batch: 2200 \tAverage Loss Per Batch: 0.8165167079817864 \t Current Batch Loss:  0.73403066\n",
      "Done Batch: 2250 \tAverage Loss Per Batch: 0.8152551103676229 \t Current Batch Loss:  0.7705032\n",
      "Done Batch: 2300 \tAverage Loss Per Batch: 0.8142683834990229 \t Current Batch Loss:  0.64696676\n",
      "Done Batch: 2350 \tAverage Loss Per Batch: 0.8138622955178667 \t Current Batch Loss:  0.6903264\n",
      "Done Batch: 2400 \tAverage Loss Per Batch: 0.8133337154630719 \t Current Batch Loss:  0.61393416\n",
      "Done Batch: 2450 \tAverage Loss Per Batch: 0.8127118988895066 \t Current Batch Loss:  0.91772443\n",
      "Done Batch: 2500 \tAverage Loss Per Batch: 0.8116074439669932 \t Current Batch Loss:  0.70780635\n",
      "Done Batch: 2550 \tAverage Loss Per Batch: 0.8104727727111672 \t Current Batch Loss:  0.60411763\n",
      "Done Batch: 2600 \tAverage Loss Per Batch: 0.8098749461677248 \t Current Batch Loss:  0.77534485\n",
      "Done Batch: 2650 \tAverage Loss Per Batch: 0.8087593580654818 \t Current Batch Loss:  0.8117532\n",
      "Done Batch: 2700 \tAverage Loss Per Batch: 0.8078160447043341 \t Current Batch Loss:  0.63157624\n",
      "Done Batch: 2750 \tAverage Loss Per Batch: 0.8070414574395003 \t Current Batch Loss:  0.8073463\n",
      "Done Batch: 2800 \tAverage Loss Per Batch: 0.8061081801023453 \t Current Batch Loss:  0.75795084\n",
      "Done Batch: 2850 \tAverage Loss Per Batch: 0.804591796192024 \t Current Batch Loss:  0.9049284\n",
      "Done Batch: 2900 \tAverage Loss Per Batch: 0.8036951215482506 \t Current Batch Loss:  0.6187088\n",
      "Done Batch: 2950 \tAverage Loss Per Batch: 0.8035333007220614 \t Current Batch Loss:  0.47701234\n",
      "Done Batch: 3000 \tAverage Loss Per Batch: 0.8026098997959968 \t Current Batch Loss:  0.75513345\n",
      "Done Batch: 3050 \tAverage Loss Per Batch: 0.8024268905692943 \t Current Batch Loss:  0.6378315\n",
      "Done Batch: 3100 \tAverage Loss Per Batch: 0.8018619002007316 \t Current Batch Loss:  0.7761331\n",
      "Done Batch: 3150 \tAverage Loss Per Batch: 0.8010914261588518 \t Current Batch Loss:  0.9556718\n",
      "Done Batch: 3200 \tAverage Loss Per Batch: 0.7999010389668983 \t Current Batch Loss:  0.52802837\n",
      "Done Batch: 3250 \tAverage Loss Per Batch: 0.7991686881651918 \t Current Batch Loss:  0.5843509\n",
      "Done Batch: 3300 \tAverage Loss Per Batch: 0.7992685831467046 \t Current Batch Loss:  0.81649625\n",
      "Done Batch: 3350 \tAverage Loss Per Batch: 0.7993398111227981 \t Current Batch Loss:  0.88619727\n",
      "Epoch: 22 \tTime: 4384.606492757797 \tAverage Loss Per Batch:: 0.7995264420824091\n",
      "Done Batch: 0 \tAverage Loss Per Batch: 0.5745160579681396 \t Current Batch Loss:  0.57451606\n",
      "Done Batch: 50 \tAverage Loss Per Batch: 0.7459387861046136 \t Current Batch Loss:  0.7456096\n",
      "Done Batch: 100 \tAverage Loss Per Batch: 0.7478732983074566 \t Current Batch Loss:  0.69888896\n",
      "Done Batch: 150 \tAverage Loss Per Batch: 0.7471067883715724 \t Current Batch Loss:  0.61388326\n",
      "Done Batch: 200 \tAverage Loss Per Batch: 0.7556238654834121 \t Current Batch Loss:  0.7619183\n",
      "Done Batch: 250 \tAverage Loss Per Batch: 0.7583008816992619 \t Current Batch Loss:  0.7671509\n",
      "Done Batch: 300 \tAverage Loss Per Batch: 0.7567722038969249 \t Current Batch Loss:  0.7809424\n",
      "Done Batch: 350 \tAverage Loss Per Batch: 0.761137967096095 \t Current Batch Loss:  0.8046648\n",
      "Done Batch: 400 \tAverage Loss Per Batch: 0.7697750991419366 \t Current Batch Loss:  0.69500333\n",
      "Done Batch: 450 \tAverage Loss Per Batch: 0.7712701492986235 \t Current Batch Loss:  0.80736345\n",
      "Done Batch: 500 \tAverage Loss Per Batch: 0.767564551023666 \t Current Batch Loss:  0.80859286\n",
      "Done Batch: 550 \tAverage Loss Per Batch: 0.7697079591115027 \t Current Batch Loss:  0.7096646\n",
      "Done Batch: 600 \tAverage Loss Per Batch: 0.7675629402456585 \t Current Batch Loss:  0.7045827\n",
      "Done Batch: 650 \tAverage Loss Per Batch: 0.768801133249945 \t Current Batch Loss:  0.7474705\n",
      "Done Batch: 700 \tAverage Loss Per Batch: 0.7702695091763849 \t Current Batch Loss:  0.7266527\n",
      "Done Batch: 750 \tAverage Loss Per Batch: 0.7691361452149011 \t Current Batch Loss:  0.91727567\n",
      "Done Batch: 800 \tAverage Loss Per Batch: 0.7700588076227762 \t Current Batch Loss:  0.9396912\n",
      "Done Batch: 850 \tAverage Loss Per Batch: 0.7705180131240401 \t Current Batch Loss:  0.74380594\n",
      "Done Batch: 900 \tAverage Loss Per Batch: 0.7713478578380686 \t Current Batch Loss:  0.8369286\n",
      "Done Batch: 950 \tAverage Loss Per Batch: 0.7760500424981243 \t Current Batch Loss:  0.88553697\n",
      "Done Batch: 1000 \tAverage Loss Per Batch: 0.7811033493810362 \t Current Batch Loss:  0.66938967\n",
      "Done Batch: 1050 \tAverage Loss Per Batch: 0.7812755298433023 \t Current Batch Loss:  0.7366799\n",
      "Done Batch: 1100 \tAverage Loss Per Batch: 0.7809626063252448 \t Current Batch Loss:  0.838258\n",
      "Done Batch: 1150 \tAverage Loss Per Batch: 0.7794395342896443 \t Current Batch Loss:  0.7290099\n",
      "Done Batch: 1200 \tAverage Loss Per Batch: 0.7781691846005824 \t Current Batch Loss:  0.75222266\n",
      "Done Batch: 1250 \tAverage Loss Per Batch: 0.7783254690879254 \t Current Batch Loss:  0.85387903\n",
      "Done Batch: 1300 \tAverage Loss Per Batch: 0.7770828440453253 \t Current Batch Loss:  0.4992917\n",
      "Done Batch: 1350 \tAverage Loss Per Batch: 0.7768365222378892 \t Current Batch Loss:  0.5635148\n",
      "Done Batch: 1400 \tAverage Loss Per Batch: 0.7744048759819864 \t Current Batch Loss:  0.848568\n",
      "Done Batch: 1450 \tAverage Loss Per Batch: 0.772116427603136 \t Current Batch Loss:  0.94234353\n",
      "Done Batch: 1500 \tAverage Loss Per Batch: 0.7716983935183322 \t Current Batch Loss:  0.68611854\n",
      "Done Batch: 1550 \tAverage Loss Per Batch: 0.7696647169242591 \t Current Batch Loss:  0.7094285\n",
      "Done Batch: 1600 \tAverage Loss Per Batch: 0.7696006794150064 \t Current Batch Loss:  0.97546005\n",
      "Done Batch: 1650 \tAverage Loss Per Batch: 0.7670348769855961 \t Current Batch Loss:  0.67764527\n",
      "Done Batch: 1700 \tAverage Loss Per Batch: 0.7652332047586368 \t Current Batch Loss:  0.6491037\n",
      "Done Batch: 1750 \tAverage Loss Per Batch: 0.7624962060775572 \t Current Batch Loss:  0.7217367\n",
      "Done Batch: 1800 \tAverage Loss Per Batch: 0.7604354470554555 \t Current Batch Loss:  0.7649441\n",
      "Done Batch: 1850 \tAverage Loss Per Batch: 0.7578346655859554 \t Current Batch Loss:  0.7568483\n",
      "Done Batch: 1900 \tAverage Loss Per Batch: 0.7558464867732576 \t Current Batch Loss:  0.62901026\n",
      "Done Batch: 1950 \tAverage Loss Per Batch: 0.7539209870574781 \t Current Batch Loss:  0.716653\n",
      "Done Batch: 2000 \tAverage Loss Per Batch: 0.7522456562709713 \t Current Batch Loss:  0.81904036\n",
      "Done Batch: 2050 \tAverage Loss Per Batch: 0.7513366722293391 \t Current Batch Loss:  0.64864075\n",
      "Done Batch: 2100 \tAverage Loss Per Batch: 0.7502689937901803 \t Current Batch Loss:  0.641388\n",
      "Done Batch: 2150 \tAverage Loss Per Batch: 0.7477671778628794 \t Current Batch Loss:  0.8951108\n",
      "Done Batch: 2200 \tAverage Loss Per Batch: 0.7460646047910632 \t Current Batch Loss:  0.67326933\n",
      "Done Batch: 2250 \tAverage Loss Per Batch: 0.7449869445460153 \t Current Batch Loss:  0.8478086\n",
      "Done Batch: 2300 \tAverage Loss Per Batch: 0.7435480445973099 \t Current Batch Loss:  0.5112109\n",
      "Done Batch: 2350 \tAverage Loss Per Batch: 0.7424757070284811 \t Current Batch Loss:  0.69316983\n",
      "Done Batch: 2400 \tAverage Loss Per Batch: 0.7421418344264525 \t Current Batch Loss:  0.82826596\n",
      "Done Batch: 2450 \tAverage Loss Per Batch: 0.7416491620470581 \t Current Batch Loss:  0.5337329\n",
      "Done Batch: 2500 \tAverage Loss Per Batch: 0.7412231388043423 \t Current Batch Loss:  0.44474488\n",
      "Done Batch: 2550 \tAverage Loss Per Batch: 0.7403692777532542 \t Current Batch Loss:  0.651193\n",
      "Done Batch: 2600 \tAverage Loss Per Batch: 0.7392883186086422 \t Current Batch Loss:  0.7840349\n",
      "Done Batch: 2650 \tAverage Loss Per Batch: 0.7383935777635585 \t Current Batch Loss:  0.82949466\n",
      "Done Batch: 2700 \tAverage Loss Per Batch: 0.7370713831693408 \t Current Batch Loss:  0.66810435\n",
      "Done Batch: 2750 \tAverage Loss Per Batch: 0.736140637625698 \t Current Batch Loss:  0.69288445\n",
      "Done Batch: 2800 \tAverage Loss Per Batch: 0.7349948045165571 \t Current Batch Loss:  0.7656779\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done Batch: 2850 \tAverage Loss Per Batch: 0.733978340988283 \t Current Batch Loss:  0.61558586\n",
      "Done Batch: 2900 \tAverage Loss Per Batch: 0.7334410329803603 \t Current Batch Loss:  0.7223669\n",
      "Done Batch: 2950 \tAverage Loss Per Batch: 0.7330032961423499 \t Current Batch Loss:  0.892408\n",
      "Done Batch: 3000 \tAverage Loss Per Batch: 0.7332496823508992 \t Current Batch Loss:  0.61247337\n",
      "Done Batch: 3050 \tAverage Loss Per Batch: 0.7332025166907415 \t Current Batch Loss:  0.8580542\n",
      "Done Batch: 3100 \tAverage Loss Per Batch: 0.732673290236386 \t Current Batch Loss:  0.508281\n",
      "Done Batch: 3150 \tAverage Loss Per Batch: 0.7322201263106312 \t Current Batch Loss:  0.7231686\n",
      "Done Batch: 3200 \tAverage Loss Per Batch: 0.731994523047954 \t Current Batch Loss:  0.7122833\n",
      "Done Batch: 3250 \tAverage Loss Per Batch: 0.7312855974082688 \t Current Batch Loss:  0.72283506\n",
      "Done Batch: 3300 \tAverage Loss Per Batch: 0.7309163013251396 \t Current Batch Loss:  0.552601\n",
      "Done Batch: 3350 \tAverage Loss Per Batch: 0.7308603728468545 \t Current Batch Loss:  0.82846284\n",
      "Epoch: 23 \tTime: 4375.525700092316 \tAverage Loss Per Batch:: 0.7309107525362117\n",
      "Done Batch: 0 \tAverage Loss Per Batch: 0.3790983557701111 \t Current Batch Loss:  0.37909836\n",
      "Done Batch: 50 \tAverage Loss Per Batch: 0.6972770638325635 \t Current Batch Loss:  0.79793227\n",
      "Done Batch: 100 \tAverage Loss Per Batch: 0.6942143425492957 \t Current Batch Loss:  0.5955612\n",
      "Done Batch: 150 \tAverage Loss Per Batch: 0.6964258488440356 \t Current Batch Loss:  0.6314276\n",
      "Done Batch: 200 \tAverage Loss Per Batch: 0.70466363889661 \t Current Batch Loss:  0.64335406\n",
      "Done Batch: 250 \tAverage Loss Per Batch: 0.7002820954379807 \t Current Batch Loss:  0.6150164\n",
      "Done Batch: 300 \tAverage Loss Per Batch: 0.7040591814193219 \t Current Batch Loss:  0.7470796\n",
      "Done Batch: 350 \tAverage Loss Per Batch: 0.7038166988269556 \t Current Batch Loss:  0.5328785\n",
      "Done Batch: 400 \tAverage Loss Per Batch: 0.7060593561341341 \t Current Batch Loss:  0.7422755\n",
      "Done Batch: 450 \tAverage Loss Per Batch: 0.703601448736804 \t Current Batch Loss:  0.75208884\n",
      "Done Batch: 500 \tAverage Loss Per Batch: 0.704531661824076 \t Current Batch Loss:  0.7097571\n",
      "Done Batch: 550 \tAverage Loss Per Batch: 0.7045282591168115 \t Current Batch Loss:  0.5049792\n",
      "Done Batch: 600 \tAverage Loss Per Batch: 0.7048548551248432 \t Current Batch Loss:  0.54672605\n",
      "Done Batch: 650 \tAverage Loss Per Batch: 0.7044474341536081 \t Current Batch Loss:  0.5018984\n",
      "Done Batch: 700 \tAverage Loss Per Batch: 0.7047836265278271 \t Current Batch Loss:  0.71225375\n",
      "Done Batch: 750 \tAverage Loss Per Batch: 0.7053103405292754 \t Current Batch Loss:  0.7587864\n",
      "Done Batch: 800 \tAverage Loss Per Batch: 0.7061266374870782 \t Current Batch Loss:  0.50147676\n",
      "Done Batch: 850 \tAverage Loss Per Batch: 0.7059227729305958 \t Current Batch Loss:  0.9366084\n",
      "Done Batch: 900 \tAverage Loss Per Batch: 0.7061010053697622 \t Current Batch Loss:  0.8598831\n",
      "Done Batch: 950 \tAverage Loss Per Batch: 0.7089557675282411 \t Current Batch Loss:  0.7449994\n",
      "Done Batch: 1000 \tAverage Loss Per Batch: 0.7116966437805187 \t Current Batch Loss:  0.68345726\n",
      "Done Batch: 1050 \tAverage Loss Per Batch: 0.712663185131425 \t Current Batch Loss:  0.933017\n",
      "Done Batch: 1100 \tAverage Loss Per Batch: 0.7122244420482504 \t Current Batch Loss:  0.75903213\n",
      "Done Batch: 1150 \tAverage Loss Per Batch: 0.7115264771867068 \t Current Batch Loss:  0.86679214\n",
      "Done Batch: 1200 \tAverage Loss Per Batch: 0.7100718869307356 \t Current Batch Loss:  0.77368474\n",
      "Done Batch: 1250 \tAverage Loss Per Batch: 0.7117700588455398 \t Current Batch Loss:  0.62637115\n",
      "Done Batch: 1300 \tAverage Loss Per Batch: 0.7095916474855102 \t Current Batch Loss:  0.6147747\n",
      "Done Batch: 1350 \tAverage Loss Per Batch: 0.7071752332035124 \t Current Batch Loss:  0.60971534\n",
      "Done Batch: 1400 \tAverage Loss Per Batch: 0.7055918400326769 \t Current Batch Loss:  0.71608925\n",
      "Done Batch: 1450 \tAverage Loss Per Batch: 0.7033417493125473 \t Current Batch Loss:  0.79126954\n",
      "Done Batch: 1500 \tAverage Loss Per Batch: 0.7024415567427933 \t Current Batch Loss:  0.77311903\n",
      "Done Batch: 1550 \tAverage Loss Per Batch: 0.7016468411641763 \t Current Batch Loss:  0.76753634\n",
      "Done Batch: 1600 \tAverage Loss Per Batch: 0.6998634070325538 \t Current Batch Loss:  0.47567803\n",
      "Done Batch: 1650 \tAverage Loss Per Batch: 0.6985499726722343 \t Current Batch Loss:  0.8062422\n",
      "Done Batch: 1700 \tAverage Loss Per Batch: 0.6971009737214644 \t Current Batch Loss:  0.7177937\n",
      "Done Batch: 1750 \tAverage Loss Per Batch: 0.6949793402975726 \t Current Batch Loss:  0.6430887\n",
      "Done Batch: 1800 \tAverage Loss Per Batch: 0.6939925391895119 \t Current Batch Loss:  0.7624504\n",
      "Done Batch: 1850 \tAverage Loss Per Batch: 0.6927016800735397 \t Current Batch Loss:  0.5532034\n",
      "Done Batch: 1900 \tAverage Loss Per Batch: 0.6907034562738992 \t Current Batch Loss:  0.49182424\n",
      "Done Batch: 1950 \tAverage Loss Per Batch: 0.6892278171972639 \t Current Batch Loss:  0.729427\n",
      "Done Batch: 2000 \tAverage Loss Per Batch: 0.6879899251735073 \t Current Batch Loss:  0.6524903\n",
      "Done Batch: 2050 \tAverage Loss Per Batch: 0.6869619370146184 \t Current Batch Loss:  0.8395897\n",
      "Done Batch: 2100 \tAverage Loss Per Batch: 0.6867030031167003 \t Current Batch Loss:  0.60443854\n",
      "Done Batch: 2150 \tAverage Loss Per Batch: 0.6854482593369007 \t Current Batch Loss:  0.66556185\n",
      "Done Batch: 2200 \tAverage Loss Per Batch: 0.6841562019457117 \t Current Batch Loss:  0.6554565\n",
      "Done Batch: 2250 \tAverage Loss Per Batch: 0.6835483642405057 \t Current Batch Loss:  0.5429789\n",
      "Done Batch: 2300 \tAverage Loss Per Batch: 0.6834403288747787 \t Current Batch Loss:  0.48938835\n",
      "Done Batch: 2350 \tAverage Loss Per Batch: 0.6821647111226629 \t Current Batch Loss:  0.6076463\n",
      "Done Batch: 2400 \tAverage Loss Per Batch: 0.6821139073158393 \t Current Batch Loss:  0.4413054\n",
      "Done Batch: 2450 \tAverage Loss Per Batch: 0.6814831397767848 \t Current Batch Loss:  0.6491283\n",
      "Done Batch: 2500 \tAverage Loss Per Batch: 0.6805078631541768 \t Current Batch Loss:  0.81992507\n",
      "Done Batch: 2550 \tAverage Loss Per Batch: 0.6796582365143491 \t Current Batch Loss:  0.5795102\n",
      "Done Batch: 2600 \tAverage Loss Per Batch: 0.67915612022632 \t Current Batch Loss:  0.6381852\n",
      "Done Batch: 2650 \tAverage Loss Per Batch: 0.6786948366576165 \t Current Batch Loss:  0.6414487\n",
      "Done Batch: 2700 \tAverage Loss Per Batch: 0.6775055240918336 \t Current Batch Loss:  0.7345924\n",
      "Done Batch: 2750 \tAverage Loss Per Batch: 0.6770051627408717 \t Current Batch Loss:  0.7003753\n",
      "Done Batch: 2800 \tAverage Loss Per Batch: 0.6766690428407649 \t Current Batch Loss:  0.6577437\n",
      "Done Batch: 2850 \tAverage Loss Per Batch: 0.6761646053741707 \t Current Batch Loss:  0.63101035\n",
      "Done Batch: 2900 \tAverage Loss Per Batch: 0.6757508841455907 \t Current Batch Loss:  0.9581467\n",
      "Done Batch: 2950 \tAverage Loss Per Batch: 0.6754382912868647 \t Current Batch Loss:  0.6553998\n",
      "Done Batch: 3000 \tAverage Loss Per Batch: 0.6749455807150224 \t Current Batch Loss:  0.6931372\n",
      "Done Batch: 3050 \tAverage Loss Per Batch: 0.6746211216630251 \t Current Batch Loss:  0.78030807\n",
      "Done Batch: 3100 \tAverage Loss Per Batch: 0.674516214522113 \t Current Batch Loss:  0.48950484\n",
      "Done Batch: 3150 \tAverage Loss Per Batch: 0.6741591060551262 \t Current Batch Loss:  0.7999238\n",
      "Done Batch: 3200 \tAverage Loss Per Batch: 0.6737039910093466 \t Current Batch Loss:  0.5302152\n",
      "Done Batch: 3250 \tAverage Loss Per Batch: 0.6734512394041474 \t Current Batch Loss:  0.55749345\n",
      "Done Batch: 3300 \tAverage Loss Per Batch: 0.6734012691261768 \t Current Batch Loss:  0.81451875\n",
      "Done Batch: 3350 \tAverage Loss Per Batch: 0.6734992013390972 \t Current Batch Loss:  0.7329538\n",
      "Epoch: 24 \tTime: 4387.3056807518005 \tAverage Loss Per Batch:: 0.673443085544712\n"
     ]
    }
   ],
   "source": [
    "def train(model, learning_rate=0.0001, batch_size=100, epochs=10):\n",
    "    \n",
    "    # defining criterion and optimizer\n",
    "    criterion = nn.NLLLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr = learning_rate)\n",
    "    \n",
    "    number_of_batches = math.ceil(len(train_image_addresses)/batch_size)\n",
    "    indexes = np.arange(len(train_image_addresses))\n",
    "    \n",
    "    loss_arr = []\n",
    "    for epoch in range(20, 20 + epochs):\n",
    "        avgloss = 0.0\n",
    "        start = time.time()\n",
    "        avg_loss_arr = []\n",
    "        for batch in range(number_of_batches):\n",
    "            train_indexes = [train_image_addresses[i] for i in indexes[batch*batch_size:(batch+1)*batch_size]]\n",
    "            image_batch, target_batch = load_batch(train_indexes)\n",
    "            loss = train_pass(image_batch, target_batch, model, optimizer, criterion)\n",
    "            avgloss += loss\n",
    "            if batch%50 == 0:\n",
    "                print (\"Done Batch:\", batch, \"\\tAverage Loss Per Batch:\", avgloss/(batch+1), \"\\t Current Batch Loss: \", loss)\n",
    "        loss_arr.append(avgloss/(batch+1))\n",
    "        print (\"Epoch:\",epoch, \"\\tTime:\", time.time() - start, \"\\tAverage Loss Per Batch::\", avgloss/(batch+1))\n",
    "        torch.save({'epoch': epoch ,'state_dict': model.state_dict(),'optimizer': optimizer.state_dict()}, open(\"outputs/anp_log_softmax_classifier_batch_\"+str(epoch), \"wb+\"))\n",
    "    loss_arr = np.array(loss_arr)\n",
    "    np.save(open('outputs/loss_log_softmax_anp_classifier', 'wb+'), loss_arr)\n",
    "    \n",
    "train(main_model, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done Batch: 0 \tAverage Loss Per Batch: 0.6470800042152405 \t Current Batch Loss:  0.64708\n",
      "Done Batch: 50 \tAverage Loss Per Batch: 0.6920763604781207 \t Current Batch Loss:  0.76685274\n",
      "Done Batch: 100 \tAverage Loss Per Batch: 0.692944719354705 \t Current Batch Loss:  0.62695813\n",
      "Done Batch: 150 \tAverage Loss Per Batch: 0.6876551672717593 \t Current Batch Loss:  0.63014597\n",
      "Done Batch: 200 \tAverage Loss Per Batch: 0.6869773889952038 \t Current Batch Loss:  0.70445055\n",
      "Done Batch: 250 \tAverage Loss Per Batch: 0.6843444137459257 \t Current Batch Loss:  0.74377525\n",
      "Done Batch: 300 \tAverage Loss Per Batch: 0.6822819361258979 \t Current Batch Loss:  0.5881488\n",
      "Done Batch: 350 \tAverage Loss Per Batch: 0.6809987730619914 \t Current Batch Loss:  0.54499\n",
      "Done Batch: 400 \tAverage Loss Per Batch: 0.6826667121966878 \t Current Batch Loss:  0.7139361\n",
      "Done Batch: 450 \tAverage Loss Per Batch: 0.6769185549932679 \t Current Batch Loss:  0.5120722\n",
      "Done Batch: 500 \tAverage Loss Per Batch: 0.6733415379614649 \t Current Batch Loss:  0.6121265\n",
      "Done Batch: 550 \tAverage Loss Per Batch: 0.6692869563717159 \t Current Batch Loss:  0.69944113\n",
      "Done Batch: 600 \tAverage Loss Per Batch: 0.6661964099538108 \t Current Batch Loss:  0.5019494\n",
      "Done Batch: 650 \tAverage Loss Per Batch: 0.6643186214332757 \t Current Batch Loss:  0.57006097\n",
      "Done Batch: 700 \tAverage Loss Per Batch: 0.6607837939738547 \t Current Batch Loss:  0.64059234\n",
      "Done Batch: 750 \tAverage Loss Per Batch: 0.6601466859942587 \t Current Batch Loss:  0.51647073\n",
      "Done Batch: 800 \tAverage Loss Per Batch: 0.6604707792829188 \t Current Batch Loss:  0.7503172\n",
      "Done Batch: 850 \tAverage Loss Per Batch: 0.6596107310960212 \t Current Batch Loss:  0.6721263\n",
      "Done Batch: 900 \tAverage Loss Per Batch: 0.659551541561821 \t Current Batch Loss:  0.5484305\n",
      "Done Batch: 950 \tAverage Loss Per Batch: 0.6592290349437862 \t Current Batch Loss:  0.679733\n",
      "Done Batch: 1000 \tAverage Loss Per Batch: 0.6602473278026599 \t Current Batch Loss:  0.4892548\n",
      "Done Batch: 1050 \tAverage Loss Per Batch: 0.6601618518157872 \t Current Batch Loss:  0.72224134\n",
      "Done Batch: 1100 \tAverage Loss Per Batch: 0.6595564853268033 \t Current Batch Loss:  0.838614\n",
      "Done Batch: 1150 \tAverage Loss Per Batch: 0.659445948128489 \t Current Batch Loss:  0.6288754\n",
      "Done Batch: 1200 \tAverage Loss Per Batch: 0.6588007424842507 \t Current Batch Loss:  0.58849984\n",
      "Done Batch: 1250 \tAverage Loss Per Batch: 0.6580740370386415 \t Current Batch Loss:  0.7225377\n",
      "Done Batch: 1300 \tAverage Loss Per Batch: 0.6566954248231892 \t Current Batch Loss:  0.64514714\n",
      "Done Batch: 1350 \tAverage Loss Per Batch: 0.6555481410573625 \t Current Batch Loss:  0.48484504\n",
      "Done Batch: 1400 \tAverage Loss Per Batch: 0.6535899013514862 \t Current Batch Loss:  0.7977226\n",
      "Done Batch: 1450 \tAverage Loss Per Batch: 0.6526549644383128 \t Current Batch Loss:  0.4634983\n",
      "Done Batch: 1500 \tAverage Loss Per Batch: 0.6512059714498717 \t Current Batch Loss:  0.6943648\n",
      "Done Batch: 1550 \tAverage Loss Per Batch: 0.6507555739791988 \t Current Batch Loss:  0.5725103\n",
      "Done Batch: 1600 \tAverage Loss Per Batch: 0.6499728356243147 \t Current Batch Loss:  0.6427974\n",
      "Done Batch: 1650 \tAverage Loss Per Batch: 0.6496127675801318 \t Current Batch Loss:  0.5301562\n",
      "Done Batch: 1700 \tAverage Loss Per Batch: 0.6490822544453916 \t Current Batch Loss:  0.8121107\n",
      "Done Batch: 1750 \tAverage Loss Per Batch: 0.6481186795547851 \t Current Batch Loss:  0.8159608\n",
      "Done Batch: 1800 \tAverage Loss Per Batch: 0.6477069917948362 \t Current Batch Loss:  0.73051375\n",
      "Done Batch: 1850 \tAverage Loss Per Batch: 0.6465423787883138 \t Current Batch Loss:  0.6160426\n",
      "Done Batch: 1900 \tAverage Loss Per Batch: 0.6451490633209275 \t Current Batch Loss:  0.5669119\n",
      "Done Batch: 1950 \tAverage Loss Per Batch: 0.6433236552989525 \t Current Batch Loss:  0.5864601\n",
      "Done Batch: 2000 \tAverage Loss Per Batch: 0.6418700527989942 \t Current Batch Loss:  0.7226862\n",
      "Done Batch: 2050 \tAverage Loss Per Batch: 0.6424790962128335 \t Current Batch Loss:  0.80110174\n",
      "Done Batch: 2100 \tAverage Loss Per Batch: 0.6425466009374462 \t Current Batch Loss:  0.6128775\n",
      "Done Batch: 2150 \tAverage Loss Per Batch: 0.6414849580802899 \t Current Batch Loss:  0.756069\n",
      "Done Batch: 2200 \tAverage Loss Per Batch: 0.6402237232383519 \t Current Batch Loss:  0.58672076\n",
      "Done Batch: 2250 \tAverage Loss Per Batch: 0.6392025314983079 \t Current Batch Loss:  0.7403521\n",
      "Done Batch: 2300 \tAverage Loss Per Batch: 0.638313403418664 \t Current Batch Loss:  0.58387095\n",
      "Done Batch: 2350 \tAverage Loss Per Batch: 0.637676641085157 \t Current Batch Loss:  0.456577\n",
      "Done Batch: 2400 \tAverage Loss Per Batch: 0.6368166699428351 \t Current Batch Loss:  0.33488637\n",
      "Done Batch: 2450 \tAverage Loss Per Batch: 0.6361255806618834 \t Current Batch Loss:  0.5504105\n",
      "Done Batch: 2500 \tAverage Loss Per Batch: 0.6354680395231205 \t Current Batch Loss:  0.7427295\n",
      "Done Batch: 2550 \tAverage Loss Per Batch: 0.635006136132053 \t Current Batch Loss:  0.6589769\n",
      "Done Batch: 2600 \tAverage Loss Per Batch: 0.6339258528910706 \t Current Batch Loss:  0.49453363\n",
      "Done Batch: 2650 \tAverage Loss Per Batch: 0.6340026905270443 \t Current Batch Loss:  0.6789906\n",
      "Done Batch: 2700 \tAverage Loss Per Batch: 0.633628269669216 \t Current Batch Loss:  0.59491384\n",
      "Done Batch: 2750 \tAverage Loss Per Batch: 0.6332715065836343 \t Current Batch Loss:  0.55864537\n",
      "Done Batch: 2800 \tAverage Loss Per Batch: 0.6327678395228401 \t Current Batch Loss:  0.6636969\n",
      "Done Batch: 2850 \tAverage Loss Per Batch: 0.6318857308773775 \t Current Batch Loss:  0.743615\n",
      "Done Batch: 2900 \tAverage Loss Per Batch: 0.6311354425211522 \t Current Batch Loss:  0.5031062\n",
      "Done Batch: 2950 \tAverage Loss Per Batch: 0.6305978891405967 \t Current Batch Loss:  0.63269717\n",
      "Done Batch: 3000 \tAverage Loss Per Batch: 0.6296697723909205 \t Current Batch Loss:  0.7084722\n",
      "Done Batch: 3050 \tAverage Loss Per Batch: 0.629418371711079 \t Current Batch Loss:  0.46396402\n",
      "Done Batch: 3100 \tAverage Loss Per Batch: 0.6288624145799359 \t Current Batch Loss:  0.5283126\n",
      "Done Batch: 3150 \tAverage Loss Per Batch: 0.628539603235456 \t Current Batch Loss:  0.5612005\n",
      "Done Batch: 3200 \tAverage Loss Per Batch: 0.6283560706670863 \t Current Batch Loss:  0.72490823\n",
      "Done Batch: 3250 \tAverage Loss Per Batch: 0.6277789872028541 \t Current Batch Loss:  0.7718568\n",
      "Done Batch: 3300 \tAverage Loss Per Batch: 0.6278309415539479 \t Current Batch Loss:  0.6036478\n",
      "Done Batch: 3350 \tAverage Loss Per Batch: 0.6279972318534316 \t Current Batch Loss:  0.64393234\n",
      "Epoch: 25 \tTime: 4384.077514886856 \tAverage Loss Per Batch:: 0.628444558427176\n",
      "Done Batch: 0 \tAverage Loss Per Batch: 0.38031005859375 \t Current Batch Loss:  0.38031006\n",
      "Done Batch: 50 \tAverage Loss Per Batch: 0.57936176423933 \t Current Batch Loss:  0.6199857\n",
      "Done Batch: 100 \tAverage Loss Per Batch: 0.5846554743181361 \t Current Batch Loss:  0.5148196\n",
      "Done Batch: 150 \tAverage Loss Per Batch: 0.5924498160548558 \t Current Batch Loss:  0.7118078\n",
      "Done Batch: 200 \tAverage Loss Per Batch: 0.5919352144151184 \t Current Batch Loss:  0.717525\n",
      "Done Batch: 250 \tAverage Loss Per Batch: 0.5989365350915141 \t Current Batch Loss:  0.47267452\n",
      "Done Batch: 300 \tAverage Loss Per Batch: 0.603486086046973 \t Current Batch Loss:  0.61702836\n",
      "Done Batch: 350 \tAverage Loss Per Batch: 0.6032427564645425 \t Current Batch Loss:  0.77288127\n",
      "Done Batch: 400 \tAverage Loss Per Batch: 0.610214950660815 \t Current Batch Loss:  0.6752513\n",
      "Done Batch: 450 \tAverage Loss Per Batch: 0.6106841655369608 \t Current Batch Loss:  0.73798525\n",
      "Done Batch: 500 \tAverage Loss Per Batch: 0.6106860215316514 \t Current Batch Loss:  0.5795109\n",
      "Done Batch: 550 \tAverage Loss Per Batch: 0.6072317598735788 \t Current Batch Loss:  0.58854836\n",
      "Done Batch: 600 \tAverage Loss Per Batch: 0.6072705996512573 \t Current Batch Loss:  0.54771674\n",
      "Done Batch: 650 \tAverage Loss Per Batch: 0.6096036799492375 \t Current Batch Loss:  0.6009697\n",
      "Done Batch: 700 \tAverage Loss Per Batch: 0.607986491093792 \t Current Batch Loss:  0.49759734\n",
      "Done Batch: 750 \tAverage Loss Per Batch: 0.609662663801691 \t Current Batch Loss:  0.44464374\n",
      "Done Batch: 800 \tAverage Loss Per Batch: 0.6110589221473341 \t Current Batch Loss:  0.8830708\n",
      "Done Batch: 850 \tAverage Loss Per Batch: 0.6112625699766094 \t Current Batch Loss:  0.70727384\n",
      "Done Batch: 900 \tAverage Loss Per Batch: 0.6132833920956716 \t Current Batch Loss:  0.6204872\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done Batch: 950 \tAverage Loss Per Batch: 0.6146891708128335 \t Current Batch Loss:  0.5177767\n",
      "Done Batch: 1000 \tAverage Loss Per Batch: 0.6161071805508582 \t Current Batch Loss:  0.5421566\n",
      "Done Batch: 1050 \tAverage Loss Per Batch: 0.6159611290177202 \t Current Batch Loss:  0.7003556\n",
      "Done Batch: 1100 \tAverage Loss Per Batch: 0.6159541552495567 \t Current Batch Loss:  0.7600567\n",
      "Done Batch: 1150 \tAverage Loss Per Batch: 0.6152691550093252 \t Current Batch Loss:  0.43111792\n",
      "Done Batch: 1200 \tAverage Loss Per Batch: 0.6150907341338118 \t Current Batch Loss:  0.70951945\n",
      "Done Batch: 1250 \tAverage Loss Per Batch: 0.6133183945568917 \t Current Batch Loss:  0.7267097\n",
      "Done Batch: 1300 \tAverage Loss Per Batch: 0.6127263454406835 \t Current Batch Loss:  0.63945735\n",
      "Done Batch: 1350 \tAverage Loss Per Batch: 0.6115885574141756 \t Current Batch Loss:  0.3831817\n",
      "Done Batch: 1400 \tAverage Loss Per Batch: 0.609755412839635 \t Current Batch Loss:  0.52524686\n",
      "Done Batch: 1450 \tAverage Loss Per Batch: 0.6064975857940071 \t Current Batch Loss:  0.44491902\n",
      "Done Batch: 1500 \tAverage Loss Per Batch: 0.6046749361152255 \t Current Batch Loss:  0.5134981\n",
      "Done Batch: 1550 \tAverage Loss Per Batch: 0.6041169927475301 \t Current Batch Loss:  0.5274451\n",
      "Done Batch: 1600 \tAverage Loss Per Batch: 0.6043035793423578 \t Current Batch Loss:  0.46366423\n",
      "Done Batch: 1650 \tAverage Loss Per Batch: 0.603265732003298 \t Current Batch Loss:  0.38603616\n",
      "Done Batch: 1700 \tAverage Loss Per Batch: 0.6026453247216083 \t Current Batch Loss:  0.48747262\n",
      "Done Batch: 1750 \tAverage Loss Per Batch: 0.6012428721858732 \t Current Batch Loss:  0.5839185\n",
      "Done Batch: 1800 \tAverage Loss Per Batch: 0.5994956357025027 \t Current Batch Loss:  0.48446533\n",
      "Done Batch: 1850 \tAverage Loss Per Batch: 0.5984620847037391 \t Current Batch Loss:  0.60290796\n",
      "Done Batch: 1900 \tAverage Loss Per Batch: 0.5978181440538008 \t Current Batch Loss:  0.6398399\n",
      "Done Batch: 1950 \tAverage Loss Per Batch: 0.5962019797504284 \t Current Batch Loss:  0.6229152\n",
      "Done Batch: 2000 \tAverage Loss Per Batch: 0.594837200769718 \t Current Batch Loss:  0.49288014\n",
      "Done Batch: 2050 \tAverage Loss Per Batch: 0.594766912839286 \t Current Batch Loss:  0.5815479\n",
      "Done Batch: 2100 \tAverage Loss Per Batch: 0.5946427748096608 \t Current Batch Loss:  0.61017966\n",
      "Done Batch: 2150 \tAverage Loss Per Batch: 0.5935551779234703 \t Current Batch Loss:  0.5294311\n",
      "Done Batch: 2200 \tAverage Loss Per Batch: 0.593284850768315 \t Current Batch Loss:  0.9041735\n",
      "Done Batch: 2250 \tAverage Loss Per Batch: 0.5931224469236139 \t Current Batch Loss:  0.59676844\n",
      "Done Batch: 2300 \tAverage Loss Per Batch: 0.5926074704777826 \t Current Batch Loss:  0.68531746\n",
      "Done Batch: 2350 \tAverage Loss Per Batch: 0.592331449904882 \t Current Batch Loss:  0.8157242\n",
      "Done Batch: 2400 \tAverage Loss Per Batch: 0.5922755912933088 \t Current Batch Loss:  0.62620443\n",
      "Done Batch: 2450 \tAverage Loss Per Batch: 0.5920883365817481 \t Current Batch Loss:  0.7304494\n",
      "Done Batch: 2500 \tAverage Loss Per Batch: 0.5917146696967156 \t Current Batch Loss:  0.51574874\n",
      "Done Batch: 2550 \tAverage Loss Per Batch: 0.5908326883145006 \t Current Batch Loss:  0.5114725\n",
      "Done Batch: 2600 \tAverage Loss Per Batch: 0.5905102129534362 \t Current Batch Loss:  0.5155952\n",
      "Done Batch: 2650 \tAverage Loss Per Batch: 0.5899185611139555 \t Current Batch Loss:  0.46622932\n",
      "Done Batch: 2700 \tAverage Loss Per Batch: 0.5890772093979795 \t Current Batch Loss:  0.6157132\n",
      "Done Batch: 2750 \tAverage Loss Per Batch: 0.5882840762568231 \t Current Batch Loss:  0.65601563\n",
      "Done Batch: 2800 \tAverage Loss Per Batch: 0.5881228121167121 \t Current Batch Loss:  0.40374038\n",
      "Done Batch: 2850 \tAverage Loss Per Batch: 0.5881920681004773 \t Current Batch Loss:  0.53240895\n",
      "Done Batch: 2900 \tAverage Loss Per Batch: 0.588265957697964 \t Current Batch Loss:  0.48315522\n",
      "Done Batch: 2950 \tAverage Loss Per Batch: 0.5880828534534769 \t Current Batch Loss:  0.4956177\n",
      "Done Batch: 3000 \tAverage Loss Per Batch: 0.5876203773717649 \t Current Batch Loss:  0.74968207\n",
      "Done Batch: 3050 \tAverage Loss Per Batch: 0.5875171349404953 \t Current Batch Loss:  0.826056\n",
      "Done Batch: 3100 \tAverage Loss Per Batch: 0.587023750577039 \t Current Batch Loss:  0.51633215\n",
      "Done Batch: 3150 \tAverage Loss Per Batch: 0.5865593132841062 \t Current Batch Loss:  0.6181358\n",
      "Done Batch: 3200 \tAverage Loss Per Batch: 0.5866363930985243 \t Current Batch Loss:  0.55130744\n",
      "Done Batch: 3250 \tAverage Loss Per Batch: 0.5858558277713816 \t Current Batch Loss:  0.52163047\n",
      "Done Batch: 3300 \tAverage Loss Per Batch: 0.586139897209014 \t Current Batch Loss:  0.5972724\n",
      "Done Batch: 3350 \tAverage Loss Per Batch: 0.5864252697946563 \t Current Batch Loss:  0.6355944\n",
      "Epoch: 26 \tTime: 4387.972913265228 \tAverage Loss Per Batch:: 0.5868308972517531\n",
      "Done Batch: 0 \tAverage Loss Per Batch: 0.5060877203941345 \t Current Batch Loss:  0.5060877\n",
      "Done Batch: 50 \tAverage Loss Per Batch: 0.5806930404083401 \t Current Batch Loss:  0.5983343\n",
      "Done Batch: 100 \tAverage Loss Per Batch: 0.5750365398897983 \t Current Batch Loss:  0.757742\n",
      "Done Batch: 150 \tAverage Loss Per Batch: 0.5819563575533052 \t Current Batch Loss:  0.47892082\n",
      "Done Batch: 200 \tAverage Loss Per Batch: 0.5856411774360125 \t Current Batch Loss:  0.582523\n",
      "Done Batch: 250 \tAverage Loss Per Batch: 0.584799203027292 \t Current Batch Loss:  0.58681893\n",
      "Done Batch: 300 \tAverage Loss Per Batch: 0.582907191542692 \t Current Batch Loss:  0.3048028\n",
      "Done Batch: 350 \tAverage Loss Per Batch: 0.5854404669711393 \t Current Batch Loss:  0.70499367\n",
      "Done Batch: 400 \tAverage Loss Per Batch: 0.5850416963683103 \t Current Batch Loss:  0.6785306\n",
      "Done Batch: 450 \tAverage Loss Per Batch: 0.5827225750671522 \t Current Batch Loss:  0.6390432\n",
      "Done Batch: 500 \tAverage Loss Per Batch: 0.5807472911185609 \t Current Batch Loss:  0.54436463\n",
      "Done Batch: 550 \tAverage Loss Per Batch: 0.5788121040092405 \t Current Batch Loss:  0.36953315\n",
      "Done Batch: 600 \tAverage Loss Per Batch: 0.5810265966540763 \t Current Batch Loss:  0.5197786\n",
      "Done Batch: 650 \tAverage Loss Per Batch: 0.5808005091232089 \t Current Batch Loss:  0.48712265\n",
      "Done Batch: 700 \tAverage Loss Per Batch: 0.5811154309676139 \t Current Batch Loss:  0.48422843\n",
      "Done Batch: 750 \tAverage Loss Per Batch: 0.5804842436678718 \t Current Batch Loss:  0.81998396\n",
      "Done Batch: 800 \tAverage Loss Per Batch: 0.5796936610515943 \t Current Batch Loss:  0.4226793\n",
      "Done Batch: 850 \tAverage Loss Per Batch: 0.578925411478473 \t Current Batch Loss:  0.5394137\n",
      "Done Batch: 900 \tAverage Loss Per Batch: 0.5791866893773603 \t Current Batch Loss:  0.48439288\n",
      "Done Batch: 950 \tAverage Loss Per Batch: 0.5807822213187955 \t Current Batch Loss:  0.67529213\n",
      "Done Batch: 1000 \tAverage Loss Per Batch: 0.5812599669981908 \t Current Batch Loss:  0.68082654\n",
      "Done Batch: 1050 \tAverage Loss Per Batch: 0.5833744401028903 \t Current Batch Loss:  0.6011439\n",
      "Done Batch: 1100 \tAverage Loss Per Batch: 0.5822751955429496 \t Current Batch Loss:  0.5719646\n",
      "Done Batch: 1150 \tAverage Loss Per Batch: 0.5823424805204936 \t Current Batch Loss:  0.53577846\n",
      "Done Batch: 1200 \tAverage Loss Per Batch: 0.5833951218439876 \t Current Batch Loss:  0.7159367\n",
      "Done Batch: 1250 \tAverage Loss Per Batch: 0.5825131138642248 \t Current Batch Loss:  0.55799395\n",
      "Done Batch: 1300 \tAverage Loss Per Batch: 0.5804655565196601 \t Current Batch Loss:  0.58518714\n",
      "Done Batch: 1350 \tAverage Loss Per Batch: 0.5788763132164162 \t Current Batch Loss:  0.5521052\n",
      "Done Batch: 1400 \tAverage Loss Per Batch: 0.5765623629603702 \t Current Batch Loss:  0.7326658\n",
      "Done Batch: 1450 \tAverage Loss Per Batch: 0.5750342020488952 \t Current Batch Loss:  0.5459638\n",
      "Done Batch: 1500 \tAverage Loss Per Batch: 0.5737983597309727 \t Current Batch Loss:  0.6885811\n",
      "Done Batch: 1550 \tAverage Loss Per Batch: 0.572240043808613 \t Current Batch Loss:  0.60026145\n",
      "Done Batch: 1600 \tAverage Loss Per Batch: 0.5711522366500213 \t Current Batch Loss:  0.4970123\n",
      "Done Batch: 1650 \tAverage Loss Per Batch: 0.5693576127778105 \t Current Batch Loss:  0.57552046\n",
      "Done Batch: 1700 \tAverage Loss Per Batch: 0.5686907159237915 \t Current Batch Loss:  0.7233529\n",
      "Done Batch: 1750 \tAverage Loss Per Batch: 0.5674083492437272 \t Current Batch Loss:  0.6076634\n",
      "Done Batch: 1800 \tAverage Loss Per Batch: 0.5657922366074494 \t Current Batch Loss:  0.35851058\n",
      "Done Batch: 1850 \tAverage Loss Per Batch: 0.5646120965770617 \t Current Batch Loss:  0.657326\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done Batch: 1900 \tAverage Loss Per Batch: 0.5635945735198457 \t Current Batch Loss:  0.6269199\n",
      "Done Batch: 1950 \tAverage Loss Per Batch: 0.5619199669770496 \t Current Batch Loss:  0.5806394\n",
      "Done Batch: 2000 \tAverage Loss Per Batch: 0.561003585447972 \t Current Batch Loss:  0.6778494\n",
      "Done Batch: 2050 \tAverage Loss Per Batch: 0.5605191179800475 \t Current Batch Loss:  0.58636534\n",
      "Done Batch: 2100 \tAverage Loss Per Batch: 0.5598905325957221 \t Current Batch Loss:  0.47600436\n",
      "Done Batch: 2150 \tAverage Loss Per Batch: 0.5591361632129859 \t Current Batch Loss:  0.54659784\n",
      "Done Batch: 2200 \tAverage Loss Per Batch: 0.5584532222467247 \t Current Batch Loss:  0.45960245\n",
      "Done Batch: 2250 \tAverage Loss Per Batch: 0.5570721280860774 \t Current Batch Loss:  0.43258888\n",
      "Done Batch: 2300 \tAverage Loss Per Batch: 0.557014996994583 \t Current Batch Loss:  0.60391283\n",
      "Done Batch: 2350 \tAverage Loss Per Batch: 0.5564197346232181 \t Current Batch Loss:  0.6078278\n",
      "Done Batch: 2400 \tAverage Loss Per Batch: 0.5561604534944163 \t Current Batch Loss:  0.6580984\n",
      "Done Batch: 2450 \tAverage Loss Per Batch: 0.5558131051180266 \t Current Batch Loss:  0.4018813\n",
      "Done Batch: 2500 \tAverage Loss Per Batch: 0.5558861340584158 \t Current Batch Loss:  0.649215\n",
      "Done Batch: 2550 \tAverage Loss Per Batch: 0.5557856484283797 \t Current Batch Loss:  0.43456116\n",
      "Done Batch: 2600 \tAverage Loss Per Batch: 0.5555281380949824 \t Current Batch Loss:  0.5847001\n",
      "Done Batch: 2650 \tAverage Loss Per Batch: 0.5554513601099307 \t Current Batch Loss:  0.73137665\n",
      "Done Batch: 2700 \tAverage Loss Per Batch: 0.554807583983851 \t Current Batch Loss:  0.55168784\n",
      "Done Batch: 2750 \tAverage Loss Per Batch: 0.5545115186512926 \t Current Batch Loss:  0.59650666\n",
      "Done Batch: 2800 \tAverage Loss Per Batch: 0.5543863710038622 \t Current Batch Loss:  0.59468275\n",
      "Done Batch: 2850 \tAverage Loss Per Batch: 0.5542222156373294 \t Current Batch Loss:  0.5773283\n",
      "Done Batch: 2900 \tAverage Loss Per Batch: 0.5544891096998272 \t Current Batch Loss:  0.39565155\n",
      "Done Batch: 2950 \tAverage Loss Per Batch: 0.5547477862100527 \t Current Batch Loss:  0.87802905\n",
      "Done Batch: 3000 \tAverage Loss Per Batch: 0.5546302013558493 \t Current Batch Loss:  0.45668975\n",
      "Done Batch: 3050 \tAverage Loss Per Batch: 0.5545719222176938 \t Current Batch Loss:  0.50490034\n",
      "Done Batch: 3100 \tAverage Loss Per Batch: 0.5545898100823136 \t Current Batch Loss:  0.5832606\n",
      "Done Batch: 3150 \tAverage Loss Per Batch: 0.5544066380240054 \t Current Batch Loss:  0.7275299\n",
      "Done Batch: 3200 \tAverage Loss Per Batch: 0.5541350312696253 \t Current Batch Loss:  0.73557097\n",
      "Done Batch: 3250 \tAverage Loss Per Batch: 0.5541421443508134 \t Current Batch Loss:  0.6279434\n",
      "Done Batch: 3300 \tAverage Loss Per Batch: 0.5546249436252372 \t Current Batch Loss:  0.6580383\n",
      "Done Batch: 3350 \tAverage Loss Per Batch: 0.5550126236735867 \t Current Batch Loss:  0.45204303\n",
      "Epoch: 27 \tTime: 4384.022177696228 \tAverage Loss Per Batch:: 0.5550956003055892\n",
      "Done Batch: 0 \tAverage Loss Per Batch: 0.4206598699092865 \t Current Batch Loss:  0.42065987\n",
      "Done Batch: 50 \tAverage Loss Per Batch: 0.5357774607106751 \t Current Batch Loss:  0.4255142\n",
      "Done Batch: 100 \tAverage Loss Per Batch: 0.5547404723002178 \t Current Batch Loss:  0.48462388\n",
      "Done Batch: 150 \tAverage Loss Per Batch: 0.5449471183565279 \t Current Batch Loss:  0.53732836\n",
      "Done Batch: 200 \tAverage Loss Per Batch: 0.5480695912493995 \t Current Batch Loss:  0.60240006\n",
      "Done Batch: 250 \tAverage Loss Per Batch: 0.5518687978921184 \t Current Batch Loss:  0.5098233\n",
      "Done Batch: 300 \tAverage Loss Per Batch: 0.5546005440510785 \t Current Batch Loss:  0.35381135\n",
      "Done Batch: 350 \tAverage Loss Per Batch: 0.5552541202629394 \t Current Batch Loss:  0.43972352\n",
      "Done Batch: 400 \tAverage Loss Per Batch: 0.5571789134321664 \t Current Batch Loss:  0.35363236\n",
      "Done Batch: 450 \tAverage Loss Per Batch: 0.5573883903793115 \t Current Batch Loss:  0.56825006\n",
      "Done Batch: 500 \tAverage Loss Per Batch: 0.5508775568591382 \t Current Batch Loss:  0.41509575\n",
      "Done Batch: 550 \tAverage Loss Per Batch: 0.5492623452969342 \t Current Batch Loss:  0.30427718\n",
      "Done Batch: 600 \tAverage Loss Per Batch: 0.5473849616957187 \t Current Batch Loss:  0.55056024\n",
      "Done Batch: 650 \tAverage Loss Per Batch: 0.5464293090841188 \t Current Batch Loss:  0.6049345\n",
      "Done Batch: 700 \tAverage Loss Per Batch: 0.5442472471617769 \t Current Batch Loss:  0.47346494\n",
      "Done Batch: 750 \tAverage Loss Per Batch: 0.544385955332916 \t Current Batch Loss:  0.7072219\n",
      "Done Batch: 800 \tAverage Loss Per Batch: 0.5438476361325022 \t Current Batch Loss:  0.49628234\n",
      "Done Batch: 850 \tAverage Loss Per Batch: 0.5442908273005177 \t Current Batch Loss:  0.74532944\n",
      "Done Batch: 900 \tAverage Loss Per Batch: 0.5456857932493768 \t Current Batch Loss:  0.7491904\n",
      "Done Batch: 950 \tAverage Loss Per Batch: 0.5468248599115857 \t Current Batch Loss:  0.80219376\n",
      "Done Batch: 1000 \tAverage Loss Per Batch: 0.5468708334656267 \t Current Batch Loss:  0.38945433\n",
      "Done Batch: 1050 \tAverage Loss Per Batch: 0.5481253182258524 \t Current Batch Loss:  0.6262768\n",
      "Done Batch: 1100 \tAverage Loss Per Batch: 0.5462774842382019 \t Current Batch Loss:  0.48146898\n",
      "Done Batch: 1150 \tAverage Loss Per Batch: 0.5455728401757654 \t Current Batch Loss:  0.6638968\n",
      "Done Batch: 1200 \tAverage Loss Per Batch: 0.5450294625625126 \t Current Batch Loss:  0.5116003\n",
      "Done Batch: 1250 \tAverage Loss Per Batch: 0.5437232148852185 \t Current Batch Loss:  0.5370251\n",
      "Done Batch: 1300 \tAverage Loss Per Batch: 0.5425976291127061 \t Current Batch Loss:  0.54436594\n",
      "Done Batch: 1350 \tAverage Loss Per Batch: 0.5416897275353431 \t Current Batch Loss:  0.4995985\n",
      "Done Batch: 1400 \tAverage Loss Per Batch: 0.5405622993959519 \t Current Batch Loss:  0.47850987\n",
      "Done Batch: 1450 \tAverage Loss Per Batch: 0.538266580949809 \t Current Batch Loss:  0.4120951\n",
      "Done Batch: 1500 \tAverage Loss Per Batch: 0.537622527211289 \t Current Batch Loss:  0.563779\n",
      "Done Batch: 1550 \tAverage Loss Per Batch: 0.5361419487276207 \t Current Batch Loss:  0.4718751\n",
      "Done Batch: 1600 \tAverage Loss Per Batch: 0.5350418882880785 \t Current Batch Loss:  0.550153\n",
      "Done Batch: 1650 \tAverage Loss Per Batch: 0.5338723643449925 \t Current Batch Loss:  0.54224485\n",
      "Done Batch: 1700 \tAverage Loss Per Batch: 0.5326613227911797 \t Current Batch Loss:  0.6014673\n",
      "Done Batch: 1750 \tAverage Loss Per Batch: 0.5317302147433937 \t Current Batch Loss:  0.37688035\n",
      "Done Batch: 1800 \tAverage Loss Per Batch: 0.5304830144389612 \t Current Batch Loss:  0.5293946\n",
      "Done Batch: 1850 \tAverage Loss Per Batch: 0.5294437524130511 \t Current Batch Loss:  0.6549231\n",
      "Done Batch: 1900 \tAverage Loss Per Batch: 0.527973572175356 \t Current Batch Loss:  0.5283848\n",
      "Done Batch: 1950 \tAverage Loss Per Batch: 0.5267718846998968 \t Current Batch Loss:  0.46796173\n",
      "Done Batch: 2000 \tAverage Loss Per Batch: 0.5259200813903147 \t Current Batch Loss:  0.79246145\n",
      "Done Batch: 2050 \tAverage Loss Per Batch: 0.5250012664853626 \t Current Batch Loss:  0.49856362\n",
      "Done Batch: 2100 \tAverage Loss Per Batch: 0.5240242522864158 \t Current Batch Loss:  0.42063904\n",
      "Done Batch: 2150 \tAverage Loss Per Batch: 0.5230560017942772 \t Current Batch Loss:  0.59362227\n",
      "Done Batch: 2200 \tAverage Loss Per Batch: 0.522028512714506 \t Current Batch Loss:  0.54031473\n",
      "Done Batch: 2250 \tAverage Loss Per Batch: 0.5212126618713763 \t Current Batch Loss:  0.5965783\n",
      "Done Batch: 2300 \tAverage Loss Per Batch: 0.5212939164784617 \t Current Batch Loss:  0.24487202\n",
      "Done Batch: 2350 \tAverage Loss Per Batch: 0.5214164919900874 \t Current Batch Loss:  0.50474924\n",
      "Done Batch: 2400 \tAverage Loss Per Batch: 0.5217114615470159 \t Current Batch Loss:  0.5297177\n",
      "Done Batch: 2450 \tAverage Loss Per Batch: 0.5213155833277008 \t Current Batch Loss:  0.55307084\n",
      "Done Batch: 2500 \tAverage Loss Per Batch: 0.5207881928801013 \t Current Batch Loss:  0.4164892\n",
      "Done Batch: 2550 \tAverage Loss Per Batch: 0.5209727523164251 \t Current Batch Loss:  0.70369935\n",
      "Done Batch: 2600 \tAverage Loss Per Batch: 0.5207333234423447 \t Current Batch Loss:  0.5416551\n",
      "Done Batch: 2650 \tAverage Loss Per Batch: 0.5206802988650439 \t Current Batch Loss:  0.4829538\n",
      "Done Batch: 2700 \tAverage Loss Per Batch: 0.5211311853382156 \t Current Batch Loss:  0.44362637\n",
      "Done Batch: 2750 \tAverage Loss Per Batch: 0.5210800711153551 \t Current Batch Loss:  0.62796754\n",
      "Done Batch: 2800 \tAverage Loss Per Batch: 0.520637917863348 \t Current Batch Loss:  0.5734899\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done Batch: 2850 \tAverage Loss Per Batch: 0.5206514797328606 \t Current Batch Loss:  0.47957715\n",
      "Done Batch: 2900 \tAverage Loss Per Batch: 0.5207884356466831 \t Current Batch Loss:  0.42414328\n",
      "Done Batch: 2950 \tAverage Loss Per Batch: 0.5210679836977704 \t Current Batch Loss:  0.4031755\n",
      "Done Batch: 3000 \tAverage Loss Per Batch: 0.520724905198592 \t Current Batch Loss:  0.5067453\n",
      "Done Batch: 3050 \tAverage Loss Per Batch: 0.5205942671862012 \t Current Batch Loss:  0.56650895\n",
      "Done Batch: 3100 \tAverage Loss Per Batch: 0.5200997423939226 \t Current Batch Loss:  0.56596595\n",
      "Done Batch: 3150 \tAverage Loss Per Batch: 0.5196191278838688 \t Current Batch Loss:  0.45780757\n",
      "Done Batch: 3200 \tAverage Loss Per Batch: 0.5194234758271012 \t Current Batch Loss:  0.61404836\n",
      "Done Batch: 3250 \tAverage Loss Per Batch: 0.5190923281245435 \t Current Batch Loss:  0.29362115\n",
      "Done Batch: 3300 \tAverage Loss Per Batch: 0.5191620054731799 \t Current Batch Loss:  0.619699\n",
      "Done Batch: 3350 \tAverage Loss Per Batch: 0.5192428286735139 \t Current Batch Loss:  0.68763804\n",
      "Epoch: 28 \tTime: 4390.916161298752 \tAverage Loss Per Batch:: 0.5193300501180396\n",
      "Done Batch: 0 \tAverage Loss Per Batch: 0.3219485580921173 \t Current Batch Loss:  0.32194856\n",
      "Done Batch: 50 \tAverage Loss Per Batch: 0.519177738942352 \t Current Batch Loss:  0.73117286\n",
      "Done Batch: 100 \tAverage Loss Per Batch: 0.51052457035178 \t Current Batch Loss:  0.5739395\n",
      "Done Batch: 150 \tAverage Loss Per Batch: 0.5132937857646815 \t Current Batch Loss:  0.5099482\n",
      "Done Batch: 200 \tAverage Loss Per Batch: 0.5029692999759123 \t Current Batch Loss:  0.5611826\n",
      "Done Batch: 250 \tAverage Loss Per Batch: 0.5036028329119739 \t Current Batch Loss:  0.48056015\n",
      "Done Batch: 300 \tAverage Loss Per Batch: 0.5042571006621237 \t Current Batch Loss:  0.4175869\n",
      "Done Batch: 350 \tAverage Loss Per Batch: 0.5056998214320919 \t Current Batch Loss:  0.33282024\n",
      "Done Batch: 400 \tAverage Loss Per Batch: 0.5043510773086786 \t Current Batch Loss:  0.41275245\n",
      "Done Batch: 450 \tAverage Loss Per Batch: 0.503974803865881 \t Current Batch Loss:  0.66899854\n",
      "Done Batch: 500 \tAverage Loss Per Batch: 0.5063212370563172 \t Current Batch Loss:  0.7327548\n",
      "Done Batch: 550 \tAverage Loss Per Batch: 0.507026831299986 \t Current Batch Loss:  0.6482428\n",
      "Done Batch: 600 \tAverage Loss Per Batch: 0.5061043624622056 \t Current Batch Loss:  0.52951044\n",
      "Done Batch: 650 \tAverage Loss Per Batch: 0.5062854212870429 \t Current Batch Loss:  0.29899147\n",
      "Done Batch: 700 \tAverage Loss Per Batch: 0.5053970713205582 \t Current Batch Loss:  0.32715172\n",
      "Done Batch: 750 \tAverage Loss Per Batch: 0.5052147317186653 \t Current Batch Loss:  0.49298817\n",
      "Done Batch: 800 \tAverage Loss Per Batch: 0.5055376173702816 \t Current Batch Loss:  0.45644075\n",
      "Done Batch: 850 \tAverage Loss Per Batch: 0.5062333830445129 \t Current Batch Loss:  0.6992025\n",
      "Done Batch: 900 \tAverage Loss Per Batch: 0.5058053175564214 \t Current Batch Loss:  0.382438\n",
      "Done Batch: 950 \tAverage Loss Per Batch: 0.5079779490812845 \t Current Batch Loss:  0.4816382\n",
      "Done Batch: 1000 \tAverage Loss Per Batch: 0.5086956251036752 \t Current Batch Loss:  0.7084177\n",
      "Done Batch: 1050 \tAverage Loss Per Batch: 0.5079545954882134 \t Current Batch Loss:  0.41399965\n",
      "Done Batch: 1100 \tAverage Loss Per Batch: 0.5083721457125381 \t Current Batch Loss:  0.7521678\n",
      "Done Batch: 1150 \tAverage Loss Per Batch: 0.5080137362850945 \t Current Batch Loss:  0.45800328\n",
      "Done Batch: 1200 \tAverage Loss Per Batch: 0.5084396446128372 \t Current Batch Loss:  0.5360715\n",
      "Done Batch: 1250 \tAverage Loss Per Batch: 0.5075223534513149 \t Current Batch Loss:  0.41422677\n",
      "Done Batch: 1300 \tAverage Loss Per Batch: 0.5059516960432857 \t Current Batch Loss:  0.52511376\n",
      "Done Batch: 1350 \tAverage Loss Per Batch: 0.5049907246137177 \t Current Batch Loss:  0.45819986\n",
      "Done Batch: 1400 \tAverage Loss Per Batch: 0.5050041705561399 \t Current Batch Loss:  0.61516184\n",
      "Done Batch: 1450 \tAverage Loss Per Batch: 0.5041062048104284 \t Current Batch Loss:  0.31754532\n",
      "Done Batch: 1500 \tAverage Loss Per Batch: 0.503112267526605 \t Current Batch Loss:  0.33861843\n",
      "Done Batch: 1550 \tAverage Loss Per Batch: 0.5031135049771679 \t Current Batch Loss:  0.5574306\n",
      "Done Batch: 1600 \tAverage Loss Per Batch: 0.5024054307143886 \t Current Batch Loss:  0.60175854\n",
      "Done Batch: 1650 \tAverage Loss Per Batch: 0.50220454878189 \t Current Batch Loss:  0.6485054\n",
      "Done Batch: 1700 \tAverage Loss Per Batch: 0.5014178463460978 \t Current Batch Loss:  0.37023255\n",
      "Done Batch: 1750 \tAverage Loss Per Batch: 0.4998370283062563 \t Current Batch Loss:  0.61959356\n",
      "Done Batch: 1800 \tAverage Loss Per Batch: 0.49848332619713387 \t Current Batch Loss:  0.408881\n",
      "Done Batch: 1850 \tAverage Loss Per Batch: 0.49772376451603084 \t Current Batch Loss:  0.4444238\n",
      "Done Batch: 1900 \tAverage Loss Per Batch: 0.496416147118302 \t Current Batch Loss:  0.45754611\n",
      "Done Batch: 1950 \tAverage Loss Per Batch: 0.49582015499587184 \t Current Batch Loss:  0.47864997\n",
      "Done Batch: 2000 \tAverage Loss Per Batch: 0.49555928095884766 \t Current Batch Loss:  0.55520517\n",
      "Done Batch: 2050 \tAverage Loss Per Batch: 0.4948837282612869 \t Current Batch Loss:  0.45829904\n",
      "Done Batch: 2100 \tAverage Loss Per Batch: 0.49506095032615927 \t Current Batch Loss:  0.5516136\n",
      "Done Batch: 2150 \tAverage Loss Per Batch: 0.4944358498052019 \t Current Batch Loss:  0.44905022\n",
      "Done Batch: 2200 \tAverage Loss Per Batch: 0.49382201730798775 \t Current Batch Loss:  0.361777\n",
      "Done Batch: 2250 \tAverage Loss Per Batch: 0.4937151577911182 \t Current Batch Loss:  0.46628994\n",
      "Done Batch: 2300 \tAverage Loss Per Batch: 0.4932341228382114 \t Current Batch Loss:  0.37270927\n",
      "Done Batch: 2350 \tAverage Loss Per Batch: 0.4932005044781163 \t Current Batch Loss:  0.33050087\n",
      "Done Batch: 2400 \tAverage Loss Per Batch: 0.492627655737403 \t Current Batch Loss:  0.433181\n",
      "Done Batch: 2450 \tAverage Loss Per Batch: 0.4923141764675243 \t Current Batch Loss:  0.43914238\n",
      "Done Batch: 2500 \tAverage Loss Per Batch: 0.49264347734378844 \t Current Batch Loss:  0.4829133\n",
      "Done Batch: 2550 \tAverage Loss Per Batch: 0.4923559787319203 \t Current Batch Loss:  0.35079536\n",
      "Done Batch: 2600 \tAverage Loss Per Batch: 0.4921690763211718 \t Current Batch Loss:  0.5665255\n",
      "Done Batch: 2650 \tAverage Loss Per Batch: 0.4923756378904823 \t Current Batch Loss:  0.57941693\n",
      "Done Batch: 2700 \tAverage Loss Per Batch: 0.49272221864174576 \t Current Batch Loss:  0.5655427\n",
      "Done Batch: 2750 \tAverage Loss Per Batch: 0.4927451185142721 \t Current Batch Loss:  0.4012207\n",
      "Done Batch: 2800 \tAverage Loss Per Batch: 0.49232948594585313 \t Current Batch Loss:  0.41191414\n",
      "Done Batch: 2850 \tAverage Loss Per Batch: 0.4924534693783687 \t Current Batch Loss:  0.5562956\n",
      "Done Batch: 2900 \tAverage Loss Per Batch: 0.49216591061620374 \t Current Batch Loss:  0.5369396\n",
      "Done Batch: 2950 \tAverage Loss Per Batch: 0.49280837151972734 \t Current Batch Loss:  0.5343206\n",
      "Done Batch: 3000 \tAverage Loss Per Batch: 0.4929257836869382 \t Current Batch Loss:  0.56688774\n",
      "Done Batch: 3050 \tAverage Loss Per Batch: 0.49279593334202687 \t Current Batch Loss:  0.45415413\n",
      "Done Batch: 3100 \tAverage Loss Per Batch: 0.49270638320877336 \t Current Batch Loss:  0.5370491\n",
      "Done Batch: 3150 \tAverage Loss Per Batch: 0.4923021229098086 \t Current Batch Loss:  0.55987215\n",
      "Done Batch: 3200 \tAverage Loss Per Batch: 0.4924936208807651 \t Current Batch Loss:  0.6997628\n",
      "Done Batch: 3250 \tAverage Loss Per Batch: 0.49227418305689796 \t Current Batch Loss:  0.5244843\n",
      "Done Batch: 3300 \tAverage Loss Per Batch: 0.4921162495899114 \t Current Batch Loss:  0.6151592\n",
      "Done Batch: 3350 \tAverage Loss Per Batch: 0.49191963050088683 \t Current Batch Loss:  0.45150155\n",
      "Epoch: 29 \tTime: 4394.834571123123 \tAverage Loss Per Batch:: 0.4919820012060584\n"
     ]
    }
   ],
   "source": [
    "def train(model, learning_rate=0.0001, batch_size=100, epochs=10):\n",
    "    \n",
    "    # defining criterion and optimizer\n",
    "    criterion = nn.NLLLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr = learning_rate)\n",
    "    \n",
    "    number_of_batches = math.ceil(len(train_image_addresses)/batch_size)\n",
    "    indexes = np.arange(len(train_image_addresses))\n",
    "    \n",
    "    loss_arr = []\n",
    "    for epoch in range(25, 25 + epochs):\n",
    "        avgloss = 0.0\n",
    "        start = time.time()\n",
    "        avg_loss_arr = []\n",
    "        for batch in range(number_of_batches):\n",
    "            train_indexes = [train_image_addresses[i] for i in indexes[batch*batch_size:(batch+1)*batch_size]]\n",
    "            image_batch, target_batch = load_batch(train_indexes)\n",
    "            loss = train_pass(image_batch, target_batch, model, optimizer, criterion)\n",
    "            avgloss += loss\n",
    "            if batch%50 == 0:\n",
    "                print (\"Done Batch:\", batch, \"\\tAverage Loss Per Batch:\", avgloss/(batch+1), \"\\t Current Batch Loss: \", loss)\n",
    "        loss_arr.append(avgloss/(batch+1))\n",
    "        print (\"Epoch:\",epoch, \"\\tTime:\", time.time() - start, \"\\tAverage Loss Per Batch::\", avgloss/(batch+1))\n",
    "        torch.save({'epoch': epoch ,'state_dict': model.state_dict(),'optimizer': optimizer.state_dict()}, open(\"outputs/anp_log_softmax_classifier_batch_\"+str(epoch), \"wb+\"))\n",
    "    loss_arr = np.array(loss_arr)\n",
    "    np.save(open('outputs/loss_log_softmax_anp_classifier', 'wb+'), loss_arr)\n",
    "    \n",
    "train(main_model, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done Batch: 0 \tAverage Loss Per Batch: 0.5198383927345276 \t Current Batch Loss:  0.5198384\n",
      "Done Batch: 50 \tAverage Loss Per Batch: 0.526612202910816 \t Current Batch Loss:  0.54812574\n",
      "Done Batch: 100 \tAverage Loss Per Batch: 0.5035295580873395 \t Current Batch Loss:  0.43842843\n",
      "Done Batch: 150 \tAverage Loss Per Batch: 0.49843262422163753 \t Current Batch Loss:  0.5084052\n",
      "Done Batch: 200 \tAverage Loss Per Batch: 0.5024441539944701 \t Current Batch Loss:  0.54613\n",
      "Done Batch: 250 \tAverage Loss Per Batch: 0.5055267090104015 \t Current Batch Loss:  0.51641464\n",
      "Done Batch: 300 \tAverage Loss Per Batch: 0.5057828857554154 \t Current Batch Loss:  0.545794\n",
      "Done Batch: 350 \tAverage Loss Per Batch: 0.5024530037261142 \t Current Batch Loss:  0.39338884\n",
      "Done Batch: 400 \tAverage Loss Per Batch: 0.5017426200118148 \t Current Batch Loss:  0.58302546\n",
      "Done Batch: 450 \tAverage Loss Per Batch: 0.49963780276130415 \t Current Batch Loss:  0.45925218\n",
      "Done Batch: 500 \tAverage Loss Per Batch: 0.5003302305222985 \t Current Batch Loss:  0.6089696\n",
      "Done Batch: 550 \tAverage Loss Per Batch: 0.5022048746014247 \t Current Batch Loss:  0.456669\n",
      "Done Batch: 600 \tAverage Loss Per Batch: 0.5013038745388612 \t Current Batch Loss:  0.34412605\n",
      "Done Batch: 650 \tAverage Loss Per Batch: 0.49829451455956414 \t Current Batch Loss:  0.294675\n",
      "Done Batch: 700 \tAverage Loss Per Batch: 0.49749645720449903 \t Current Batch Loss:  0.5728744\n",
      "Done Batch: 750 \tAverage Loss Per Batch: 0.4977447386389566 \t Current Batch Loss:  0.57284796\n",
      "Done Batch: 800 \tAverage Loss Per Batch: 0.49774289456720505 \t Current Batch Loss:  0.5721868\n",
      "Done Batch: 850 \tAverage Loss Per Batch: 0.49689672134387647 \t Current Batch Loss:  0.51022846\n",
      "Done Batch: 900 \tAverage Loss Per Batch: 0.496593515381432 \t Current Batch Loss:  0.6479404\n",
      "Done Batch: 950 \tAverage Loss Per Batch: 0.4975510663445941 \t Current Batch Loss:  0.5326701\n",
      "Done Batch: 1000 \tAverage Loss Per Batch: 0.49746593786822213 \t Current Batch Loss:  0.4069368\n",
      "Done Batch: 1050 \tAverage Loss Per Batch: 0.4966709274518615 \t Current Batch Loss:  0.4593805\n",
      "Done Batch: 1100 \tAverage Loss Per Batch: 0.49512719968629904 \t Current Batch Loss:  0.39249116\n",
      "Done Batch: 1150 \tAverage Loss Per Batch: 0.4943967882728287 \t Current Batch Loss:  0.6606021\n",
      "Done Batch: 1200 \tAverage Loss Per Batch: 0.4935451817080738 \t Current Batch Loss:  0.46961915\n",
      "Done Batch: 1250 \tAverage Loss Per Batch: 0.49241628714030883 \t Current Batch Loss:  0.7076639\n",
      "Done Batch: 1300 \tAverage Loss Per Batch: 0.49079518882610723 \t Current Batch Loss:  0.44020027\n",
      "Done Batch: 1350 \tAverage Loss Per Batch: 0.4897793324720762 \t Current Batch Loss:  0.38436595\n",
      "Done Batch: 1400 \tAverage Loss Per Batch: 0.4882635212769941 \t Current Batch Loss:  0.5078383\n",
      "Done Batch: 1450 \tAverage Loss Per Batch: 0.4871774165281996 \t Current Batch Loss:  0.38472185\n",
      "Done Batch: 1500 \tAverage Loss Per Batch: 0.4868258835374316 \t Current Batch Loss:  0.54687464\n",
      "Done Batch: 1550 \tAverage Loss Per Batch: 0.486385446366074 \t Current Batch Loss:  0.49936748\n",
      "Done Batch: 1600 \tAverage Loss Per Batch: 0.48615781555207055 \t Current Batch Loss:  0.4981056\n",
      "Done Batch: 1650 \tAverage Loss Per Batch: 0.4871299576300841 \t Current Batch Loss:  0.42129326\n",
      "Done Batch: 1700 \tAverage Loss Per Batch: 0.4862031701433175 \t Current Batch Loss:  0.41306192\n",
      "Done Batch: 1750 \tAverage Loss Per Batch: 0.4847573350746927 \t Current Batch Loss:  0.43182418\n",
      "Done Batch: 1800 \tAverage Loss Per Batch: 0.48423611420283774 \t Current Batch Loss:  0.47775525\n",
      "Done Batch: 1850 \tAverage Loss Per Batch: 0.4843155059909125 \t Current Batch Loss:  0.43359977\n",
      "Done Batch: 1900 \tAverage Loss Per Batch: 0.4844397697039243 \t Current Batch Loss:  0.45399627\n",
      "Done Batch: 1950 \tAverage Loss Per Batch: 0.4837823797729552 \t Current Batch Loss:  0.36706915\n",
      "Done Batch: 2000 \tAverage Loss Per Batch: 0.48294277286630816 \t Current Batch Loss:  0.6019759\n",
      "Done Batch: 2050 \tAverage Loss Per Batch: 0.4820432365667175 \t Current Batch Loss:  0.31306216\n",
      "Done Batch: 2100 \tAverage Loss Per Batch: 0.4821209517165288 \t Current Batch Loss:  0.55869704\n",
      "Done Batch: 2150 \tAverage Loss Per Batch: 0.4815369028371193 \t Current Batch Loss:  0.47521806\n",
      "Done Batch: 2200 \tAverage Loss Per Batch: 0.4819411460186665 \t Current Batch Loss:  0.34382966\n",
      "Done Batch: 2250 \tAverage Loss Per Batch: 0.4821638030536648 \t Current Batch Loss:  0.6010917\n",
      "Done Batch: 2300 \tAverage Loss Per Batch: 0.48214147006465474 \t Current Batch Loss:  0.35858783\n",
      "Done Batch: 2350 \tAverage Loss Per Batch: 0.48232864907628575 \t Current Batch Loss:  0.36333892\n",
      "Done Batch: 2400 \tAverage Loss Per Batch: 0.4813424952759538 \t Current Batch Loss:  0.4578353\n",
      "Done Batch: 2450 \tAverage Loss Per Batch: 0.480771793784389 \t Current Batch Loss:  0.28721893\n",
      "Done Batch: 2500 \tAverage Loss Per Batch: 0.4801374605980838 \t Current Batch Loss:  0.50849605\n",
      "Done Batch: 2550 \tAverage Loss Per Batch: 0.4796007088250807 \t Current Batch Loss:  0.33828166\n",
      "Done Batch: 2600 \tAverage Loss Per Batch: 0.47940176558673314 \t Current Batch Loss:  0.5686072\n",
      "Done Batch: 2650 \tAverage Loss Per Batch: 0.47893228193289017 \t Current Batch Loss:  0.44287694\n",
      "Done Batch: 2700 \tAverage Loss Per Batch: 0.47905014075755364 \t Current Batch Loss:  0.45691335\n",
      "Done Batch: 2750 \tAverage Loss Per Batch: 0.47824210707966264 \t Current Batch Loss:  0.56657547\n",
      "Done Batch: 2800 \tAverage Loss Per Batch: 0.4776436078925083 \t Current Batch Loss:  0.56530297\n",
      "Done Batch: 2850 \tAverage Loss Per Batch: 0.4774737575260308 \t Current Batch Loss:  0.3653709\n",
      "Done Batch: 2900 \tAverage Loss Per Batch: 0.4770746953690393 \t Current Batch Loss:  0.53780496\n",
      "Done Batch: 2950 \tAverage Loss Per Batch: 0.47714675440298665 \t Current Batch Loss:  0.42232373\n",
      "Done Batch: 3000 \tAverage Loss Per Batch: 0.4765641308504119 \t Current Batch Loss:  0.37653366\n",
      "Done Batch: 3050 \tAverage Loss Per Batch: 0.4763019278288982 \t Current Batch Loss:  0.50942904\n",
      "Done Batch: 3100 \tAverage Loss Per Batch: 0.4764525334572108 \t Current Batch Loss:  0.3554287\n",
      "Done Batch: 3150 \tAverage Loss Per Batch: 0.4764736675811321 \t Current Batch Loss:  0.6497406\n",
      "Done Batch: 3200 \tAverage Loss Per Batch: 0.476861738676356 \t Current Batch Loss:  0.40180698\n",
      "Done Batch: 3250 \tAverage Loss Per Batch: 0.4766073631840609 \t Current Batch Loss:  0.4865509\n",
      "Done Batch: 3300 \tAverage Loss Per Batch: 0.4769521727774296 \t Current Batch Loss:  0.53373224\n",
      "Done Batch: 3350 \tAverage Loss Per Batch: 0.47745526985103287 \t Current Batch Loss:  0.5087612\n",
      "Epoch: 30 \tTime: 4399.774293661118 \tAverage Loss Per Batch:: 0.47765029262046554\n",
      "Done Batch: 0 \tAverage Loss Per Batch: 0.21687310934066772 \t Current Batch Loss:  0.21687311\n",
      "Done Batch: 50 \tAverage Loss Per Batch: 0.45046891184414134 \t Current Batch Loss:  0.41005817\n",
      "Done Batch: 100 \tAverage Loss Per Batch: 0.46751389320534054 \t Current Batch Loss:  0.43094453\n",
      "Done Batch: 150 \tAverage Loss Per Batch: 0.47336677229957075 \t Current Batch Loss:  0.59701777\n",
      "Done Batch: 200 \tAverage Loss Per Batch: 0.4701459255088028 \t Current Batch Loss:  0.48479858\n",
      "Done Batch: 250 \tAverage Loss Per Batch: 0.46984095592422787 \t Current Batch Loss:  0.40198502\n",
      "Done Batch: 300 \tAverage Loss Per Batch: 0.46658138009994926 \t Current Batch Loss:  0.5250846\n",
      "Done Batch: 350 \tAverage Loss Per Batch: 0.46576314948053443 \t Current Batch Loss:  0.52650374\n",
      "Done Batch: 400 \tAverage Loss Per Batch: 0.4680953339076696 \t Current Batch Loss:  0.4683838\n",
      "Done Batch: 450 \tAverage Loss Per Batch: 0.47115923563419054 \t Current Batch Loss:  0.48375252\n",
      "Done Batch: 500 \tAverage Loss Per Batch: 0.47117302669379524 \t Current Batch Loss:  0.54010946\n",
      "Done Batch: 550 \tAverage Loss Per Batch: 0.46932557114347573 \t Current Batch Loss:  0.5420192\n",
      "Done Batch: 600 \tAverage Loss Per Batch: 0.4703534493678421 \t Current Batch Loss:  0.5912313\n",
      "Done Batch: 650 \tAverage Loss Per Batch: 0.46973527368221046 \t Current Batch Loss:  0.55572367\n",
      "Done Batch: 700 \tAverage Loss Per Batch: 0.47038609450536856 \t Current Batch Loss:  0.5737338\n",
      "Done Batch: 750 \tAverage Loss Per Batch: 0.468532134884207 \t Current Batch Loss:  0.47150216\n",
      "Done Batch: 800 \tAverage Loss Per Batch: 0.47015358831328846 \t Current Batch Loss:  0.5118444\n",
      "Done Batch: 850 \tAverage Loss Per Batch: 0.4703344392265192 \t Current Batch Loss:  0.33936805\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done Batch: 900 \tAverage Loss Per Batch: 0.46922912659510124 \t Current Batch Loss:  0.39601752\n",
      "Done Batch: 950 \tAverage Loss Per Batch: 0.4714678284779708 \t Current Batch Loss:  0.6237751\n",
      "Done Batch: 1000 \tAverage Loss Per Batch: 0.4723278832334381 \t Current Batch Loss:  0.39840344\n",
      "Done Batch: 1050 \tAverage Loss Per Batch: 0.4717315890384106 \t Current Batch Loss:  0.3441699\n",
      "Done Batch: 1100 \tAverage Loss Per Batch: 0.47166131937590433 \t Current Batch Loss:  0.4231961\n",
      "Done Batch: 1150 \tAverage Loss Per Batch: 0.47161186951901163 \t Current Batch Loss:  0.31337675\n",
      "Done Batch: 1200 \tAverage Loss Per Batch: 0.47118722358958903 \t Current Batch Loss:  0.3591612\n",
      "Done Batch: 1250 \tAverage Loss Per Batch: 0.4698692706587027 \t Current Batch Loss:  0.4292105\n",
      "Done Batch: 1300 \tAverage Loss Per Batch: 0.4677000044631005 \t Current Batch Loss:  0.46978402\n",
      "Done Batch: 1350 \tAverage Loss Per Batch: 0.46605817540145467 \t Current Batch Loss:  0.49637672\n",
      "Done Batch: 1400 \tAverage Loss Per Batch: 0.4646237609983767 \t Current Batch Loss:  0.3322014\n",
      "Done Batch: 1450 \tAverage Loss Per Batch: 0.4631914040349747 \t Current Batch Loss:  0.4945491\n",
      "Done Batch: 1500 \tAverage Loss Per Batch: 0.46126198382436395 \t Current Batch Loss:  0.41260907\n",
      "Done Batch: 1550 \tAverage Loss Per Batch: 0.4607385443169251 \t Current Batch Loss:  0.40356147\n",
      "Done Batch: 1600 \tAverage Loss Per Batch: 0.4609324633609794 \t Current Batch Loss:  0.26834977\n",
      "Done Batch: 1650 \tAverage Loss Per Batch: 0.4603538136186923 \t Current Batch Loss:  0.5722766\n",
      "Done Batch: 1700 \tAverage Loss Per Batch: 0.46036541524142255 \t Current Batch Loss:  0.5679236\n",
      "Done Batch: 1750 \tAverage Loss Per Batch: 0.4591934847310909 \t Current Batch Loss:  0.3218853\n",
      "Done Batch: 1800 \tAverage Loss Per Batch: 0.45879874689952327 \t Current Batch Loss:  0.57992285\n",
      "Done Batch: 1850 \tAverage Loss Per Batch: 0.4579024495127264 \t Current Batch Loss:  0.42214835\n",
      "Done Batch: 1900 \tAverage Loss Per Batch: 0.45758579290427387 \t Current Batch Loss:  0.38715854\n",
      "Done Batch: 1950 \tAverage Loss Per Batch: 0.45735756457157467 \t Current Batch Loss:  0.54237837\n",
      "Done Batch: 2000 \tAverage Loss Per Batch: 0.4570519355000525 \t Current Batch Loss:  0.38229606\n",
      "Done Batch: 2050 \tAverage Loss Per Batch: 0.4569162760258651 \t Current Batch Loss:  0.59589756\n",
      "Done Batch: 2100 \tAverage Loss Per Batch: 0.4564832297536318 \t Current Batch Loss:  0.43949226\n",
      "Done Batch: 2150 \tAverage Loss Per Batch: 0.4558678570942566 \t Current Batch Loss:  0.5431885\n",
      "Done Batch: 2200 \tAverage Loss Per Batch: 0.4558135220837669 \t Current Batch Loss:  0.5947169\n",
      "Done Batch: 2250 \tAverage Loss Per Batch: 0.4550010405053885 \t Current Batch Loss:  0.4195052\n",
      "Done Batch: 2300 \tAverage Loss Per Batch: 0.4544289267643386 \t Current Batch Loss:  0.69341326\n",
      "Done Batch: 2350 \tAverage Loss Per Batch: 0.4544562826583559 \t Current Batch Loss:  0.55174017\n",
      "Done Batch: 2400 \tAverage Loss Per Batch: 0.454475844014391 \t Current Batch Loss:  0.5329443\n",
      "Done Batch: 2450 \tAverage Loss Per Batch: 0.4544176960942308 \t Current Batch Loss:  0.5040203\n",
      "Done Batch: 2500 \tAverage Loss Per Batch: 0.45397195650762867 \t Current Batch Loss:  0.4614694\n",
      "Done Batch: 2550 \tAverage Loss Per Batch: 0.45381080815134117 \t Current Batch Loss:  0.393555\n",
      "Done Batch: 2600 \tAverage Loss Per Batch: 0.45376608337131386 \t Current Batch Loss:  0.6847825\n",
      "Done Batch: 2650 \tAverage Loss Per Batch: 0.4535166901694024 \t Current Batch Loss:  0.4675573\n",
      "Done Batch: 2700 \tAverage Loss Per Batch: 0.4538247829254183 \t Current Batch Loss:  0.42540124\n",
      "Done Batch: 2750 \tAverage Loss Per Batch: 0.45332303407061364 \t Current Batch Loss:  0.38319755\n",
      "Done Batch: 2800 \tAverage Loss Per Batch: 0.45324713303463154 \t Current Batch Loss:  0.19346505\n",
      "Done Batch: 2850 \tAverage Loss Per Batch: 0.4529954631969996 \t Current Batch Loss:  0.41239372\n",
      "Done Batch: 2900 \tAverage Loss Per Batch: 0.4526660306541643 \t Current Batch Loss:  0.35360238\n",
      "Done Batch: 2950 \tAverage Loss Per Batch: 0.4522314158860646 \t Current Batch Loss:  0.4718314\n",
      "Done Batch: 3000 \tAverage Loss Per Batch: 0.4518187296257461 \t Current Batch Loss:  0.43845725\n",
      "Done Batch: 3050 \tAverage Loss Per Batch: 0.4516849663708101 \t Current Batch Loss:  0.5004245\n",
      "Done Batch: 3100 \tAverage Loss Per Batch: 0.45162039851996255 \t Current Batch Loss:  0.23648496\n",
      "Done Batch: 3150 \tAverage Loss Per Batch: 0.45140411143679726 \t Current Batch Loss:  0.375076\n",
      "Done Batch: 3200 \tAverage Loss Per Batch: 0.45164994206960335 \t Current Batch Loss:  0.42109096\n",
      "Done Batch: 3250 \tAverage Loss Per Batch: 0.4516086678107457 \t Current Batch Loss:  0.5336982\n",
      "Done Batch: 3300 \tAverage Loss Per Batch: 0.451386124924868 \t Current Batch Loss:  0.3803444\n",
      "Done Batch: 3350 \tAverage Loss Per Batch: 0.4513934122310116 \t Current Batch Loss:  0.34791335\n",
      "Epoch: 31 \tTime: 4384.398597717285 \tAverage Loss Per Batch:: 0.4519985578412078\n",
      "Done Batch: 0 \tAverage Loss Per Batch: 0.43191367387771606 \t Current Batch Loss:  0.43191367\n",
      "Done Batch: 50 \tAverage Loss Per Batch: 0.4235960648340337 \t Current Batch Loss:  0.5107087\n",
      "Done Batch: 100 \tAverage Loss Per Batch: 0.4397464949305695 \t Current Batch Loss:  0.4009807\n",
      "Done Batch: 150 \tAverage Loss Per Batch: 0.4458315374440705 \t Current Batch Loss:  0.40512237\n",
      "Done Batch: 200 \tAverage Loss Per Batch: 0.4539271548612794 \t Current Batch Loss:  0.80602187\n",
      "Done Batch: 250 \tAverage Loss Per Batch: 0.4533963191556741 \t Current Batch Loss:  0.4679773\n",
      "Done Batch: 300 \tAverage Loss Per Batch: 0.45172657067593547 \t Current Batch Loss:  0.3770765\n",
      "Done Batch: 350 \tAverage Loss Per Batch: 0.44992662070483563 \t Current Batch Loss:  0.5167263\n",
      "Done Batch: 400 \tAverage Loss Per Batch: 0.45008710257133044 \t Current Batch Loss:  0.43644783\n",
      "Done Batch: 450 \tAverage Loss Per Batch: 0.4479565060720211 \t Current Batch Loss:  0.3299431\n",
      "Done Batch: 500 \tAverage Loss Per Batch: 0.4467242347028203 \t Current Batch Loss:  0.48934478\n",
      "Done Batch: 550 \tAverage Loss Per Batch: 0.44508389877104715 \t Current Batch Loss:  0.5590519\n",
      "Done Batch: 600 \tAverage Loss Per Batch: 0.44463972457236733 \t Current Batch Loss:  0.2723883\n",
      "Done Batch: 650 \tAverage Loss Per Batch: 0.4450051600482607 \t Current Batch Loss:  0.30697134\n",
      "Done Batch: 700 \tAverage Loss Per Batch: 0.44459582236030815 \t Current Batch Loss:  0.29982314\n",
      "Done Batch: 750 \tAverage Loss Per Batch: 0.44499083450646915 \t Current Batch Loss:  0.49105304\n",
      "Done Batch: 800 \tAverage Loss Per Batch: 0.4456009357497933 \t Current Batch Loss:  0.5969394\n",
      "Done Batch: 850 \tAverage Loss Per Batch: 0.4456807184093287 \t Current Batch Loss:  0.36892384\n",
      "Done Batch: 900 \tAverage Loss Per Batch: 0.4451923738176894 \t Current Batch Loss:  0.4468908\n",
      "Done Batch: 950 \tAverage Loss Per Batch: 0.4446479246318027 \t Current Batch Loss:  0.40477794\n",
      "Done Batch: 1000 \tAverage Loss Per Batch: 0.44406890115895115 \t Current Batch Loss:  0.38412452\n",
      "Done Batch: 1050 \tAverage Loss Per Batch: 0.4437925716619283 \t Current Batch Loss:  0.44739005\n",
      "Done Batch: 1100 \tAverage Loss Per Batch: 0.4433850635191617 \t Current Batch Loss:  0.39397058\n",
      "Done Batch: 1150 \tAverage Loss Per Batch: 0.4429475972695935 \t Current Batch Loss:  0.40687516\n",
      "Done Batch: 1200 \tAverage Loss Per Batch: 0.44268170332878853 \t Current Batch Loss:  0.5133566\n",
      "Done Batch: 1250 \tAverage Loss Per Batch: 0.44228193194841403 \t Current Batch Loss:  0.38281727\n",
      "Done Batch: 1300 \tAverage Loss Per Batch: 0.44300474223038677 \t Current Batch Loss:  0.39327285\n",
      "Done Batch: 1350 \tAverage Loss Per Batch: 0.4425894430639301 \t Current Batch Loss:  0.33962\n",
      "Done Batch: 1400 \tAverage Loss Per Batch: 0.4423425653783191 \t Current Batch Loss:  0.61856973\n",
      "Done Batch: 1450 \tAverage Loss Per Batch: 0.44165548129709403 \t Current Batch Loss:  0.36273757\n",
      "Done Batch: 1500 \tAverage Loss Per Batch: 0.4414417883660458 \t Current Batch Loss:  0.3829081\n",
      "Done Batch: 1550 \tAverage Loss Per Batch: 0.4408788458905936 \t Current Batch Loss:  0.2523768\n",
      "Done Batch: 1600 \tAverage Loss Per Batch: 0.4402846625490087 \t Current Batch Loss:  0.4581007\n",
      "Done Batch: 1650 \tAverage Loss Per Batch: 0.43943951077204196 \t Current Batch Loss:  0.41737235\n",
      "Done Batch: 1700 \tAverage Loss Per Batch: 0.437570458874851 \t Current Batch Loss:  0.30470935\n",
      "Done Batch: 1750 \tAverage Loss Per Batch: 0.43634064968724307 \t Current Batch Loss:  0.4865751\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done Batch: 1800 \tAverage Loss Per Batch: 0.43559465984679935 \t Current Batch Loss:  0.39905846\n",
      "Done Batch: 1850 \tAverage Loss Per Batch: 0.4351434658776097 \t Current Batch Loss:  0.41442186\n",
      "Done Batch: 1900 \tAverage Loss Per Batch: 0.43427019115186627 \t Current Batch Loss:  0.3767636\n",
      "Done Batch: 1950 \tAverage Loss Per Batch: 0.4331440161904086 \t Current Batch Loss:  0.575536\n",
      "Done Batch: 2000 \tAverage Loss Per Batch: 0.43204596414529106 \t Current Batch Loss:  0.3622721\n",
      "Done Batch: 2050 \tAverage Loss Per Batch: 0.4312306720852445 \t Current Batch Loss:  0.5753864\n",
      "Done Batch: 2100 \tAverage Loss Per Batch: 0.43083450933656825 \t Current Batch Loss:  0.4273488\n",
      "Done Batch: 2150 \tAverage Loss Per Batch: 0.4298635093085658 \t Current Batch Loss:  0.32486463\n",
      "Done Batch: 2200 \tAverage Loss Per Batch: 0.42930202249747307 \t Current Batch Loss:  0.26101732\n",
      "Done Batch: 2250 \tAverage Loss Per Batch: 0.429327445062517 \t Current Batch Loss:  0.27738515\n",
      "Done Batch: 2300 \tAverage Loss Per Batch: 0.4290219725395793 \t Current Batch Loss:  0.26971447\n",
      "Done Batch: 2350 \tAverage Loss Per Batch: 0.4293106491707174 \t Current Batch Loss:  0.49115136\n",
      "Done Batch: 2400 \tAverage Loss Per Batch: 0.42990458913392593 \t Current Batch Loss:  0.36220917\n",
      "Done Batch: 2450 \tAverage Loss Per Batch: 0.4298247919611326 \t Current Batch Loss:  0.46110696\n",
      "Done Batch: 2500 \tAverage Loss Per Batch: 0.43016959123256826 \t Current Batch Loss:  0.683246\n",
      "Done Batch: 2550 \tAverage Loss Per Batch: 0.43016758250377074 \t Current Batch Loss:  0.39923537\n",
      "Done Batch: 2600 \tAverage Loss Per Batch: 0.4294526739699801 \t Current Batch Loss:  0.35628182\n",
      "Done Batch: 2650 \tAverage Loss Per Batch: 0.4298009440859324 \t Current Batch Loss:  0.34968606\n",
      "Done Batch: 2700 \tAverage Loss Per Batch: 0.42970610596170605 \t Current Batch Loss:  0.38361427\n",
      "Done Batch: 2750 \tAverage Loss Per Batch: 0.4298041841845996 \t Current Batch Loss:  0.32779276\n",
      "Done Batch: 2800 \tAverage Loss Per Batch: 0.42950176335113127 \t Current Batch Loss:  0.52173495\n",
      "Done Batch: 2850 \tAverage Loss Per Batch: 0.42996376083320337 \t Current Batch Loss:  0.5977502\n",
      "Done Batch: 2900 \tAverage Loss Per Batch: 0.4302137144955221 \t Current Batch Loss:  0.5416936\n",
      "Done Batch: 2950 \tAverage Loss Per Batch: 0.4302474637782521 \t Current Batch Loss:  0.27631953\n",
      "Done Batch: 3000 \tAverage Loss Per Batch: 0.43007856454006316 \t Current Batch Loss:  0.41180778\n",
      "Done Batch: 3050 \tAverage Loss Per Batch: 0.42988906867169663 \t Current Batch Loss:  0.45938942\n",
      "Done Batch: 3100 \tAverage Loss Per Batch: 0.42973523834219907 \t Current Batch Loss:  0.4454423\n",
      "Done Batch: 3150 \tAverage Loss Per Batch: 0.4295601507087845 \t Current Batch Loss:  0.44267875\n",
      "Done Batch: 3200 \tAverage Loss Per Batch: 0.42945746342415586 \t Current Batch Loss:  0.4146141\n",
      "Done Batch: 3250 \tAverage Loss Per Batch: 0.42973670864409574 \t Current Batch Loss:  0.47834516\n",
      "Done Batch: 3300 \tAverage Loss Per Batch: 0.4297018129919125 \t Current Batch Loss:  0.4555639\n",
      "Done Batch: 3350 \tAverage Loss Per Batch: 0.43027785539146823 \t Current Batch Loss:  0.33561066\n",
      "Epoch: 32 \tTime: 4393.010811328888 \tAverage Loss Per Batch:: 0.4304182907090118\n",
      "Done Batch: 0 \tAverage Loss Per Batch: 0.3470068871974945 \t Current Batch Loss:  0.3470069\n",
      "Done Batch: 50 \tAverage Loss Per Batch: 0.4051170369573668 \t Current Batch Loss:  0.4118783\n",
      "Done Batch: 100 \tAverage Loss Per Batch: 0.42838122127669875 \t Current Batch Loss:  0.440369\n",
      "Done Batch: 150 \tAverage Loss Per Batch: 0.422752962404529 \t Current Batch Loss:  0.38535514\n",
      "Done Batch: 200 \tAverage Loss Per Batch: 0.42907141853327774 \t Current Batch Loss:  0.56357545\n",
      "Done Batch: 250 \tAverage Loss Per Batch: 0.42773661480481884 \t Current Batch Loss:  0.44073442\n",
      "Done Batch: 300 \tAverage Loss Per Batch: 0.4315480709669994 \t Current Batch Loss:  0.4706985\n",
      "Done Batch: 350 \tAverage Loss Per Batch: 0.43267636251585434 \t Current Batch Loss:  0.48980767\n",
      "Done Batch: 400 \tAverage Loss Per Batch: 0.4339310768759459 \t Current Batch Loss:  0.40717325\n",
      "Done Batch: 450 \tAverage Loss Per Batch: 0.4322283062464382 \t Current Batch Loss:  0.3369883\n",
      "Done Batch: 500 \tAverage Loss Per Batch: 0.43165318022469085 \t Current Batch Loss:  0.62403864\n",
      "Done Batch: 550 \tAverage Loss Per Batch: 0.42857122956732874 \t Current Batch Loss:  0.27536148\n",
      "Done Batch: 600 \tAverage Loss Per Batch: 0.4261390042681861 \t Current Batch Loss:  0.35233214\n",
      "Done Batch: 650 \tAverage Loss Per Batch: 0.42449068508115234 \t Current Batch Loss:  0.45419586\n",
      "Done Batch: 700 \tAverage Loss Per Batch: 0.42550128360362605 \t Current Batch Loss:  0.24303785\n",
      "Done Batch: 750 \tAverage Loss Per Batch: 0.42644690528769946 \t Current Batch Loss:  0.47538814\n",
      "Done Batch: 800 \tAverage Loss Per Batch: 0.4271122324890858 \t Current Batch Loss:  0.3988169\n",
      "Done Batch: 850 \tAverage Loss Per Batch: 0.4287340942371886 \t Current Batch Loss:  0.2885338\n",
      "Done Batch: 900 \tAverage Loss Per Batch: 0.4283582626549438 \t Current Batch Loss:  0.36417344\n",
      "Done Batch: 950 \tAverage Loss Per Batch: 0.42938285481603866 \t Current Batch Loss:  0.34179595\n",
      "Done Batch: 1000 \tAverage Loss Per Batch: 0.4298846626228148 \t Current Batch Loss:  0.3992988\n",
      "Done Batch: 1050 \tAverage Loss Per Batch: 0.4299068225496276 \t Current Batch Loss:  0.5548521\n",
      "Done Batch: 1100 \tAverage Loss Per Batch: 0.4306862138001947 \t Current Batch Loss:  0.48843324\n",
      "Done Batch: 1150 \tAverage Loss Per Batch: 0.4307783140269287 \t Current Batch Loss:  0.42474878\n",
      "Done Batch: 1200 \tAverage Loss Per Batch: 0.42956398711613475 \t Current Batch Loss:  0.36247268\n",
      "Done Batch: 1250 \tAverage Loss Per Batch: 0.42836128191934597 \t Current Batch Loss:  0.40623927\n",
      "Done Batch: 1300 \tAverage Loss Per Batch: 0.4274252274923376 \t Current Batch Loss:  0.37845314\n",
      "Done Batch: 1350 \tAverage Loss Per Batch: 0.4275120995293539 \t Current Batch Loss:  0.32850906\n",
      "Done Batch: 1400 \tAverage Loss Per Batch: 0.42687269387970134 \t Current Batch Loss:  0.5461857\n",
      "Done Batch: 1450 \tAverage Loss Per Batch: 0.4277547878450069 \t Current Batch Loss:  0.333395\n",
      "Done Batch: 1500 \tAverage Loss Per Batch: 0.4272823163880895 \t Current Batch Loss:  0.40565532\n",
      "Done Batch: 1550 \tAverage Loss Per Batch: 0.42597438010725186 \t Current Batch Loss:  0.44978347\n",
      "Done Batch: 1600 \tAverage Loss Per Batch: 0.4253524528731412 \t Current Batch Loss:  0.27901846\n",
      "Done Batch: 1650 \tAverage Loss Per Batch: 0.4240055850857608 \t Current Batch Loss:  0.47138867\n",
      "Done Batch: 1700 \tAverage Loss Per Batch: 0.4232025034265754 \t Current Batch Loss:  0.45171028\n",
      "Done Batch: 1750 \tAverage Loss Per Batch: 0.42306843770190283 \t Current Batch Loss:  0.26977235\n",
      "Done Batch: 1800 \tAverage Loss Per Batch: 0.4223504905350535 \t Current Batch Loss:  0.38658103\n",
      "Done Batch: 1850 \tAverage Loss Per Batch: 0.4221120847762436 \t Current Batch Loss:  0.408271\n",
      "Done Batch: 1900 \tAverage Loss Per Batch: 0.4210143598332523 \t Current Batch Loss:  0.4392687\n",
      "Done Batch: 1950 \tAverage Loss Per Batch: 0.4200402184259213 \t Current Batch Loss:  0.5555542\n",
      "Done Batch: 2000 \tAverage Loss Per Batch: 0.41917454448716157 \t Current Batch Loss:  0.37426892\n",
      "Done Batch: 2050 \tAverage Loss Per Batch: 0.41823378006229744 \t Current Batch Loss:  0.4046481\n",
      "Done Batch: 2100 \tAverage Loss Per Batch: 0.4180803962871042 \t Current Batch Loss:  0.4251915\n",
      "Done Batch: 2150 \tAverage Loss Per Batch: 0.417538301549031 \t Current Batch Loss:  0.46451437\n",
      "Done Batch: 2200 \tAverage Loss Per Batch: 0.4174737786363873 \t Current Batch Loss:  0.27874827\n",
      "Done Batch: 2250 \tAverage Loss Per Batch: 0.4169542039370124 \t Current Batch Loss:  0.46480256\n",
      "Done Batch: 2300 \tAverage Loss Per Batch: 0.41614819994469093 \t Current Batch Loss:  0.46732017\n",
      "Done Batch: 2350 \tAverage Loss Per Batch: 0.4159547958169679 \t Current Batch Loss:  0.21632767\n",
      "Done Batch: 2400 \tAverage Loss Per Batch: 0.41605557863139353 \t Current Batch Loss:  0.4380781\n",
      "Done Batch: 2450 \tAverage Loss Per Batch: 0.41570128251196947 \t Current Batch Loss:  0.34836638\n",
      "Done Batch: 2500 \tAverage Loss Per Batch: 0.41527727902865036 \t Current Batch Loss:  0.3485914\n",
      "Done Batch: 2550 \tAverage Loss Per Batch: 0.4152552666721602 \t Current Batch Loss:  0.45532548\n",
      "Done Batch: 2600 \tAverage Loss Per Batch: 0.41443164900420765 \t Current Batch Loss:  0.657657\n",
      "Done Batch: 2650 \tAverage Loss Per Batch: 0.4141865590919597 \t Current Batch Loss:  0.40671661\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done Batch: 2700 \tAverage Loss Per Batch: 0.41411625725478873 \t Current Batch Loss:  0.5449706\n",
      "Done Batch: 2750 \tAverage Loss Per Batch: 0.41387438129529397 \t Current Batch Loss:  0.5142951\n",
      "Done Batch: 2800 \tAverage Loss Per Batch: 0.4138747092146228 \t Current Batch Loss:  0.58128846\n",
      "Done Batch: 2850 \tAverage Loss Per Batch: 0.41406528553453426 \t Current Batch Loss:  0.38831466\n",
      "Done Batch: 2900 \tAverage Loss Per Batch: 0.4135703724782987 \t Current Batch Loss:  0.24317579\n",
      "Done Batch: 2950 \tAverage Loss Per Batch: 0.41356353637466026 \t Current Batch Loss:  0.3210947\n",
      "Done Batch: 3000 \tAverage Loss Per Batch: 0.41325378837048393 \t Current Batch Loss:  0.4255554\n",
      "Done Batch: 3050 \tAverage Loss Per Batch: 0.41366721265767686 \t Current Batch Loss:  0.44268668\n",
      "Done Batch: 3100 \tAverage Loss Per Batch: 0.413837693744573 \t Current Batch Loss:  0.29494992\n",
      "Done Batch: 3150 \tAverage Loss Per Batch: 0.4140370016824779 \t Current Batch Loss:  0.4698454\n",
      "Done Batch: 3200 \tAverage Loss Per Batch: 0.4144293900133483 \t Current Batch Loss:  0.3176366\n",
      "Done Batch: 3250 \tAverage Loss Per Batch: 0.4139675915937943 \t Current Batch Loss:  0.4090389\n",
      "Done Batch: 3300 \tAverage Loss Per Batch: 0.41407173078409004 \t Current Batch Loss:  0.5087136\n",
      "Done Batch: 3350 \tAverage Loss Per Batch: 0.4145703475641087 \t Current Batch Loss:  0.5318211\n",
      "Epoch: 33 \tTime: 4391.075647592545 \tAverage Loss Per Batch:: 0.4147474699702644\n",
      "Done Batch: 0 \tAverage Loss Per Batch: 0.2156389206647873 \t Current Batch Loss:  0.21563892\n",
      "Done Batch: 50 \tAverage Loss Per Batch: 0.3949814324869829 \t Current Batch Loss:  0.27782097\n",
      "Done Batch: 100 \tAverage Loss Per Batch: 0.3925614330438104 \t Current Batch Loss:  0.47152004\n",
      "Done Batch: 150 \tAverage Loss Per Batch: 0.3891363961017684 \t Current Batch Loss:  0.38781852\n",
      "Done Batch: 200 \tAverage Loss Per Batch: 0.398253487710336 \t Current Batch Loss:  0.46916062\n",
      "Done Batch: 250 \tAverage Loss Per Batch: 0.4036235111168181 \t Current Batch Loss:  0.49714133\n",
      "Done Batch: 300 \tAverage Loss Per Batch: 0.4095528553490623 \t Current Batch Loss:  0.443708\n",
      "Done Batch: 350 \tAverage Loss Per Batch: 0.41265805451958265 \t Current Batch Loss:  0.4792524\n",
      "Done Batch: 400 \tAverage Loss Per Batch: 0.41709736361170646 \t Current Batch Loss:  0.41606244\n",
      "Done Batch: 450 \tAverage Loss Per Batch: 0.41725082619491544 \t Current Batch Loss:  0.382182\n",
      "Done Batch: 500 \tAverage Loss Per Batch: 0.41762192785264013 \t Current Batch Loss:  0.4058268\n",
      "Done Batch: 550 \tAverage Loss Per Batch: 0.4164936670698834 \t Current Batch Loss:  0.39484382\n",
      "Done Batch: 600 \tAverage Loss Per Batch: 0.4161359442648991 \t Current Batch Loss:  0.74298996\n",
      "Done Batch: 650 \tAverage Loss Per Batch: 0.41661062845802893 \t Current Batch Loss:  0.46949607\n",
      "Done Batch: 700 \tAverage Loss Per Batch: 0.4149727258252009 \t Current Batch Loss:  0.42745262\n",
      "Done Batch: 750 \tAverage Loss Per Batch: 0.4129078697943973 \t Current Batch Loss:  0.29769614\n",
      "Done Batch: 800 \tAverage Loss Per Batch: 0.4113131709834014 \t Current Batch Loss:  0.31740257\n",
      "Done Batch: 850 \tAverage Loss Per Batch: 0.4104549045358225 \t Current Batch Loss:  0.53731996\n",
      "Done Batch: 900 \tAverage Loss Per Batch: 0.4108448840603315 \t Current Batch Loss:  0.40737426\n",
      "Done Batch: 950 \tAverage Loss Per Batch: 0.411477348672228 \t Current Batch Loss:  0.36925927\n",
      "Done Batch: 1000 \tAverage Loss Per Batch: 0.41081709714678977 \t Current Batch Loss:  0.48158005\n",
      "Done Batch: 1050 \tAverage Loss Per Batch: 0.4105244283586542 \t Current Batch Loss:  0.40839708\n",
      "Done Batch: 1100 \tAverage Loss Per Batch: 0.4101045872410463 \t Current Batch Loss:  0.2827261\n",
      "Done Batch: 1150 \tAverage Loss Per Batch: 0.408267730596167 \t Current Batch Loss:  0.45869428\n",
      "Done Batch: 1200 \tAverage Loss Per Batch: 0.4081285473557932 \t Current Batch Loss:  0.3076151\n",
      "Done Batch: 1250 \tAverage Loss Per Batch: 0.4074341778417857 \t Current Batch Loss:  0.6327491\n",
      "Done Batch: 1300 \tAverage Loss Per Batch: 0.4069684652939473 \t Current Batch Loss:  0.44985938\n",
      "Done Batch: 1350 \tAverage Loss Per Batch: 0.40569523208499575 \t Current Batch Loss:  0.418545\n",
      "Done Batch: 1400 \tAverage Loss Per Batch: 0.40520859147547655 \t Current Batch Loss:  0.36167884\n",
      "Done Batch: 1450 \tAverage Loss Per Batch: 0.404776873022092 \t Current Batch Loss:  0.40824735\n",
      "Done Batch: 1500 \tAverage Loss Per Batch: 0.40367439394192567 \t Current Batch Loss:  0.37487465\n",
      "Done Batch: 1550 \tAverage Loss Per Batch: 0.4036929598072127 \t Current Batch Loss:  0.36376685\n",
      "Done Batch: 1600 \tAverage Loss Per Batch: 0.40376344074650455 \t Current Batch Loss:  0.58774894\n",
      "Done Batch: 1650 \tAverage Loss Per Batch: 0.4036921001542055 \t Current Batch Loss:  0.35638317\n",
      "Done Batch: 1700 \tAverage Loss Per Batch: 0.4034933449677479 \t Current Batch Loss:  0.40606365\n",
      "Done Batch: 1750 \tAverage Loss Per Batch: 0.402873423178151 \t Current Batch Loss:  0.40859994\n",
      "Done Batch: 1800 \tAverage Loss Per Batch: 0.4014575094199723 \t Current Batch Loss:  0.28792533\n",
      "Done Batch: 1850 \tAverage Loss Per Batch: 0.40041748617047684 \t Current Batch Loss:  0.47271636\n",
      "Done Batch: 1900 \tAverage Loss Per Batch: 0.3994379340936045 \t Current Batch Loss:  0.28330937\n",
      "Done Batch: 1950 \tAverage Loss Per Batch: 0.39891551720262003 \t Current Batch Loss:  0.25399774\n",
      "Done Batch: 2000 \tAverage Loss Per Batch: 0.39818007864620253 \t Current Batch Loss:  0.34102875\n",
      "Done Batch: 2050 \tAverage Loss Per Batch: 0.3975982512390538 \t Current Batch Loss:  0.39253435\n",
      "Done Batch: 2100 \tAverage Loss Per Batch: 0.3973602058382786 \t Current Batch Loss:  0.3528167\n",
      "Done Batch: 2150 \tAverage Loss Per Batch: 0.3971431676016082 \t Current Batch Loss:  0.34541267\n",
      "Done Batch: 2200 \tAverage Loss Per Batch: 0.39691860632279524 \t Current Batch Loss:  0.43149474\n",
      "Done Batch: 2250 \tAverage Loss Per Batch: 0.396697250172092 \t Current Batch Loss:  0.45303267\n",
      "Done Batch: 2300 \tAverage Loss Per Batch: 0.39686277672253395 \t Current Batch Loss:  0.45955208\n",
      "Done Batch: 2350 \tAverage Loss Per Batch: 0.39677207333224723 \t Current Batch Loss:  0.4344348\n",
      "Done Batch: 2400 \tAverage Loss Per Batch: 0.3965991228130002 \t Current Batch Loss:  0.31304088\n",
      "Done Batch: 2450 \tAverage Loss Per Batch: 0.3971368193304184 \t Current Batch Loss:  0.37981567\n",
      "Done Batch: 2500 \tAverage Loss Per Batch: 0.397620934392752 \t Current Batch Loss:  0.44974616\n",
      "Done Batch: 2550 \tAverage Loss Per Batch: 0.39749090265744064 \t Current Batch Loss:  0.3097265\n",
      "Done Batch: 2600 \tAverage Loss Per Batch: 0.3975999777401149 \t Current Batch Loss:  0.5484343\n",
      "Done Batch: 2650 \tAverage Loss Per Batch: 0.3973833995943967 \t Current Batch Loss:  0.26209632\n",
      "Done Batch: 2700 \tAverage Loss Per Batch: 0.3977289568165283 \t Current Batch Loss:  0.834496\n",
      "Done Batch: 2750 \tAverage Loss Per Batch: 0.3977656194764846 \t Current Batch Loss:  0.42675364\n",
      "Done Batch: 2800 \tAverage Loss Per Batch: 0.3970868048291043 \t Current Batch Loss:  0.3366028\n",
      "Done Batch: 2850 \tAverage Loss Per Batch: 0.3968077229407117 \t Current Batch Loss:  0.39841703\n",
      "Done Batch: 2900 \tAverage Loss Per Batch: 0.3965378594923455 \t Current Batch Loss:  0.34069687\n",
      "Done Batch: 2950 \tAverage Loss Per Batch: 0.39614411090004853 \t Current Batch Loss:  0.3499154\n",
      "Done Batch: 3000 \tAverage Loss Per Batch: 0.3960982348219985 \t Current Batch Loss:  0.2723605\n",
      "Done Batch: 3050 \tAverage Loss Per Batch: 0.39617738016108456 \t Current Batch Loss:  0.37352622\n",
      "Done Batch: 3100 \tAverage Loss Per Batch: 0.39603811541619666 \t Current Batch Loss:  0.47390166\n",
      "Done Batch: 3150 \tAverage Loss Per Batch: 0.3959254174029513 \t Current Batch Loss:  0.40274513\n",
      "Done Batch: 3200 \tAverage Loss Per Batch: 0.3960016434968579 \t Current Batch Loss:  0.43270674\n",
      "Done Batch: 3250 \tAverage Loss Per Batch: 0.39657160113340745 \t Current Batch Loss:  0.38344872\n",
      "Done Batch: 3300 \tAverage Loss Per Batch: 0.39688164255982705 \t Current Batch Loss:  0.52626187\n",
      "Done Batch: 3350 \tAverage Loss Per Batch: 0.3975795954310229 \t Current Batch Loss:  0.31001607\n",
      "Epoch: 34 \tTime: 4384.088863134384 \tAverage Loss Per Batch:: 0.3976191633812727\n",
      "Done Batch: 0 \tAverage Loss Per Batch: 0.2986350953578949 \t Current Batch Loss:  0.2986351\n",
      "Done Batch: 50 \tAverage Loss Per Batch: 0.4071012302940967 \t Current Batch Loss:  0.5018204\n",
      "Done Batch: 100 \tAverage Loss Per Batch: 0.4033038160293409 \t Current Batch Loss:  0.44435692\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done Batch: 150 \tAverage Loss Per Batch: 0.4048157980702571 \t Current Batch Loss:  0.45821527\n",
      "Done Batch: 200 \tAverage Loss Per Batch: 0.40502812301934654 \t Current Batch Loss:  0.41389844\n",
      "Done Batch: 250 \tAverage Loss Per Batch: 0.4034014428516783 \t Current Batch Loss:  0.37397254\n",
      "Done Batch: 300 \tAverage Loss Per Batch: 0.40163991617601974 \t Current Batch Loss:  0.45173806\n",
      "Done Batch: 350 \tAverage Loss Per Batch: 0.40197763976208506 \t Current Batch Loss:  0.2758668\n",
      "Done Batch: 400 \tAverage Loss Per Batch: 0.40513103516619103 \t Current Batch Loss:  0.4588617\n",
      "Done Batch: 450 \tAverage Loss Per Batch: 0.40125944153168247 \t Current Batch Loss:  0.32935306\n",
      "Done Batch: 500 \tAverage Loss Per Batch: 0.40233262802312475 \t Current Batch Loss:  0.26594642\n",
      "Done Batch: 550 \tAverage Loss Per Batch: 0.40337697407078615 \t Current Batch Loss:  0.5642208\n",
      "Done Batch: 600 \tAverage Loss Per Batch: 0.40282271442913176 \t Current Batch Loss:  0.35610265\n",
      "Done Batch: 650 \tAverage Loss Per Batch: 0.40233518922567 \t Current Batch Loss:  0.51924545\n",
      "Done Batch: 700 \tAverage Loss Per Batch: 0.40013197999707983 \t Current Batch Loss:  0.35037255\n",
      "Done Batch: 750 \tAverage Loss Per Batch: 0.39855017629111017 \t Current Batch Loss:  0.3897657\n",
      "Done Batch: 800 \tAverage Loss Per Batch: 0.39926839531509767 \t Current Batch Loss:  0.309617\n",
      "Done Batch: 850 \tAverage Loss Per Batch: 0.398745187598726 \t Current Batch Loss:  0.38313374\n",
      "Done Batch: 900 \tAverage Loss Per Batch: 0.3984261107663071 \t Current Batch Loss:  0.2123725\n",
      "Done Batch: 950 \tAverage Loss Per Batch: 0.39906118550072456 \t Current Batch Loss:  0.51917154\n",
      "Done Batch: 1000 \tAverage Loss Per Batch: 0.3989321858762623 \t Current Batch Loss:  0.42933792\n",
      "Done Batch: 1050 \tAverage Loss Per Batch: 0.39844944069319743 \t Current Batch Loss:  0.37763306\n",
      "Done Batch: 1100 \tAverage Loss Per Batch: 0.3979355148517685 \t Current Batch Loss:  0.5017724\n",
      "Done Batch: 1150 \tAverage Loss Per Batch: 0.3985023694541742 \t Current Batch Loss:  0.34269965\n",
      "Done Batch: 1200 \tAverage Loss Per Batch: 0.39846581942731396 \t Current Batch Loss:  0.26612493\n",
      "Done Batch: 1250 \tAverage Loss Per Batch: 0.3978520499216281 \t Current Batch Loss:  0.21194746\n",
      "Done Batch: 1300 \tAverage Loss Per Batch: 0.39692824999934245 \t Current Batch Loss:  0.45945477\n",
      "Done Batch: 1350 \tAverage Loss Per Batch: 0.3966049326736781 \t Current Batch Loss:  0.40072984\n",
      "Done Batch: 1400 \tAverage Loss Per Batch: 0.39524137081417504 \t Current Batch Loss:  0.24305046\n",
      "Done Batch: 1450 \tAverage Loss Per Batch: 0.39413961458009 \t Current Batch Loss:  0.3395548\n",
      "Done Batch: 1500 \tAverage Loss Per Batch: 0.39301635189941136 \t Current Batch Loss:  0.2723384\n",
      "Done Batch: 1550 \tAverage Loss Per Batch: 0.39182828902852374 \t Current Batch Loss:  0.34690693\n",
      "Done Batch: 1600 \tAverage Loss Per Batch: 0.3906102635054794 \t Current Batch Loss:  0.47918496\n",
      "Done Batch: 1650 \tAverage Loss Per Batch: 0.3901592882509162 \t Current Batch Loss:  0.35686684\n",
      "Done Batch: 1700 \tAverage Loss Per Batch: 0.3896393314976331 \t Current Batch Loss:  0.46350777\n",
      "Done Batch: 1750 \tAverage Loss Per Batch: 0.38823406890179346 \t Current Batch Loss:  0.31925145\n",
      "Done Batch: 1800 \tAverage Loss Per Batch: 0.3881776454232257 \t Current Batch Loss:  0.56130195\n",
      "Done Batch: 1850 \tAverage Loss Per Batch: 0.3880589170545452 \t Current Batch Loss:  0.41540727\n",
      "Done Batch: 1900 \tAverage Loss Per Batch: 0.3879462576489396 \t Current Batch Loss:  0.3273554\n",
      "Done Batch: 1950 \tAverage Loss Per Batch: 0.3874325068836515 \t Current Batch Loss:  0.4887809\n",
      "Done Batch: 2000 \tAverage Loss Per Batch: 0.38681944586645656 \t Current Batch Loss:  0.33985442\n",
      "Done Batch: 2050 \tAverage Loss Per Batch: 0.3866289379624609 \t Current Batch Loss:  0.48444816\n",
      "Done Batch: 2100 \tAverage Loss Per Batch: 0.3868728728733535 \t Current Batch Loss:  0.48811847\n",
      "Done Batch: 2150 \tAverage Loss Per Batch: 0.38654603706244145 \t Current Batch Loss:  0.37134337\n",
      "Done Batch: 2200 \tAverage Loss Per Batch: 0.3859647791096429 \t Current Batch Loss:  0.39167556\n",
      "Done Batch: 2250 \tAverage Loss Per Batch: 0.3854602086496374 \t Current Batch Loss:  0.4117388\n",
      "Done Batch: 2300 \tAverage Loss Per Batch: 0.38501655422750114 \t Current Batch Loss:  0.42572263\n",
      "Done Batch: 2350 \tAverage Loss Per Batch: 0.38473061300728584 \t Current Batch Loss:  0.34158418\n",
      "Done Batch: 2400 \tAverage Loss Per Batch: 0.3843876946188419 \t Current Batch Loss:  0.38863206\n",
      "Done Batch: 2450 \tAverage Loss Per Batch: 0.38408622715313645 \t Current Batch Loss:  0.52987206\n",
      "Done Batch: 2500 \tAverage Loss Per Batch: 0.38365563556033583 \t Current Batch Loss:  0.36516562\n",
      "Done Batch: 2550 \tAverage Loss Per Batch: 0.3838671441465581 \t Current Batch Loss:  0.4379405\n",
      "Done Batch: 2600 \tAverage Loss Per Batch: 0.383461481639854 \t Current Batch Loss:  0.33522052\n",
      "Done Batch: 2650 \tAverage Loss Per Batch: 0.3837311161616666 \t Current Batch Loss:  0.32895732\n",
      "Done Batch: 2700 \tAverage Loss Per Batch: 0.38401888386490873 \t Current Batch Loss:  0.3357543\n",
      "Done Batch: 2750 \tAverage Loss Per Batch: 0.3837052467345151 \t Current Batch Loss:  0.22595291\n",
      "Done Batch: 2800 \tAverage Loss Per Batch: 0.38365132713564715 \t Current Batch Loss:  0.4590233\n",
      "Done Batch: 2850 \tAverage Loss Per Batch: 0.3836686681369362 \t Current Batch Loss:  0.3612975\n",
      "Done Batch: 2900 \tAverage Loss Per Batch: 0.383506991629229 \t Current Batch Loss:  0.46310088\n",
      "Done Batch: 2950 \tAverage Loss Per Batch: 0.38353826094990623 \t Current Batch Loss:  0.34481156\n",
      "Done Batch: 3000 \tAverage Loss Per Batch: 0.38341939011362147 \t Current Batch Loss:  0.24199739\n",
      "Done Batch: 3050 \tAverage Loss Per Batch: 0.3831714545555171 \t Current Batch Loss:  0.29873788\n",
      "Done Batch: 3100 \tAverage Loss Per Batch: 0.3832641397703467 \t Current Batch Loss:  0.26231635\n",
      "Done Batch: 3150 \tAverage Loss Per Batch: 0.3831021486294152 \t Current Batch Loss:  0.4579532\n",
      "Done Batch: 3200 \tAverage Loss Per Batch: 0.38329892251052844 \t Current Batch Loss:  0.50663465\n",
      "Done Batch: 3250 \tAverage Loss Per Batch: 0.3835536801000992 \t Current Batch Loss:  0.41390565\n",
      "Done Batch: 3300 \tAverage Loss Per Batch: 0.3839132087370079 \t Current Batch Loss:  0.53586614\n",
      "Done Batch: 3350 \tAverage Loss Per Batch: 0.3838395609166579 \t Current Batch Loss:  0.37966576\n",
      "Epoch: 35 \tTime: 4381.298206090927 \tAverage Loss Per Batch:: 0.38383568299783244\n",
      "Done Batch: 0 \tAverage Loss Per Batch: 0.24213942885398865 \t Current Batch Loss:  0.24213943\n",
      "Done Batch: 50 \tAverage Loss Per Batch: 0.38390540141685336 \t Current Batch Loss:  0.4457749\n",
      "Done Batch: 100 \tAverage Loss Per Batch: 0.3887190264050323 \t Current Batch Loss:  0.47471264\n",
      "Done Batch: 150 \tAverage Loss Per Batch: 0.39624493574069825 \t Current Batch Loss:  0.283455\n",
      "Done Batch: 200 \tAverage Loss Per Batch: 0.3909949221421237 \t Current Batch Loss:  0.514991\n",
      "Done Batch: 250 \tAverage Loss Per Batch: 0.3928732017715614 \t Current Batch Loss:  0.44316003\n",
      "Done Batch: 300 \tAverage Loss Per Batch: 0.39263151358330367 \t Current Batch Loss:  0.37295723\n",
      "Done Batch: 350 \tAverage Loss Per Batch: 0.3939491768083681 \t Current Batch Loss:  0.3109899\n",
      "Done Batch: 400 \tAverage Loss Per Batch: 0.3911592799826453 \t Current Batch Loss:  0.4717744\n",
      "Done Batch: 450 \tAverage Loss Per Batch: 0.3889106788947154 \t Current Batch Loss:  0.54141885\n",
      "Done Batch: 500 \tAverage Loss Per Batch: 0.38942209210343465 \t Current Batch Loss:  0.36699829\n",
      "Done Batch: 550 \tAverage Loss Per Batch: 0.38802086690376547 \t Current Batch Loss:  0.34107167\n",
      "Done Batch: 600 \tAverage Loss Per Batch: 0.388415858818965 \t Current Batch Loss:  0.4643322\n",
      "Done Batch: 650 \tAverage Loss Per Batch: 0.38715040145839597 \t Current Batch Loss:  0.33906212\n",
      "Done Batch: 700 \tAverage Loss Per Batch: 0.3853370303051618 \t Current Batch Loss:  0.2796875\n",
      "Done Batch: 750 \tAverage Loss Per Batch: 0.38475434301537614 \t Current Batch Loss:  0.5859452\n",
      "Done Batch: 800 \tAverage Loss Per Batch: 0.3837277717730228 \t Current Batch Loss:  0.29742327\n",
      "Done Batch: 850 \tAverage Loss Per Batch: 0.3844763026884664 \t Current Batch Loss:  0.28959188\n",
      "Done Batch: 900 \tAverage Loss Per Batch: 0.3832188488442149 \t Current Batch Loss:  0.20940131\n",
      "Done Batch: 950 \tAverage Loss Per Batch: 0.3820539706638308 \t Current Batch Loss:  0.40140945\n",
      "Done Batch: 1000 \tAverage Loss Per Batch: 0.3822089476989104 \t Current Batch Loss:  0.34451026\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done Batch: 1050 \tAverage Loss Per Batch: 0.3815984695043936 \t Current Batch Loss:  0.36075366\n",
      "Done Batch: 1100 \tAverage Loss Per Batch: 0.38121907107132763 \t Current Batch Loss:  0.38723388\n",
      "Done Batch: 1150 \tAverage Loss Per Batch: 0.38089567697327825 \t Current Batch Loss:  0.2972775\n",
      "Done Batch: 1200 \tAverage Loss Per Batch: 0.3800016696395028 \t Current Batch Loss:  0.29261538\n",
      "Done Batch: 1250 \tAverage Loss Per Batch: 0.3791576219822387 \t Current Batch Loss:  0.3959412\n",
      "Done Batch: 1300 \tAverage Loss Per Batch: 0.378142102836967 \t Current Batch Loss:  0.3067314\n",
      "Done Batch: 1350 \tAverage Loss Per Batch: 0.3775828743944955 \t Current Batch Loss:  0.3142283\n",
      "Done Batch: 1400 \tAverage Loss Per Batch: 0.3764150223450776 \t Current Batch Loss:  0.41525605\n",
      "Done Batch: 1450 \tAverage Loss Per Batch: 0.3761385553391287 \t Current Batch Loss:  0.38266346\n",
      "Done Batch: 1500 \tAverage Loss Per Batch: 0.3757325461125628 \t Current Batch Loss:  0.54991835\n",
      "Done Batch: 1550 \tAverage Loss Per Batch: 0.3749227081332416 \t Current Batch Loss:  0.3084352\n",
      "Done Batch: 1600 \tAverage Loss Per Batch: 0.37452712547566874 \t Current Batch Loss:  0.36161467\n",
      "Done Batch: 1650 \tAverage Loss Per Batch: 0.37336487783348105 \t Current Batch Loss:  0.38519832\n",
      "Done Batch: 1700 \tAverage Loss Per Batch: 0.37247671499226387 \t Current Batch Loss:  0.40853012\n",
      "Done Batch: 1750 \tAverage Loss Per Batch: 0.371241925117256 \t Current Batch Loss:  0.18363346\n",
      "Done Batch: 1800 \tAverage Loss Per Batch: 0.3708560718332047 \t Current Batch Loss:  0.35764807\n",
      "Done Batch: 1850 \tAverage Loss Per Batch: 0.3712463998338907 \t Current Batch Loss:  0.31919372\n",
      "Done Batch: 1900 \tAverage Loss Per Batch: 0.3714422823009774 \t Current Batch Loss:  0.69954073\n",
      "Done Batch: 1950 \tAverage Loss Per Batch: 0.37125811130113934 \t Current Batch Loss:  0.38530272\n",
      "Done Batch: 2000 \tAverage Loss Per Batch: 0.3709924967176672 \t Current Batch Loss:  0.48672268\n",
      "Done Batch: 2050 \tAverage Loss Per Batch: 0.3707014480846385 \t Current Batch Loss:  0.2702274\n",
      "Done Batch: 2100 \tAverage Loss Per Batch: 0.37087320434244286 \t Current Batch Loss:  0.3513186\n",
      "Done Batch: 2150 \tAverage Loss Per Batch: 0.37073800616530916 \t Current Batch Loss:  0.34312546\n",
      "Done Batch: 2200 \tAverage Loss Per Batch: 0.37041313696089034 \t Current Batch Loss:  0.409332\n",
      "Done Batch: 2250 \tAverage Loss Per Batch: 0.36997168622364845 \t Current Batch Loss:  0.25318128\n",
      "Done Batch: 2300 \tAverage Loss Per Batch: 0.36958959424420573 \t Current Batch Loss:  0.4107459\n",
      "Done Batch: 2350 \tAverage Loss Per Batch: 0.36907143114108526 \t Current Batch Loss:  0.41218603\n",
      "Done Batch: 2400 \tAverage Loss Per Batch: 0.3690424603361579 \t Current Batch Loss:  0.3256324\n",
      "Done Batch: 2450 \tAverage Loss Per Batch: 0.36938307985742447 \t Current Batch Loss:  0.3692177\n",
      "Done Batch: 2500 \tAverage Loss Per Batch: 0.3692785222254625 \t Current Batch Loss:  0.3313853\n",
      "Done Batch: 2550 \tAverage Loss Per Batch: 0.3691965302577303 \t Current Batch Loss:  0.27595842\n",
      "Done Batch: 2600 \tAverage Loss Per Batch: 0.36940270387028695 \t Current Batch Loss:  0.45129672\n",
      "Done Batch: 2650 \tAverage Loss Per Batch: 0.3690867648933888 \t Current Batch Loss:  0.29490843\n",
      "Done Batch: 2700 \tAverage Loss Per Batch: 0.3687646023595346 \t Current Batch Loss:  0.29592806\n",
      "Done Batch: 2750 \tAverage Loss Per Batch: 0.36856172010112964 \t Current Batch Loss:  0.27537733\n",
      "Done Batch: 2800 \tAverage Loss Per Batch: 0.36840639891144805 \t Current Batch Loss:  0.41508102\n",
      "Done Batch: 2850 \tAverage Loss Per Batch: 0.36820687947096387 \t Current Batch Loss:  0.43800506\n",
      "Done Batch: 2900 \tAverage Loss Per Batch: 0.36820397474146516 \t Current Batch Loss:  0.24264213\n",
      "Done Batch: 2950 \tAverage Loss Per Batch: 0.368654137814966 \t Current Batch Loss:  0.33231476\n",
      "Done Batch: 3000 \tAverage Loss Per Batch: 0.36904367696033324 \t Current Batch Loss:  0.37836045\n",
      "Done Batch: 3050 \tAverage Loss Per Batch: 0.36881349704639516 \t Current Batch Loss:  0.24950224\n",
      "Done Batch: 3100 \tAverage Loss Per Batch: 0.3683071797536443 \t Current Batch Loss:  0.39585572\n",
      "Done Batch: 3150 \tAverage Loss Per Batch: 0.36887236969011694 \t Current Batch Loss:  0.36192277\n",
      "Done Batch: 3200 \tAverage Loss Per Batch: 0.3686144503438074 \t Current Batch Loss:  0.27549857\n",
      "Done Batch: 3250 \tAverage Loss Per Batch: 0.3680051215444481 \t Current Batch Loss:  0.42936966\n",
      "Done Batch: 3300 \tAverage Loss Per Batch: 0.3681023939654019 \t Current Batch Loss:  0.61564004\n",
      "Done Batch: 3350 \tAverage Loss Per Batch: 0.36855878124429875 \t Current Batch Loss:  0.2975349\n",
      "Epoch: 36 \tTime: 4383.754285335541 \tAverage Loss Per Batch:: 0.3684460995804584\n",
      "Done Batch: 0 \tAverage Loss Per Batch: 0.3351241648197174 \t Current Batch Loss:  0.33512416\n",
      "Done Batch: 50 \tAverage Loss Per Batch: 0.37418723252474095 \t Current Batch Loss:  0.29935697\n",
      "Done Batch: 100 \tAverage Loss Per Batch: 0.3721214037130375 \t Current Batch Loss:  0.24246755\n",
      "Done Batch: 150 \tAverage Loss Per Batch: 0.3737973612270608 \t Current Batch Loss:  0.41860324\n",
      "Done Batch: 200 \tAverage Loss Per Batch: 0.36955615196061964 \t Current Batch Loss:  0.2777491\n",
      "Done Batch: 250 \tAverage Loss Per Batch: 0.3686260803643451 \t Current Batch Loss:  0.2805178\n",
      "Done Batch: 300 \tAverage Loss Per Batch: 0.3707362802519751 \t Current Batch Loss:  0.38643643\n",
      "Done Batch: 350 \tAverage Loss Per Batch: 0.3715162810861555 \t Current Batch Loss:  0.33016005\n",
      "Done Batch: 400 \tAverage Loss Per Batch: 0.37004590692216915 \t Current Batch Loss:  0.43961024\n",
      "Done Batch: 450 \tAverage Loss Per Batch: 0.3731832084330646 \t Current Batch Loss:  0.33987907\n",
      "Done Batch: 500 \tAverage Loss Per Batch: 0.37248658301111703 \t Current Batch Loss:  0.545686\n",
      "Done Batch: 550 \tAverage Loss Per Batch: 0.37325933021231267 \t Current Batch Loss:  0.40944955\n",
      "Done Batch: 600 \tAverage Loss Per Batch: 0.3733287674565482 \t Current Batch Loss:  0.3190174\n",
      "Done Batch: 650 \tAverage Loss Per Batch: 0.3722863831598821 \t Current Batch Loss:  0.3148812\n",
      "Done Batch: 700 \tAverage Loss Per Batch: 0.37239958711510546 \t Current Batch Loss:  0.27658233\n",
      "Done Batch: 750 \tAverage Loss Per Batch: 0.37213170968898923 \t Current Batch Loss:  0.3148363\n",
      "Done Batch: 800 \tAverage Loss Per Batch: 0.3705201143591889 \t Current Batch Loss:  0.3935122\n",
      "Done Batch: 850 \tAverage Loss Per Batch: 0.3697987544305456 \t Current Batch Loss:  0.3681749\n",
      "Done Batch: 900 \tAverage Loss Per Batch: 0.3692616043523467 \t Current Batch Loss:  0.22044279\n",
      "Done Batch: 950 \tAverage Loss Per Batch: 0.36906801156315766 \t Current Batch Loss:  0.33955216\n",
      "Done Batch: 1000 \tAverage Loss Per Batch: 0.3702649988613643 \t Current Batch Loss:  0.43974423\n",
      "Done Batch: 1050 \tAverage Loss Per Batch: 0.3692283114714355 \t Current Batch Loss:  0.6343396\n",
      "Done Batch: 1100 \tAverage Loss Per Batch: 0.36887260590706167 \t Current Batch Loss:  0.6565619\n",
      "Done Batch: 1150 \tAverage Loss Per Batch: 0.3677904698431336 \t Current Batch Loss:  0.44271213\n",
      "Done Batch: 1200 \tAverage Loss Per Batch: 0.3671014988849205 \t Current Batch Loss:  0.45164615\n",
      "Done Batch: 1250 \tAverage Loss Per Batch: 0.36628860123247076 \t Current Batch Loss:  0.29619294\n",
      "Done Batch: 1300 \tAverage Loss Per Batch: 0.3651474763186137 \t Current Batch Loss:  0.65515196\n",
      "Done Batch: 1350 \tAverage Loss Per Batch: 0.3647524148093428 \t Current Batch Loss:  0.48417205\n",
      "Done Batch: 1400 \tAverage Loss Per Batch: 0.3634811594263304 \t Current Batch Loss:  0.35539836\n",
      "Done Batch: 1450 \tAverage Loss Per Batch: 0.3629877098690305 \t Current Batch Loss:  0.23440193\n",
      "Done Batch: 1500 \tAverage Loss Per Batch: 0.36174424771941716 \t Current Batch Loss:  0.40159515\n",
      "Done Batch: 1550 \tAverage Loss Per Batch: 0.36138136328458015 \t Current Batch Loss:  0.4242998\n",
      "Done Batch: 1600 \tAverage Loss Per Batch: 0.36068336432014386 \t Current Batch Loss:  0.31165838\n",
      "Done Batch: 1650 \tAverage Loss Per Batch: 0.3593129062656198 \t Current Batch Loss:  0.42406175\n",
      "Done Batch: 1700 \tAverage Loss Per Batch: 0.35816548773465895 \t Current Batch Loss:  0.5620104\n",
      "Done Batch: 1750 \tAverage Loss Per Batch: 0.3577702691271127 \t Current Batch Loss:  0.3504953\n",
      "Done Batch: 1800 \tAverage Loss Per Batch: 0.3575799045365496 \t Current Batch Loss:  0.26589003\n",
      "Done Batch: 1850 \tAverage Loss Per Batch: 0.35730446314792386 \t Current Batch Loss:  0.5329409\n",
      "Done Batch: 1900 \tAverage Loss Per Batch: 0.3573057948210439 \t Current Batch Loss:  0.19844273\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done Batch: 1950 \tAverage Loss Per Batch: 0.35676725586443786 \t Current Batch Loss:  0.24628001\n",
      "Done Batch: 2000 \tAverage Loss Per Batch: 0.3565418747538093 \t Current Batch Loss:  0.4690899\n",
      "Done Batch: 2050 \tAverage Loss Per Batch: 0.35606006037412996 \t Current Batch Loss:  0.28104487\n",
      "Done Batch: 2100 \tAverage Loss Per Batch: 0.3555599261307535 \t Current Batch Loss:  0.22843157\n",
      "Done Batch: 2150 \tAverage Loss Per Batch: 0.3559320685455878 \t Current Batch Loss:  0.5255421\n",
      "Done Batch: 2200 \tAverage Loss Per Batch: 0.35568429804874624 \t Current Batch Loss:  0.41016418\n",
      "Done Batch: 2250 \tAverage Loss Per Batch: 0.35541796274168763 \t Current Batch Loss:  0.3044624\n",
      "Done Batch: 2300 \tAverage Loss Per Batch: 0.35547778536539293 \t Current Batch Loss:  0.26783732\n",
      "Done Batch: 2350 \tAverage Loss Per Batch: 0.35514309138744143 \t Current Batch Loss:  0.23312514\n",
      "Done Batch: 2400 \tAverage Loss Per Batch: 0.3549876015125985 \t Current Batch Loss:  0.18277366\n",
      "Done Batch: 2450 \tAverage Loss Per Batch: 0.354630731350181 \t Current Batch Loss:  0.20907192\n",
      "Done Batch: 2500 \tAverage Loss Per Batch: 0.35424121122498936 \t Current Batch Loss:  0.32301155\n",
      "Done Batch: 2550 \tAverage Loss Per Batch: 0.3540211016964277 \t Current Batch Loss:  0.18047862\n",
      "Done Batch: 2600 \tAverage Loss Per Batch: 0.3539608571304582 \t Current Batch Loss:  0.33059052\n",
      "Done Batch: 2650 \tAverage Loss Per Batch: 0.35423287817585114 \t Current Batch Loss:  0.5585592\n",
      "Done Batch: 2700 \tAverage Loss Per Batch: 0.35483010469355525 \t Current Batch Loss:  0.40194198\n",
      "Done Batch: 2750 \tAverage Loss Per Batch: 0.35468198510407534 \t Current Batch Loss:  0.40792164\n",
      "Done Batch: 2800 \tAverage Loss Per Batch: 0.35461193224982424 \t Current Batch Loss:  0.4747345\n",
      "Done Batch: 2850 \tAverage Loss Per Batch: 0.3545642780085816 \t Current Batch Loss:  0.36308318\n",
      "Done Batch: 2900 \tAverage Loss Per Batch: 0.3548265206494318 \t Current Batch Loss:  0.27332583\n",
      "Done Batch: 2950 \tAverage Loss Per Batch: 0.3552589965312007 \t Current Batch Loss:  0.29067382\n",
      "Done Batch: 3000 \tAverage Loss Per Batch: 0.35555309066918245 \t Current Batch Loss:  0.6456484\n",
      "Done Batch: 3050 \tAverage Loss Per Batch: 0.3558781134817531 \t Current Batch Loss:  0.50835913\n",
      "Done Batch: 3100 \tAverage Loss Per Batch: 0.3564005339988506 \t Current Batch Loss:  0.3717489\n",
      "Done Batch: 3150 \tAverage Loss Per Batch: 0.3564044106071883 \t Current Batch Loss:  0.3846149\n",
      "Done Batch: 3200 \tAverage Loss Per Batch: 0.3560759460007128 \t Current Batch Loss:  0.26096994\n",
      "Done Batch: 3250 \tAverage Loss Per Batch: 0.35599183094562 \t Current Batch Loss:  0.2747585\n",
      "Done Batch: 3300 \tAverage Loss Per Batch: 0.3559741679984163 \t Current Batch Loss:  0.3190628\n",
      "Done Batch: 3350 \tAverage Loss Per Batch: 0.3560022739257182 \t Current Batch Loss:  0.33394563\n",
      "Epoch: 37 \tTime: 4386.138866186142 \tAverage Loss Per Batch:: 0.3562028152933983\n",
      "Done Batch: 0 \tAverage Loss Per Batch: 0.1836545765399933 \t Current Batch Loss:  0.18365458\n",
      "Done Batch: 50 \tAverage Loss Per Batch: 0.36891858834846347 \t Current Batch Loss:  0.37797776\n",
      "Done Batch: 100 \tAverage Loss Per Batch: 0.3623708313939595 \t Current Batch Loss:  0.4975394\n",
      "Done Batch: 150 \tAverage Loss Per Batch: 0.35257866998381965 \t Current Batch Loss:  0.37232026\n",
      "Done Batch: 200 \tAverage Loss Per Batch: 0.35526775424160173 \t Current Batch Loss:  0.39652842\n",
      "Done Batch: 250 \tAverage Loss Per Batch: 0.35757455722506776 \t Current Batch Loss:  0.23064409\n",
      "Done Batch: 300 \tAverage Loss Per Batch: 0.3571176126450795 \t Current Batch Loss:  0.40703893\n",
      "Done Batch: 350 \tAverage Loss Per Batch: 0.3562696800272689 \t Current Batch Loss:  0.46743354\n",
      "Done Batch: 400 \tAverage Loss Per Batch: 0.35694830149337836 \t Current Batch Loss:  0.2159754\n",
      "Done Batch: 450 \tAverage Loss Per Batch: 0.35923144108968935 \t Current Batch Loss:  0.3113852\n",
      "Done Batch: 500 \tAverage Loss Per Batch: 0.3578034336576443 \t Current Batch Loss:  0.44586074\n",
      "Done Batch: 550 \tAverage Loss Per Batch: 0.35936236719578885 \t Current Batch Loss:  0.3070985\n",
      "Done Batch: 600 \tAverage Loss Per Batch: 0.3593636867210591 \t Current Batch Loss:  0.24213496\n",
      "Done Batch: 650 \tAverage Loss Per Batch: 0.36034531410663356 \t Current Batch Loss:  0.26458248\n",
      "Done Batch: 700 \tAverage Loss Per Batch: 0.3577079679843533 \t Current Batch Loss:  0.50720924\n",
      "Done Batch: 750 \tAverage Loss Per Batch: 0.35505553234750514 \t Current Batch Loss:  0.2592628\n",
      "Done Batch: 800 \tAverage Loss Per Batch: 0.3555064487286126 \t Current Batch Loss:  0.2898352\n",
      "Done Batch: 850 \tAverage Loss Per Batch: 0.3566934121761703 \t Current Batch Loss:  0.34596866\n",
      "Done Batch: 900 \tAverage Loss Per Batch: 0.35735120575515333 \t Current Batch Loss:  0.28259954\n",
      "Done Batch: 950 \tAverage Loss Per Batch: 0.3573693441761781 \t Current Batch Loss:  0.38236892\n",
      "Done Batch: 1000 \tAverage Loss Per Batch: 0.3577686595869112 \t Current Batch Loss:  0.27821344\n",
      "Done Batch: 1050 \tAverage Loss Per Batch: 0.35764809331363095 \t Current Batch Loss:  0.26253805\n",
      "Done Batch: 1100 \tAverage Loss Per Batch: 0.35680935010110976 \t Current Batch Loss:  0.3663448\n",
      "Done Batch: 1150 \tAverage Loss Per Batch: 0.3564372680220368 \t Current Batch Loss:  0.3024002\n",
      "Done Batch: 1200 \tAverage Loss Per Batch: 0.3557407089837088 \t Current Batch Loss:  0.2216962\n",
      "Done Batch: 1250 \tAverage Loss Per Batch: 0.3556832679146104 \t Current Batch Loss:  0.391028\n",
      "Done Batch: 1300 \tAverage Loss Per Batch: 0.3558687016562624 \t Current Batch Loss:  0.21704063\n",
      "Done Batch: 1350 \tAverage Loss Per Batch: 0.3560275771523475 \t Current Batch Loss:  0.3013157\n",
      "Done Batch: 1400 \tAverage Loss Per Batch: 0.3554237134898245 \t Current Batch Loss:  0.34741387\n",
      "Done Batch: 1450 \tAverage Loss Per Batch: 0.35540549983902686 \t Current Batch Loss:  0.44357452\n",
      "Done Batch: 1500 \tAverage Loss Per Batch: 0.35491185617955184 \t Current Batch Loss:  0.37591195\n",
      "Done Batch: 1550 \tAverage Loss Per Batch: 0.35467287084312304 \t Current Batch Loss:  0.27135676\n",
      "Done Batch: 1600 \tAverage Loss Per Batch: 0.3548232773871514 \t Current Batch Loss:  0.22433124\n",
      "Done Batch: 1650 \tAverage Loss Per Batch: 0.354145105989105 \t Current Batch Loss:  0.35131195\n",
      "Done Batch: 1700 \tAverage Loss Per Batch: 0.35332287867268275 \t Current Batch Loss:  0.4553246\n",
      "Done Batch: 1750 \tAverage Loss Per Batch: 0.35268779633421005 \t Current Batch Loss:  0.37175652\n",
      "Done Batch: 1800 \tAverage Loss Per Batch: 0.35129593101757756 \t Current Batch Loss:  0.23900439\n",
      "Done Batch: 1850 \tAverage Loss Per Batch: 0.3508484912105408 \t Current Batch Loss:  0.28400534\n",
      "Done Batch: 1900 \tAverage Loss Per Batch: 0.35020558329113904 \t Current Batch Loss:  0.2566842\n",
      "Done Batch: 1950 \tAverage Loss Per Batch: 0.3496708811658398 \t Current Batch Loss:  0.26056418\n",
      "Done Batch: 2000 \tAverage Loss Per Batch: 0.3493199726973457 \t Current Batch Loss:  0.2217608\n",
      "Done Batch: 2050 \tAverage Loss Per Batch: 0.3490478320856434 \t Current Batch Loss:  0.2831782\n",
      "Done Batch: 2100 \tAverage Loss Per Batch: 0.34912072954746387 \t Current Batch Loss:  0.40932694\n",
      "Done Batch: 2150 \tAverage Loss Per Batch: 0.34836096159666985 \t Current Batch Loss:  0.3639836\n",
      "Done Batch: 2200 \tAverage Loss Per Batch: 0.3484238278237542 \t Current Batch Loss:  0.4141596\n",
      "Done Batch: 2250 \tAverage Loss Per Batch: 0.3475980350424162 \t Current Batch Loss:  0.37229407\n",
      "Done Batch: 2300 \tAverage Loss Per Batch: 0.34713464097865815 \t Current Batch Loss:  0.39316857\n",
      "Done Batch: 2350 \tAverage Loss Per Batch: 0.34704793187873206 \t Current Batch Loss:  0.26554\n",
      "Done Batch: 2400 \tAverage Loss Per Batch: 0.3471229485003762 \t Current Batch Loss:  0.31135657\n",
      "Done Batch: 2450 \tAverage Loss Per Batch: 0.34666797398961546 \t Current Batch Loss:  0.16546783\n",
      "Done Batch: 2500 \tAverage Loss Per Batch: 0.3467233106583321 \t Current Batch Loss:  0.33261004\n",
      "Done Batch: 2550 \tAverage Loss Per Batch: 0.3466645597547047 \t Current Batch Loss:  0.38596588\n",
      "Done Batch: 2600 \tAverage Loss Per Batch: 0.34641188113265936 \t Current Batch Loss:  0.42493287\n",
      "Done Batch: 2650 \tAverage Loss Per Batch: 0.3463646157963327 \t Current Batch Loss:  0.31564194\n",
      "Done Batch: 2700 \tAverage Loss Per Batch: 0.34628871651500825 \t Current Batch Loss:  0.36733627\n",
      "Done Batch: 2750 \tAverage Loss Per Batch: 0.34625040888461317 \t Current Batch Loss:  0.34336367\n",
      "Done Batch: 2800 \tAverage Loss Per Batch: 0.3459849896621636 \t Current Batch Loss:  0.28557262\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done Batch: 2850 \tAverage Loss Per Batch: 0.3457486242557651 \t Current Batch Loss:  0.33280984\n",
      "Done Batch: 2900 \tAverage Loss Per Batch: 0.345880292528089 \t Current Batch Loss:  0.314932\n",
      "Done Batch: 2950 \tAverage Loss Per Batch: 0.34592442044319843 \t Current Batch Loss:  0.32298416\n",
      "Done Batch: 3000 \tAverage Loss Per Batch: 0.345720558638614 \t Current Batch Loss:  0.39067292\n",
      "Done Batch: 3050 \tAverage Loss Per Batch: 0.34554887749315755 \t Current Batch Loss:  0.32814285\n",
      "Done Batch: 3100 \tAverage Loss Per Batch: 0.3456367425853612 \t Current Batch Loss:  0.38342407\n",
      "Done Batch: 3150 \tAverage Loss Per Batch: 0.3459436959858661 \t Current Batch Loss:  0.42516717\n",
      "Done Batch: 3200 \tAverage Loss Per Batch: 0.3459871487621589 \t Current Batch Loss:  0.46888933\n",
      "Done Batch: 3250 \tAverage Loss Per Batch: 0.34601062115926956 \t Current Batch Loss:  0.39264068\n",
      "Done Batch: 3300 \tAverage Loss Per Batch: 0.3459912927014941 \t Current Batch Loss:  0.29322922\n",
      "Done Batch: 3350 \tAverage Loss Per Batch: 0.3463959703691608 \t Current Batch Loss:  0.18263198\n",
      "Epoch: 38 \tTime: 4388.2847056388855 \tAverage Loss Per Batch:: 0.346196222752731\n",
      "Done Batch: 0 \tAverage Loss Per Batch: 0.3348313271999359 \t Current Batch Loss:  0.33483133\n",
      "Done Batch: 50 \tAverage Loss Per Batch: 0.3299913733613257 \t Current Batch Loss:  0.32431954\n",
      "Done Batch: 100 \tAverage Loss Per Batch: 0.33226466326430293 \t Current Batch Loss:  0.42552507\n",
      "Done Batch: 150 \tAverage Loss Per Batch: 0.3404591219709409 \t Current Batch Loss:  0.34825462\n",
      "Done Batch: 200 \tAverage Loss Per Batch: 0.3451444548160876 \t Current Batch Loss:  0.49180982\n",
      "Done Batch: 250 \tAverage Loss Per Batch: 0.34560950017307857 \t Current Batch Loss:  0.30459878\n",
      "Done Batch: 300 \tAverage Loss Per Batch: 0.34480229604283835 \t Current Batch Loss:  0.40867782\n",
      "Done Batch: 350 \tAverage Loss Per Batch: 0.346834119738337 \t Current Batch Loss:  0.30964196\n",
      "Done Batch: 400 \tAverage Loss Per Batch: 0.35065512556090317 \t Current Batch Loss:  0.2761864\n",
      "Done Batch: 450 \tAverage Loss Per Batch: 0.3466578886871063 \t Current Batch Loss:  0.3415609\n",
      "Done Batch: 500 \tAverage Loss Per Batch: 0.3456157636261748 \t Current Batch Loss:  0.3204416\n",
      "Done Batch: 550 \tAverage Loss Per Batch: 0.3456394450363793 \t Current Batch Loss:  0.26927483\n",
      "Done Batch: 600 \tAverage Loss Per Batch: 0.344466729067923 \t Current Batch Loss:  0.3946985\n",
      "Done Batch: 650 \tAverage Loss Per Batch: 0.3458488335898761 \t Current Batch Loss:  0.17192169\n",
      "Done Batch: 700 \tAverage Loss Per Batch: 0.3447458804206399 \t Current Batch Loss:  0.23615946\n",
      "Done Batch: 750 \tAverage Loss Per Batch: 0.34361801882796533 \t Current Batch Loss:  0.19922365\n",
      "Done Batch: 800 \tAverage Loss Per Batch: 0.3440773292873683 \t Current Batch Loss:  0.3307578\n",
      "Done Batch: 850 \tAverage Loss Per Batch: 0.3445147085764153 \t Current Batch Loss:  0.44335216\n",
      "Done Batch: 900 \tAverage Loss Per Batch: 0.3446045585894558 \t Current Batch Loss:  0.29428604\n",
      "Done Batch: 950 \tAverage Loss Per Batch: 0.3464198904082602 \t Current Batch Loss:  0.24332634\n",
      "Done Batch: 1000 \tAverage Loss Per Batch: 0.3474880244646158 \t Current Batch Loss:  0.23349562\n",
      "Done Batch: 1050 \tAverage Loss Per Batch: 0.34670947088047394 \t Current Batch Loss:  0.17982693\n",
      "Done Batch: 1100 \tAverage Loss Per Batch: 0.3461134531869334 \t Current Batch Loss:  0.27356732\n",
      "Done Batch: 1150 \tAverage Loss Per Batch: 0.34563311366129706 \t Current Batch Loss:  0.37521264\n",
      "Done Batch: 1200 \tAverage Loss Per Batch: 0.3453329214446253 \t Current Batch Loss:  0.26182073\n",
      "Done Batch: 1250 \tAverage Loss Per Batch: 0.34441560843913294 \t Current Batch Loss:  0.34888732\n",
      "Done Batch: 1300 \tAverage Loss Per Batch: 0.34355496210285924 \t Current Batch Loss:  0.19046223\n",
      "Done Batch: 1350 \tAverage Loss Per Batch: 0.3429157804397192 \t Current Batch Loss:  0.19581807\n",
      "Done Batch: 1400 \tAverage Loss Per Batch: 0.342101776072334 \t Current Batch Loss:  0.23378909\n",
      "Done Batch: 1450 \tAverage Loss Per Batch: 0.3405731646543039 \t Current Batch Loss:  0.2373773\n",
      "Done Batch: 1500 \tAverage Loss Per Batch: 0.33991314627622304 \t Current Batch Loss:  0.22908539\n",
      "Done Batch: 1550 \tAverage Loss Per Batch: 0.33909213606347427 \t Current Batch Loss:  0.28500733\n",
      "Done Batch: 1600 \tAverage Loss Per Batch: 0.33906760941178704 \t Current Batch Loss:  0.25804198\n",
      "Done Batch: 1650 \tAverage Loss Per Batch: 0.3386481134170189 \t Current Batch Loss:  0.3037666\n",
      "Done Batch: 1700 \tAverage Loss Per Batch: 0.3378413416913928 \t Current Batch Loss:  0.2390142\n",
      "Done Batch: 1750 \tAverage Loss Per Batch: 0.337245708334657 \t Current Batch Loss:  0.28735757\n",
      "Done Batch: 1800 \tAverage Loss Per Batch: 0.3366341850017588 \t Current Batch Loss:  0.36540473\n",
      "Done Batch: 1850 \tAverage Loss Per Batch: 0.3357716397151375 \t Current Batch Loss:  0.27666286\n",
      "Done Batch: 1900 \tAverage Loss Per Batch: 0.3351728790907531 \t Current Batch Loss:  0.2431174\n",
      "Done Batch: 1950 \tAverage Loss Per Batch: 0.3355462513157553 \t Current Batch Loss:  0.28268227\n",
      "Done Batch: 2000 \tAverage Loss Per Batch: 0.33436512034842514 \t Current Batch Loss:  0.26823854\n",
      "Done Batch: 2050 \tAverage Loss Per Batch: 0.3341766721530405 \t Current Batch Loss:  0.4020214\n",
      "Done Batch: 2100 \tAverage Loss Per Batch: 0.3341268185971182 \t Current Batch Loss:  0.26285845\n",
      "Done Batch: 2150 \tAverage Loss Per Batch: 0.3337340014291774 \t Current Batch Loss:  0.3070473\n",
      "Done Batch: 2200 \tAverage Loss Per Batch: 0.33331901033741945 \t Current Batch Loss:  0.30274695\n",
      "Done Batch: 2250 \tAverage Loss Per Batch: 0.33347901356042836 \t Current Batch Loss:  0.3396328\n",
      "Done Batch: 2300 \tAverage Loss Per Batch: 0.3332059543121643 \t Current Batch Loss:  0.4126381\n",
      "Done Batch: 2350 \tAverage Loss Per Batch: 0.33299409016897097 \t Current Batch Loss:  0.30841523\n",
      "Done Batch: 2400 \tAverage Loss Per Batch: 0.3329246815065948 \t Current Batch Loss:  0.24600281\n",
      "Done Batch: 2450 \tAverage Loss Per Batch: 0.3327501774509067 \t Current Batch Loss:  0.43293872\n",
      "Done Batch: 2500 \tAverage Loss Per Batch: 0.3332918223436 \t Current Batch Loss:  0.26149085\n",
      "Done Batch: 2550 \tAverage Loss Per Batch: 0.3330409175499847 \t Current Batch Loss:  0.30424628\n",
      "Done Batch: 2600 \tAverage Loss Per Batch: 0.3327093768359285 \t Current Batch Loss:  0.2638167\n",
      "Done Batch: 2650 \tAverage Loss Per Batch: 0.3324668639164203 \t Current Batch Loss:  0.4221473\n",
      "Done Batch: 2700 \tAverage Loss Per Batch: 0.3323203573394422 \t Current Batch Loss:  0.38092864\n",
      "Done Batch: 2750 \tAverage Loss Per Batch: 0.33234958396428976 \t Current Batch Loss:  0.29458377\n",
      "Done Batch: 2800 \tAverage Loss Per Batch: 0.33228930773766113 \t Current Batch Loss:  0.23229304\n",
      "Done Batch: 2850 \tAverage Loss Per Batch: 0.332473827216948 \t Current Batch Loss:  0.3593938\n",
      "Done Batch: 2900 \tAverage Loss Per Batch: 0.3325846238907392 \t Current Batch Loss:  0.21440248\n",
      "Done Batch: 2950 \tAverage Loss Per Batch: 0.33240346522431824 \t Current Batch Loss:  0.15776494\n",
      "Done Batch: 3000 \tAverage Loss Per Batch: 0.3327611718141846 \t Current Batch Loss:  0.43987983\n",
      "Done Batch: 3050 \tAverage Loss Per Batch: 0.3328139919363956 \t Current Batch Loss:  0.31798944\n",
      "Done Batch: 3100 \tAverage Loss Per Batch: 0.3330341506339742 \t Current Batch Loss:  0.35732964\n",
      "Done Batch: 3150 \tAverage Loss Per Batch: 0.3331445357778533 \t Current Batch Loss:  0.25138372\n",
      "Done Batch: 3200 \tAverage Loss Per Batch: 0.3337090779113345 \t Current Batch Loss:  0.297161\n",
      "Done Batch: 3250 \tAverage Loss Per Batch: 0.33398330299414625 \t Current Batch Loss:  0.4347677\n",
      "Done Batch: 3300 \tAverage Loss Per Batch: 0.3342334794022213 \t Current Batch Loss:  0.17466906\n",
      "Done Batch: 3350 \tAverage Loss Per Batch: 0.334487031385974 \t Current Batch Loss:  0.28668603\n",
      "Epoch: 39 \tTime: 4389.159208059311 \tAverage Loss Per Batch:: 0.3342232415543879\n"
     ]
    }
   ],
   "source": [
    "def train(model, learning_rate=0.0001, batch_size=100, epochs=10):\n",
    "    \n",
    "    # defining criterion and optimizer\n",
    "    criterion = nn.NLLLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr = learning_rate)\n",
    "    \n",
    "    number_of_batches = math.ceil(len(train_image_addresses)/batch_size)\n",
    "    indexes = np.arange(len(train_image_addresses))\n",
    "    \n",
    "    loss_arr = []\n",
    "    for epoch in range(30, 30 + epochs):\n",
    "        avgloss = 0.0\n",
    "        start = time.time()\n",
    "        avg_loss_arr = []\n",
    "        for batch in range(number_of_batches):\n",
    "            train_indexes = [train_image_addresses[i] for i in indexes[batch*batch_size:(batch+1)*batch_size]]\n",
    "            image_batch, target_batch = load_batch(train_indexes)\n",
    "            loss = train_pass(image_batch, target_batch, model, optimizer, criterion)\n",
    "            avgloss += loss\n",
    "            if batch%50 == 0:\n",
    "                print (\"Done Batch:\", batch, \"\\tAverage Loss Per Batch:\", avgloss/(batch+1), \"\\t Current Batch Loss: \", loss)\n",
    "        loss_arr.append(avgloss/(batch+1))\n",
    "        print (\"Epoch:\",epoch, \"\\tTime:\", time.time() - start, \"\\tAverage Loss Per Batch::\", avgloss/(batch+1))\n",
    "        torch.save({'epoch': epoch ,'state_dict': model.state_dict(),'optimizer': optimizer.state_dict()}, open(\"outputs/anp_log_softmax_classifier_batch_\"+str(epoch), \"wb+\"))\n",
    "    loss_arr = np.array(loss_arr)\n",
    "    np.save(open('outputs/loss_log_softmax_anp_classifier', 'wb+'), loss_arr)\n",
    "    \n",
    "train(main_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation\n",
    "\n",
    "Here we tested different epochs of training extensively to evaluate validation accuracy with respect to more training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outdoor training from validation set wasn't found in the train set.\n",
      "outdoor training from test set wasn't found in the train set.\n",
      "Checking validation set for having the right tags.\n",
      "Train tag not found at data/vso/vso_images_with_cc/outdoor_training_validation/5025612600_9c58b5081e.jpg\n",
      "Image data/vso/vso_images_with_cc/outdoor_training_validation/5025612600_9c58b5081e.jpg was removed.\n",
      "Train tag not found at data/vso/vso_images_with_cc/outdoor_training_validation/51229414_2f4b4d40b6.jpg\n",
      "Image data/vso/vso_images_with_cc/outdoor_training_validation/51229414_2f4b4d40b6.jpg was removed.\n",
      "Train tag not found at data/vso/vso_images_with_cc/outdoor_training_validation/8119335721_a3bce90cac.jpg\n",
      "Image data/vso/vso_images_with_cc/outdoor_training_validation/8119335721_a3bce90cac.jpg was removed.\n",
      "Train tag not found at data/vso/vso_images_with_cc/outdoor_training_validation/8041258048_a9d5c51e2a.jpg\n",
      "Image data/vso/vso_images_with_cc/outdoor_training_validation/8041258048_a9d5c51e2a.jpg was removed.\n",
      "Train tag not found at data/vso/vso_images_with_cc/outdoor_training_validation/5643530365_e8dbcf6f72.jpg\n",
      "Image data/vso/vso_images_with_cc/outdoor_training_validation/5643530365_e8dbcf6f72.jpg was removed.\n",
      "Train tag not found at data/vso/vso_images_with_cc/outdoor_training_validation/5739054725_7587f7da4d.jpg\n",
      "Image data/vso/vso_images_with_cc/outdoor_training_validation/5739054725_7587f7da4d.jpg was removed.\n",
      "Train tag not found at data/vso/vso_images_with_cc/outdoor_training_validation/974498816_0053e880d2.jpg\n",
      "Image data/vso/vso_images_with_cc/outdoor_training_validation/974498816_0053e880d2.jpg was removed.\n",
      "Train tag not found at data/vso/vso_images_with_cc/outdoor_training_validation/5025593886_8e70847c70.jpg\n",
      "Image data/vso/vso_images_with_cc/outdoor_training_validation/5025593886_8e70847c70.jpg was removed.\n",
      "Train tag not found at data/vso/vso_images_with_cc/outdoor_training_validation/2380840546_007c910100.jpg\n",
      "Image data/vso/vso_images_with_cc/outdoor_training_validation/2380840546_007c910100.jpg was removed.\n",
      "Checking test set for having the right tags.\n",
      "Train tag not found at data/vso/vso_images_with_cc/outdoor_training_test/3479025064_4a45a51828.jpg\n",
      "Image data/vso/vso_images_with_cc/outdoor_training_test/3479025064_4a45a51828.jpg was removed.\n",
      "Train tag not found at data/vso/vso_images_with_cc/outdoor_training_test/684877593_c0e5d59d0e.jpg\n",
      "Image data/vso/vso_images_with_cc/outdoor_training_test/684877593_c0e5d59d0e.jpg was removed.\n",
      "Train tag not found at data/vso/vso_images_with_cc/outdoor_training_test/685761898_a9ff5f7406.jpg\n",
      "Image data/vso/vso_images_with_cc/outdoor_training_test/685761898_a9ff5f7406.jpg was removed.\n",
      "Train tag not found at data/vso/vso_images_with_cc/outdoor_training_test/4100643470_dcf96c361e.jpg\n",
      "Image data/vso/vso_images_with_cc/outdoor_training_test/4100643470_dcf96c361e.jpg was removed.\n",
      "Train tag not found at data/vso/vso_images_with_cc/outdoor_training_test/5025004515_4feb823c42.jpg\n",
      "Image data/vso/vso_images_with_cc/outdoor_training_test/5025004515_4feb823c42.jpg was removed.\n",
      "Train tag not found at data/vso/vso_images_with_cc/outdoor_training_test/4056245456_69f1081a91.jpg\n",
      "Image data/vso/vso_images_with_cc/outdoor_training_test/4056245456_69f1081a91.jpg was removed.\n",
      "Train tag not found at data/vso/vso_images_with_cc/outdoor_training_test/51032900_48156af973.jpg\n",
      "Image data/vso/vso_images_with_cc/outdoor_training_test/51032900_48156af973.jpg was removed.\n",
      "Train tag not found at data/vso/vso_images_with_cc/outdoor_training_test/4056296566_6ca85e520b.jpg\n",
      "Image data/vso/vso_images_with_cc/outdoor_training_test/4056296566_6ca85e520b.jpg was removed.\n",
      "Train tag not found at data/vso/vso_images_with_cc/outdoor_training_test/5704817308_9c0bfc6469.jpg\n",
      "Image data/vso/vso_images_with_cc/outdoor_training_test/5704817308_9c0bfc6469.jpg was removed.\n",
      "Train tag not found at data/vso/vso_images_with_cc/outdoor_training_test/5025614358_1418356704.jpg\n",
      "Image data/vso/vso_images_with_cc/outdoor_training_test/5025614358_1418356704.jpg was removed.\n"
     ]
    }
   ],
   "source": [
    "for tag in validation_anp_tags:\n",
    "    if not tag in train_anp_tags:\n",
    "        print(tag + \" from validation set wasn't found in the train set.\")\n",
    "        \n",
    "for tag in test_anp_tags:\n",
    "    if not tag in train_anp_tags:\n",
    "        print(tag + \" from test set wasn't found in the train set.\")\n",
    "        \n",
    "print(\"Checking validation set for having the right tags.\")\n",
    "for image in validation_image_addresses:\n",
    "    if not validation_image_to_anp_tag[image] in train_anp_tags:\n",
    "        print(\"Train tag not found at \" + image)\n",
    "        validation_image_addresses.remove(image)\n",
    "        print(\"Image \" + image + \" was removed.\")\n",
    "        \n",
    "print(\"Checking test set for having the right tags.\")\n",
    "for image in test_image_addresses:\n",
    "    if not test_image_to_anp_tag[image] in train_anp_tags:\n",
    "        print(\"Train tag not found at \" + image)\n",
    "        test_image_addresses.remove(image)\n",
    "        print(\"Image \" + image + \" was removed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done Validation Batch: 0 \n",
      "Average Single Error Per Batch: 0.97 \t Current Single Error:  0.97 \n",
      "Average Top5 Error Per Batch: 0.92 \t Current Top5 Error:  0.92 \n",
      "Average Top10 Error Per Batch: 0.88 \t Current Top10 Error:  0.88 \n",
      "Average Top20 Error Per Batch: 0.84 \t Current Top20 Error:  0.84\n",
      "Done Validation Batch: 50 \n",
      "Average Single Error Per Batch: 0.9799999999999995 \t Current Single Error:  1.0 \n",
      "Average Top5 Error Per Batch: 0.9313725490196078 \t Current Top5 Error:  0.95 \n",
      "Average Top10 Error Per Batch: 0.8901960784313724 \t Current Top10 Error:  0.89 \n",
      "Average Top20 Error Per Batch: 0.8366666666666667 \t Current Top20 Error:  0.84\n",
      "Done Validation Batch: 100 \n",
      "Average Single Error Per Batch: 0.9790099009900982 \t Current Single Error:  0.97 \n",
      "Average Top5 Error Per Batch: 0.9293069306930695 \t Current Top5 Error:  0.9 \n",
      "Average Top10 Error Per Batch: 0.8900000000000001 \t Current Top10 Error:  0.87 \n",
      "Average Top20 Error Per Batch: 0.8362376237623759 \t Current Top20 Error:  0.84\n",
      "Done Validation Batch: 150 \n",
      "Average Single Error Per Batch: 0.9788079470198667 \t Current Single Error:  0.98 \n",
      "Average Top5 Error Per Batch: 0.9290728476821195 \t Current Top5 Error:  0.92 \n",
      "Average Top10 Error Per Batch: 0.8905960264900665 \t Current Top10 Error:  0.86 \n",
      "Average Top20 Error Per Batch: 0.8368874172185424 \t Current Top20 Error:  0.81\n",
      "Done Validation Batch: 200 \n",
      "Average Single Error Per Batch: 0.9783582089552231 \t Current Single Error:  0.96 \n",
      "Average Top5 Error Per Batch: 0.9280099502487557 \t Current Top5 Error:  0.9299999999999999 \n",
      "Average Top10 Error Per Batch: 0.8897014925373135 \t Current Top10 Error:  0.9 \n",
      "Average Top20 Error Per Batch: 0.8362189054726362 \t Current Top20 Error:  0.8200000000000001\n",
      "Done Validation Batch: 250 \n",
      "Average Single Error Per Batch: 0.9781673306772898 \t Current Single Error:  1.0 \n",
      "Average Top5 Error Per Batch: 0.9282071713147402 \t Current Top5 Error:  0.95 \n",
      "Average Top10 Error Per Batch: 0.8887250996015933 \t Current Top10 Error:  0.91 \n",
      "Average Top20 Error Per Batch: 0.8360956175298804 \t Current Top20 Error:  0.86\n",
      "Done Validation Batch: 300 \n",
      "Average Single Error Per Batch: 0.9777740863787375 \t Current Single Error:  1.0 \n",
      "Average Top5 Error Per Batch: 0.9281727574750823 \t Current Top5 Error:  0.96 \n",
      "Average Top10 Error Per Batch: 0.888870431893688 \t Current Top10 Error:  0.9299999999999999 \n",
      "Average Top20 Error Per Batch: 0.8355813953488374 \t Current Top20 Error:  0.86\n",
      "Done Validation Batch: 350 \n",
      "Average Single Error Per Batch: 0.9776353276353292 \t Current Single Error:  0.97 \n",
      "Average Top5 Error Per Batch: 0.9288034188034184 \t Current Top5 Error:  0.94 \n",
      "Average Top10 Error Per Batch: 0.8900569800569808 \t Current Top10 Error:  0.89 \n",
      "Average Top20 Error Per Batch: 0.836609686609687 \t Current Top20 Error:  0.87\n",
      "Done Validation Batch: 400 \n",
      "Average Single Error Per Batch: 0.977406483790526 \t Current Single Error:  0.96 \n",
      "Average Top5 Error Per Batch: 0.9287780548628431 \t Current Top5 Error:  0.96 \n",
      "Average Top10 Error Per Batch: 0.8900997506234423 \t Current Top10 Error:  0.92 \n",
      "Average Top20 Error Per Batch: 0.8370074812967584 \t Current Top20 Error:  0.88\n",
      "Done Validation Batch: 450 \n",
      "Average Single Error Per Batch: 0.9772727272727305 \t Current Single Error:  0.99 \n",
      "Average Top5 Error Per Batch: 0.9285587583148567 \t Current Top5 Error:  0.97 \n",
      "Average Top10 Error Per Batch: 0.8900886917960094 \t Current Top10 Error:  0.95 \n",
      "Average Top20 Error Per Batch: 0.8371618625277158 \t Current Top20 Error:  0.88\n",
      "Done Validation Batch: 500 \n",
      "Average Single Error Per Batch: 0.9772654690618804 \t Current Single Error:  0.99 \n",
      "Average Top5 Error Per Batch: 0.9287225548902207 \t Current Top5 Error:  0.95 \n",
      "Average Top10 Error Per Batch: 0.8905588822355298 \t Current Top10 Error:  0.94 \n",
      "Average Top20 Error Per Batch: 0.8373253493013971 \t Current Top20 Error:  0.85\n",
      "Done Validation Batch: 550 \n",
      "Average Single Error Per Batch: 0.9768058076225096 \t Current Single Error:  1.0 \n",
      "Average Top5 Error Per Batch: 0.9284936479128867 \t Current Top5 Error:  0.96 \n",
      "Average Top10 Error Per Batch: 0.8901088929219609 \t Current Top10 Error:  0.91 \n",
      "Average Top20 Error Per Batch: 0.8372958257713244 \t Current Top20 Error:  0.89\n",
      "Done Validation Batch: 600 \n",
      "Average Single Error Per Batch: 0.9766389351081594 \t Current Single Error:  1.0 \n",
      "Average Top5 Error Per Batch: 0.9282529118136451 \t Current Top5 Error:  0.95 \n",
      "Average Top10 Error Per Batch: 0.8897337770382697 \t Current Top10 Error:  0.9299999999999999 \n",
      "Average Top20 Error Per Batch: 0.836921797004991 \t Current Top20 Error:  0.83\n",
      "Done Validation Batch: 650 \n",
      "Average Single Error Per Batch: 0.9765745007680566 \t Current Single Error:  0.97 \n",
      "Average Top5 Error Per Batch: 0.9283870967741947 \t Current Top5 Error:  0.9 \n",
      "Average Top10 Error Per Batch: 0.89010752688172 \t Current Top10 Error:  0.88 \n",
      "Average Top20 Error Per Batch: 0.8371889400921655 \t Current Top20 Error:  0.81\n",
      "Done Validation Batch: 700 \n",
      "Average Single Error Per Batch: 0.9767760342368129 \t Current Single Error:  0.97 \n",
      "Average Top5 Error Per Batch: 0.9286875891583468 \t Current Top5 Error:  0.9299999999999999 \n",
      "Average Top10 Error Per Batch: 0.890142653352353 \t Current Top10 Error:  0.88 \n",
      "Average Top20 Error Per Batch: 0.8370185449358056 \t Current Top20 Error:  0.84\n",
      "Validation evaluation complete. \n",
      "Average Single Error Per Batch: 0.9767817679558098 \n",
      "Average Top5 Error Per Batch: 0.9287016574585646 \n",
      "Average Top10 Error Per Batch: 0.8899999999999989 \n",
      "Average Top20 Error Per Batch: 0.836878453038674\n"
     ]
    }
   ],
   "source": [
    "USE_CUDA = True\n",
    "\n",
    "checkpoint = torch.load(\"outputs/anp_log_softmax_classifier_batch_39\")\n",
    "main_model = ANPClassifier(len(train_anp_tags))\n",
    "main_model.load_state_dict(checkpoint['state_dict'])\n",
    "main_model.eval()\n",
    "\n",
    "def load_validation_batch(image_addresses, volatile=False):\n",
    "    \n",
    "    img_tensor = load_image(image_addresses[0])\n",
    "    for i in range(1, len(image_addresses)):\n",
    "        img_tensor = torch.cat((img_tensor, load_image(image_addresses[i])))\n",
    "        \n",
    "    target_tensor = torch.from_numpy(anp_tag_to_vector[validation_image_to_anp_tag[image_addresses[0]]]).unsqueeze(0)\n",
    "    for i in range(1, len(image_addresses)):\n",
    "        target_tensor = torch.cat((target_tensor, torch.from_numpy(anp_tag_to_vector[validation_image_to_anp_tag[image_addresses[i]]]).unsqueeze(0)))\n",
    "    \n",
    "    return img_tensor, target_tensor.float()\n",
    "\n",
    "def evaluate_validation_pass(model, image_batch, target_batch):\n",
    "    if USE_CUDA:\n",
    "        image_batch = image_batch.cuda()\n",
    "        target_batch = target_batch.cuda()\n",
    "        model = model.cuda()\n",
    "    \n",
    "    model_output = model(image_batch)\n",
    "    target_values, target_indices = torch.max(target_batch, 1)\n",
    "    output_values, out_indices = torch.max(model_output, 1)\n",
    "    accumulated_single_target_error = torch.sum(torch.eq(target_indices, out_indices)).data.cpu().numpy()\n",
    "    \n",
    "    count = 0\n",
    "    accumulated_top_5_error = 0\n",
    "    for output in model_output:\n",
    "        top5_predicted_labels, top5_predicted_indices = torch.topk(output, 5)\n",
    "        if target_indices[count] in top5_predicted_indices:\n",
    "            accumulated_top_5_error += 1\n",
    "        count += 1\n",
    "    \n",
    "    count = 0\n",
    "    accumulated_top_10_error = 0\n",
    "    for output in model_output:\n",
    "        top10_predicted_labels, top10_predicted_indices = torch.topk(output, 10)\n",
    "        if target_indices[count] in top10_predicted_indices:\n",
    "            accumulated_top_10_error += 1\n",
    "        count += 1\n",
    "        \n",
    "    count = 0\n",
    "    accumulated_top_20_error = 0\n",
    "    for output in model_output:\n",
    "        top20_predicted_labels, top20_predicted_indices = torch.topk(output, 20)\n",
    "        if target_indices[count] in top20_predicted_indices:\n",
    "            accumulated_top_20_error += 1\n",
    "        count += 1\n",
    "        \n",
    "    return accumulated_single_target_error, accumulated_top_5_error, accumulated_top_10_error, accumulated_top_20_error\n",
    "\n",
    "def evaluate_validation(model, batch_size=100):\n",
    "    number_of_batches = math.ceil(len(validation_image_addresses)/batch_size)\n",
    "    indexes = np.arange(len(validation_image_addresses))\n",
    "    avg_single_error_rate = 0\n",
    "    avg_top5_error_rate = 0\n",
    "    avg_top10_error_rate = 0\n",
    "    avg_top20_error_rate = 0\n",
    "    \n",
    "    for batch in range(number_of_batches):\n",
    "        validation_indexes = [validation_image_addresses[i] for i in indexes[batch*batch_size:(batch+1)*batch_size]]\n",
    "        image_batch, target_batch = load_validation_batch(validation_indexes)\n",
    "            \n",
    "        single_acc_error, top5_acc_error, top10_acc_error, top20_acc_error = evaluate_validation_pass(model, image_batch, target_batch)\n",
    "        single_error_rate = 1 - (single_acc_error / batch_size)\n",
    "        top5_error_rate = 1 - (top5_acc_error / batch_size)\n",
    "        top10_error_rate = 1 - (top10_acc_error / batch_size)\n",
    "        top20_error_rate = 1 - (top20_acc_error / batch_size)\n",
    "        \n",
    "        avg_single_error_rate += single_error_rate\n",
    "        avg_top5_error_rate += top5_error_rate\n",
    "        avg_top10_error_rate += top10_error_rate\n",
    "        avg_top20_error_rate += top20_error_rate\n",
    "        if batch%50 == 0:\n",
    "            print (\"Done Validation Batch:\", batch, \"\\nAverage Single Error Per Batch:\", avg_single_error_rate/(batch+1), \"\\t Current Single Error: \", \n",
    "                   single_error_rate, \"\\nAverage Top5 Error Per Batch:\", avg_top5_error_rate/(batch+1), \"\\t Current Top5 Error: \", top5_error_rate, \"\\nAverage Top10 Error Per Batch:\", avg_top10_error_rate/(batch+1), \"\\t Current Top10 Error: \", top10_error_rate\n",
    "                  , \"\\nAverage Top20 Error Per Batch:\", avg_top20_error_rate/(batch+1), \"\\t Current Top20 Error: \", top20_error_rate)\n",
    "    print (\"Validation evaluation complete.\", \"\\nAverage Single Error Per Batch:\", avg_single_error_rate/(batch+1), \"\\nAverage Top5 Error Per Batch:\", avg_top5_error_rate/(batch+1),\n",
    "            \"\\nAverage Top10 Error Per Batch:\", avg_top10_error_rate/(batch+1), \"\\nAverage Top20 Error Per Batch:\", avg_top20_error_rate/(batch+1))\n",
    "                \n",
    "evaluate_validation(main_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done Validation Batch: 0 \n",
      "Average Single Error Per Batch: 0.97 \t Current Single Error:  0.97 \n",
      "Average Top5 Error Per Batch: 0.94 \t Current Top5 Error:  0.94 \n",
      "Average Top10 Error Per Batch: 0.92 \t Current Top10 Error:  0.92 \n",
      "Average Top20 Error Per Batch: 0.9 \t Current Top20 Error:  0.9\n",
      "Done Validation Batch: 50 \n",
      "Average Single Error Per Batch: 0.9798039215686271 \t Current Single Error:  0.98 \n",
      "Average Top5 Error Per Batch: 0.9282352941176469 \t Current Top5 Error:  0.9299999999999999 \n",
      "Average Top10 Error Per Batch: 0.8923529411764707 \t Current Top10 Error:  0.92 \n",
      "Average Top20 Error Per Batch: 0.8427450980392156 \t Current Top20 Error:  0.86\n",
      "Done Validation Batch: 100 \n",
      "Average Single Error Per Batch: 0.9803960396039596 \t Current Single Error:  0.98 \n",
      "Average Top5 Error Per Batch: 0.9279207920792082 \t Current Top5 Error:  0.94 \n",
      "Average Top10 Error Per Batch: 0.8907920792079214 \t Current Top10 Error:  0.9 \n",
      "Average Top20 Error Per Batch: 0.8372277227722774 \t Current Top20 Error:  0.85\n",
      "Done Validation Batch: 150 \n",
      "Average Single Error Per Batch: 0.9789403973509929 \t Current Single Error:  0.96 \n",
      "Average Top5 Error Per Batch: 0.9293377483443713 \t Current Top5 Error:  0.91 \n",
      "Average Top10 Error Per Batch: 0.8907284768211924 \t Current Top10 Error:  0.81 \n",
      "Average Top20 Error Per Batch: 0.8374834437086095 \t Current Top20 Error:  0.79\n",
      "Done Validation Batch: 200 \n",
      "Average Single Error Per Batch: 0.9780597014925375 \t Current Single Error:  0.98 \n",
      "Average Top5 Error Per Batch: 0.9292039800995024 \t Current Top5 Error:  0.9299999999999999 \n",
      "Average Top10 Error Per Batch: 0.890398009950249 \t Current Top10 Error:  0.89 \n",
      "Average Top20 Error Per Batch: 0.836467661691543 \t Current Top20 Error:  0.81\n",
      "Done Validation Batch: 250 \n",
      "Average Single Error Per Batch: 0.9781673306772904 \t Current Single Error:  0.99 \n",
      "Average Top5 Error Per Batch: 0.9294820717131466 \t Current Top5 Error:  0.97 \n",
      "Average Top10 Error Per Batch: 0.8891633466135459 \t Current Top10 Error:  0.95 \n",
      "Average Top20 Error Per Batch: 0.8346613545816738 \t Current Top20 Error:  0.89\n",
      "Done Validation Batch: 300 \n",
      "Average Single Error Per Batch: 0.9783056478405328 \t Current Single Error:  0.99 \n",
      "Average Top5 Error Per Batch: 0.9298006644518261 \t Current Top5 Error:  0.9299999999999999 \n",
      "Average Top10 Error Per Batch: 0.8898671096345512 \t Current Top10 Error:  0.91 \n",
      "Average Top20 Error Per Batch: 0.8355149501661137 \t Current Top20 Error:  0.8200000000000001\n",
      "Done Validation Batch: 350 \n",
      "Average Single Error Per Batch: 0.9782051282051307 \t Current Single Error:  0.98 \n",
      "Average Top5 Error Per Batch: 0.9296866096866087 \t Current Top5 Error:  0.91 \n",
      "Average Top10 Error Per Batch: 0.8896011396011393 \t Current Top10 Error:  0.89 \n",
      "Average Top20 Error Per Batch: 0.8358689458689461 \t Current Top20 Error:  0.85\n",
      "Done Validation Batch: 400 \n",
      "Average Single Error Per Batch: 0.9777556109725717 \t Current Single Error:  0.99 \n",
      "Average Top5 Error Per Batch: 0.9290523690773062 \t Current Top5 Error:  0.9299999999999999 \n",
      "Average Top10 Error Per Batch: 0.8892269326683293 \t Current Top10 Error:  0.9 \n",
      "Average Top20 Error Per Batch: 0.8347630922693267 \t Current Top20 Error:  0.85\n",
      "Done Validation Batch: 450 \n",
      "Average Single Error Per Batch: 0.9778048780487841 \t Current Single Error:  0.99 \n",
      "Average Top5 Error Per Batch: 0.9290687361419064 \t Current Top5 Error:  0.94 \n",
      "Average Top10 Error Per Batch: 0.8890909090909094 \t Current Top10 Error:  0.9 \n",
      "Average Top20 Error Per Batch: 0.8346341463414632 \t Current Top20 Error:  0.85\n",
      "Done Validation Batch: 500 \n",
      "Average Single Error Per Batch: 0.9778243512974097 \t Current Single Error:  0.97 \n",
      "Average Top5 Error Per Batch: 0.9290419161676643 \t Current Top5 Error:  0.9299999999999999 \n",
      "Average Top10 Error Per Batch: 0.8887624750498999 \t Current Top10 Error:  0.9 \n",
      "Average Top20 Error Per Batch: 0.8341716566866259 \t Current Top20 Error:  0.83\n",
      "Done Validation Batch: 550 \n",
      "Average Single Error Per Batch: 0.9774591651542704 \t Current Single Error:  0.97 \n",
      "Average Top5 Error Per Batch: 0.9289836660617059 \t Current Top5 Error:  0.94 \n",
      "Average Top10 Error Per Batch: 0.8887295825771325 \t Current Top10 Error:  0.9299999999999999 \n",
      "Average Top20 Error Per Batch: 0.8343012704174219 \t Current Top20 Error:  0.86\n",
      "Done Validation Batch: 600 \n",
      "Average Single Error Per Batch: 0.9772379367720535 \t Current Single Error:  0.99 \n",
      "Average Top5 Error Per Batch: 0.9289184692179694 \t Current Top5 Error:  0.95 \n",
      "Average Top10 Error Per Batch: 0.8886855241264555 \t Current Top10 Error:  0.92 \n",
      "Average Top20 Error Per Batch: 0.8339600665557393 \t Current Top20 Error:  0.8200000000000001\n",
      "Done Validation Batch: 650 \n",
      "Average Single Error Per Batch: 0.9771274961597622 \t Current Single Error:  0.96 \n",
      "Average Top5 Error Per Batch: 0.9290937019969276 \t Current Top5 Error:  0.9 \n",
      "Average Top10 Error Per Batch: 0.8887403993855594 \t Current Top10 Error:  0.88 \n",
      "Average Top20 Error Per Batch: 0.8337941628264198 \t Current Top20 Error:  0.83\n",
      "Done Validation Batch: 700 \n",
      "Average Single Error Per Batch: 0.9769757489301086 \t Current Single Error:  0.98 \n",
      "Average Top5 Error Per Batch: 0.9288587731811694 \t Current Top5 Error:  0.9299999999999999 \n",
      "Average Top10 Error Per Batch: 0.8887303851640499 \t Current Top10 Error:  0.9 \n",
      "Average Top20 Error Per Batch: 0.8340941512125527 \t Current Top20 Error:  0.86\n",
      "Validation evaluation complete. \n",
      "Average Single Error Per Batch: 0.9770027624309484 \n",
      "Average Top5 Error Per Batch: 0.928591160220994 \n",
      "Average Top10 Error Per Batch: 0.8885773480662972 \n",
      "Average Top20 Error Per Batch: 0.8339502762430929\n"
     ]
    }
   ],
   "source": [
    "USE_CUDA = True\n",
    "\n",
    "checkpoint = torch.load(\"outputs/anp_log_softmax_classifier_batch_37\")\n",
    "main_model = ANPClassifier(len(train_anp_tags))\n",
    "main_model.load_state_dict(checkpoint['state_dict'])\n",
    "main_model.eval()\n",
    "\n",
    "evaluate_validation(main_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done Validation Batch: 0 \n",
      "Average Single Error Per Batch: 0.96 \t Current Single Error:  0.96 \n",
      "Average Top5 Error Per Batch: 0.91 \t Current Top5 Error:  0.91 \n",
      "Average Top10 Error Per Batch: 0.86 \t Current Top10 Error:  0.86 \n",
      "Average Top20 Error Per Batch: 0.83 \t Current Top20 Error:  0.83\n",
      "Done Validation Batch: 50 \n",
      "Average Single Error Per Batch: 0.9801960784313725 \t Current Single Error:  0.95 \n",
      "Average Top5 Error Per Batch: 0.9323529411764703 \t Current Top5 Error:  0.9299999999999999 \n",
      "Average Top10 Error Per Batch: 0.8937254901960784 \t Current Top10 Error:  0.89 \n",
      "Average Top20 Error Per Batch: 0.8388235294117642 \t Current Top20 Error:  0.83\n",
      "Done Validation Batch: 100 \n",
      "Average Single Error Per Batch: 0.9774257425742567 \t Current Single Error:  0.97 \n",
      "Average Top5 Error Per Batch: 0.9264356435643564 \t Current Top5 Error:  0.91 \n",
      "Average Top10 Error Per Batch: 0.8863366336633666 \t Current Top10 Error:  0.87 \n",
      "Average Top20 Error Per Batch: 0.8334653465346532 \t Current Top20 Error:  0.8\n",
      "Done Validation Batch: 150 \n",
      "Average Single Error Per Batch: 0.9767549668874164 \t Current Single Error:  0.98 \n",
      "Average Top5 Error Per Batch: 0.9279470198675495 \t Current Top5 Error:  0.97 \n",
      "Average Top10 Error Per Batch: 0.8876158940397355 \t Current Top10 Error:  0.94 \n",
      "Average Top20 Error Per Batch: 0.8333112582781452 \t Current Top20 Error:  0.9\n",
      "Done Validation Batch: 200 \n",
      "Average Single Error Per Batch: 0.9759203980099497 \t Current Single Error:  0.99 \n",
      "Average Top5 Error Per Batch: 0.9271144278606955 \t Current Top5 Error:  0.89 \n",
      "Average Top10 Error Per Batch: 0.8877114427860694 \t Current Top10 Error:  0.85 \n",
      "Average Top20 Error Per Batch: 0.832537313432836 \t Current Top20 Error:  0.81\n",
      "Done Validation Batch: 250 \n",
      "Average Single Error Per Batch: 0.9754980079681269 \t Current Single Error:  0.97 \n",
      "Average Top5 Error Per Batch: 0.9267729083665323 \t Current Top5 Error:  0.95 \n",
      "Average Top10 Error Per Batch: 0.8874501992031865 \t Current Top10 Error:  0.88 \n",
      "Average Top20 Error Per Batch: 0.832470119521913 \t Current Top20 Error:  0.81\n",
      "Done Validation Batch: 300 \n",
      "Average Single Error Per Batch: 0.9759800664451838 \t Current Single Error:  1.0 \n",
      "Average Top5 Error Per Batch: 0.9266777408637864 \t Current Top5 Error:  0.95 \n",
      "Average Top10 Error Per Batch: 0.8871428571428563 \t Current Top10 Error:  0.9 \n",
      "Average Top20 Error Per Batch: 0.832126245847177 \t Current Top20 Error:  0.8200000000000001\n",
      "Done Validation Batch: 350 \n",
      "Average Single Error Per Batch: 0.9757549857549881 \t Current Single Error:  1.0 \n",
      "Average Top5 Error Per Batch: 0.9267521367521365 \t Current Top5 Error:  0.96 \n",
      "Average Top10 Error Per Batch: 0.8868091168091161 \t Current Top10 Error:  0.9 \n",
      "Average Top20 Error Per Batch: 0.8321937321937324 \t Current Top20 Error:  0.86\n",
      "Done Validation Batch: 400 \n",
      "Average Single Error Per Batch: 0.975037406483793 \t Current Single Error:  0.97 \n",
      "Average Top5 Error Per Batch: 0.9256109725685783 \t Current Top5 Error:  0.9299999999999999 \n",
      "Average Top10 Error Per Batch: 0.8858603491271814 \t Current Top10 Error:  0.91 \n",
      "Average Top20 Error Per Batch: 0.8314713216957608 \t Current Top20 Error:  0.87\n",
      "Done Validation Batch: 450 \n",
      "Average Single Error Per Batch: 0.9747450110864777 \t Current Single Error:  0.99 \n",
      "Average Top5 Error Per Batch: 0.9253658536585366 \t Current Top5 Error:  0.96 \n",
      "Average Top10 Error Per Batch: 0.8860310421286024 \t Current Top10 Error:  0.92 \n",
      "Average Top20 Error Per Batch: 0.8310421286031041 \t Current Top20 Error:  0.8200000000000001\n",
      "Done Validation Batch: 500 \n",
      "Average Single Error Per Batch: 0.9750898203592855 \t Current Single Error:  0.97 \n",
      "Average Top5 Error Per Batch: 0.9255688622754497 \t Current Top5 Error:  0.9299999999999999 \n",
      "Average Top10 Error Per Batch: 0.8861277445109775 \t Current Top10 Error:  0.87 \n",
      "Average Top20 Error Per Batch: 0.8311576846307387 \t Current Top20 Error:  0.8200000000000001\n",
      "Done Validation Batch: 550 \n",
      "Average Single Error Per Batch: 0.975027223230495 \t Current Single Error:  0.99 \n",
      "Average Top5 Error Per Batch: 0.9249364791288573 \t Current Top5 Error:  0.97 \n",
      "Average Top10 Error Per Batch: 0.8858620689655164 \t Current Top10 Error:  0.96 \n",
      "Average Top20 Error Per Batch: 0.8311615245009075 \t Current Top20 Error:  0.91\n",
      "Done Validation Batch: 600 \n",
      "Average Single Error Per Batch: 0.9752745424292908 \t Current Single Error:  0.98 \n",
      "Average Top5 Error Per Batch: 0.9250249584026617 \t Current Top5 Error:  0.95 \n",
      "Average Top10 Error Per Batch: 0.8857570715474197 \t Current Top10 Error:  0.88 \n",
      "Average Top20 Error Per Batch: 0.8309650582362728 \t Current Top20 Error:  0.8200000000000001\n",
      "Done Validation Batch: 650 \n",
      "Average Single Error Per Batch: 0.9753456221198231 \t Current Single Error:  0.96 \n",
      "Average Top5 Error Per Batch: 0.9250998463901681 \t Current Top5 Error:  0.9299999999999999 \n",
      "Average Top10 Error Per Batch: 0.8853456221198143 \t Current Top10 Error:  0.9 \n",
      "Average Top20 Error Per Batch: 0.8304761904761905 \t Current Top20 Error:  0.85\n",
      "Done Validation Batch: 700 \n",
      "Average Single Error Per Batch: 0.9754493580599226 \t Current Single Error:  0.97 \n",
      "Average Top5 Error Per Batch: 0.9253922967189723 \t Current Top5 Error:  0.94 \n",
      "Average Top10 Error Per Batch: 0.8857203994293846 \t Current Top10 Error:  0.91 \n",
      "Average Top20 Error Per Batch: 0.8304136947218265 \t Current Top20 Error:  0.87\n",
      "Validation evaluation complete. \n",
      "Average Single Error Per Batch: 0.9754558011049811 \n",
      "Average Top5 Error Per Batch: 0.9253453038674024 \n",
      "Average Top10 Error Per Batch: 0.8856767955801086 \n",
      "Average Top20 Error Per Batch: 0.8303176795580116\n"
     ]
    }
   ],
   "source": [
    "USE_CUDA = True\n",
    "\n",
    "checkpoint = torch.load(\"outputs/anp_log_softmax_classifier_batch_35\")\n",
    "main_model = ANPClassifier(len(train_anp_tags))\n",
    "main_model.load_state_dict(checkpoint['state_dict'])\n",
    "main_model.eval()\n",
    "\n",
    "evaluate_validation(main_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done Validation Batch: 0 \n",
      "Average Single Error Per Batch: 0.96 \t Current Single Error:  0.96 \n",
      "Average Top5 Error Per Batch: 0.89 \t Current Top5 Error:  0.89 \n",
      "Average Top10 Error Per Batch: 0.85 \t Current Top10 Error:  0.85 \n",
      "Average Top20 Error Per Batch: 0.8 \t Current Top20 Error:  0.8\n",
      "Done Validation Batch: 50 \n",
      "Average Single Error Per Batch: 0.980196078431372 \t Current Single Error:  0.99 \n",
      "Average Top5 Error Per Batch: 0.9321568627450981 \t Current Top5 Error:  0.92 \n",
      "Average Top10 Error Per Batch: 0.8919607843137257 \t Current Top10 Error:  0.88 \n",
      "Average Top20 Error Per Batch: 0.8349019607843137 \t Current Top20 Error:  0.8\n",
      "Done Validation Batch: 100 \n",
      "Average Single Error Per Batch: 0.9779207920792078 \t Current Single Error:  0.95 \n",
      "Average Top5 Error Per Batch: 0.9319801980198025 \t Current Top5 Error:  0.9299999999999999 \n",
      "Average Top10 Error Per Batch: 0.8950495049504954 \t Current Top10 Error:  0.89 \n",
      "Average Top20 Error Per Batch: 0.8378217821782177 \t Current Top20 Error:  0.86\n",
      "Done Validation Batch: 150 \n",
      "Average Single Error Per Batch: 0.9768874172185424 \t Current Single Error:  0.97 \n",
      "Average Top5 Error Per Batch: 0.9314569536423843 \t Current Top5 Error:  0.94 \n",
      "Average Top10 Error Per Batch: 0.8943046357615899 \t Current Top10 Error:  0.91 \n",
      "Average Top20 Error Per Batch: 0.8372847682119204 \t Current Top20 Error:  0.84\n",
      "Done Validation Batch: 200 \n",
      "Average Single Error Per Batch: 0.9765671641791035 \t Current Single Error:  1.0 \n",
      "Average Top5 Error Per Batch: 0.9315920398009945 \t Current Top5 Error:  0.96 \n",
      "Average Top10 Error Per Batch: 0.894228855721393 \t Current Top10 Error:  0.88 \n",
      "Average Top20 Error Per Batch: 0.8363184079601993 \t Current Top20 Error:  0.81\n",
      "Done Validation Batch: 250 \n",
      "Average Single Error Per Batch: 0.9774900398406361 \t Current Single Error:  1.0 \n",
      "Average Top5 Error Per Batch: 0.9316733067729073 \t Current Top5 Error:  0.95 \n",
      "Average Top10 Error Per Batch: 0.8943824701195217 \t Current Top10 Error:  0.88 \n",
      "Average Top20 Error Per Batch: 0.8362948207171317 \t Current Top20 Error:  0.84\n",
      "Done Validation Batch: 300 \n",
      "Average Single Error Per Batch: 0.9776079734219263 \t Current Single Error:  0.99 \n",
      "Average Top5 Error Per Batch: 0.931461794019933 \t Current Top5 Error:  0.9299999999999999 \n",
      "Average Top10 Error Per Batch: 0.8939202657807305 \t Current Top10 Error:  0.89 \n",
      "Average Top20 Error Per Batch: 0.8363455149501666 \t Current Top20 Error:  0.83\n",
      "Done Validation Batch: 350 \n",
      "Average Single Error Per Batch: 0.9771794871794875 \t Current Single Error:  0.98 \n",
      "Average Top5 Error Per Batch: 0.9317663817663815 \t Current Top5 Error:  0.94 \n",
      "Average Top10 Error Per Batch: 0.8948717948717945 \t Current Top10 Error:  0.89 \n",
      "Average Top20 Error Per Batch: 0.8374643874643878 \t Current Top20 Error:  0.84\n",
      "Done Validation Batch: 400 \n",
      "Average Single Error Per Batch: 0.9772568578553631 \t Current Single Error:  0.99 \n",
      "Average Top5 Error Per Batch: 0.9314463840399002 \t Current Top5 Error:  0.92 \n",
      "Average Top10 Error Per Batch: 0.8945137157107226 \t Current Top10 Error:  0.9 \n",
      "Average Top20 Error Per Batch: 0.8365835411471321 \t Current Top20 Error:  0.83\n",
      "Done Validation Batch: 450 \n",
      "Average Single Error Per Batch: 0.977250554323728 \t Current Single Error:  1.0 \n",
      "Average Top5 Error Per Batch: 0.9315077605321513 \t Current Top5 Error:  0.94 \n",
      "Average Top10 Error Per Batch: 0.8944789356984473 \t Current Top10 Error:  0.9299999999999999 \n",
      "Average Top20 Error Per Batch: 0.8372727272727272 \t Current Top20 Error:  0.91\n",
      "Done Validation Batch: 500 \n",
      "Average Single Error Per Batch: 0.9772455089820397 \t Current Single Error:  0.99 \n",
      "Average Top5 Error Per Batch: 0.9312974051896213 \t Current Top5 Error:  0.92 \n",
      "Average Top10 Error Per Batch: 0.8942115768463067 \t Current Top10 Error:  0.89 \n",
      "Average Top20 Error Per Batch: 0.8375648702594809 \t Current Top20 Error:  0.8200000000000001\n",
      "Done Validation Batch: 550 \n",
      "Average Single Error Per Batch: 0.9771324863883897 \t Current Single Error:  1.0 \n",
      "Average Top5 Error Per Batch: 0.9313793103448283 \t Current Top5 Error:  0.95 \n",
      "Average Top10 Error Per Batch: 0.8940471869328486 \t Current Top10 Error:  0.94 \n",
      "Average Top20 Error Per Batch: 0.8373502722323047 \t Current Top20 Error:  0.9\n",
      "Done Validation Batch: 600 \n",
      "Average Single Error Per Batch: 0.9768552412645656 \t Current Single Error:  0.98 \n",
      "Average Top5 Error Per Batch: 0.9309151414309479 \t Current Top5 Error:  0.95 \n",
      "Average Top10 Error Per Batch: 0.8934608985024948 \t Current Top10 Error:  0.92 \n",
      "Average Top20 Error Per Batch: 0.8367886855241266 \t Current Top20 Error:  0.85\n",
      "Done Validation Batch: 650 \n",
      "Average Single Error Per Batch: 0.9768356374808064 \t Current Single Error:  0.99 \n",
      "Average Top5 Error Per Batch: 0.9307373271889392 \t Current Top5 Error:  0.91 \n",
      "Average Top10 Error Per Batch: 0.8933026113671262 \t Current Top10 Error:  0.83 \n",
      "Average Top20 Error Per Batch: 0.836866359447005 \t Current Top20 Error:  0.77\n",
      "Done Validation Batch: 700 \n",
      "Average Single Error Per Batch: 0.9769472182596376 \t Current Single Error:  0.99 \n",
      "Average Top5 Error Per Batch: 0.9309557774607695 \t Current Top5 Error:  0.92 \n",
      "Average Top10 Error Per Batch: 0.8934807417974303 \t Current Top10 Error:  0.9 \n",
      "Average Top20 Error Per Batch: 0.8373609129814558 \t Current Top20 Error:  0.88\n",
      "Validation evaluation complete. \n",
      "Average Single Error Per Batch: 0.9769889502762519 \n",
      "Average Top5 Error Per Batch: 0.9305801104972364 \n",
      "Average Top10 Error Per Batch: 0.8931491712707165 \n",
      "Average Top20 Error Per Batch: 0.8372375690607741\n"
     ]
    }
   ],
   "source": [
    "USE_CUDA = True\n",
    "\n",
    "checkpoint = torch.load(\"outputs/anp_log_softmax_classifier_batch_33\")\n",
    "main_model = ANPClassifier(len(train_anp_tags))\n",
    "main_model.load_state_dict(checkpoint['state_dict'])\n",
    "main_model.eval()\n",
    "\n",
    "evaluate_validation(main_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done Validation Batch: 0 \n",
      "Average Single Error Per Batch: 0.94 \t Current Single Error:  0.94 \n",
      "Average Top5 Error Per Batch: 0.92 \t Current Top5 Error:  0.92 \n",
      "Average Top10 Error Per Batch: 0.89 \t Current Top10 Error:  0.89 \n",
      "Average Top20 Error Per Batch: 0.83 \t Current Top20 Error:  0.83\n",
      "Done Validation Batch: 50 \n",
      "Average Single Error Per Batch: 0.9774509803921565 \t Current Single Error:  0.98 \n",
      "Average Top5 Error Per Batch: 0.9274509803921568 \t Current Top5 Error:  0.94 \n",
      "Average Top10 Error Per Batch: 0.890588235294118 \t Current Top10 Error:  0.9299999999999999 \n",
      "Average Top20 Error Per Batch: 0.8337254901960783 \t Current Top20 Error:  0.88\n",
      "Done Validation Batch: 100 \n",
      "Average Single Error Per Batch: 0.9770297029702963 \t Current Single Error:  0.96 \n",
      "Average Top5 Error Per Batch: 0.9262376237623762 \t Current Top5 Error:  0.92 \n",
      "Average Top10 Error Per Batch: 0.8863366336633665 \t Current Top10 Error:  0.85 \n",
      "Average Top20 Error Per Batch: 0.8297029702970297 \t Current Top20 Error:  0.81\n",
      "Done Validation Batch: 150 \n",
      "Average Single Error Per Batch: 0.9766887417218538 \t Current Single Error:  0.99 \n",
      "Average Top5 Error Per Batch: 0.9274834437086094 \t Current Top5 Error:  0.94 \n",
      "Average Top10 Error Per Batch: 0.8876821192052985 \t Current Top10 Error:  0.91 \n",
      "Average Top20 Error Per Batch: 0.8313907284768209 \t Current Top20 Error:  0.85\n",
      "Done Validation Batch: 200 \n",
      "Average Single Error Per Batch: 0.976666666666666 \t Current Single Error:  0.98 \n",
      "Average Top5 Error Per Batch: 0.9263681592039799 \t Current Top5 Error:  0.91 \n",
      "Average Top10 Error Per Batch: 0.8865671641791047 \t Current Top10 Error:  0.88 \n",
      "Average Top20 Error Per Batch: 0.8296517412935326 \t Current Top20 Error:  0.83\n",
      "Done Validation Batch: 250 \n",
      "Average Single Error Per Batch: 0.9769721115537839 \t Current Single Error:  1.0 \n",
      "Average Top5 Error Per Batch: 0.925896414342629 \t Current Top5 Error:  0.94 \n",
      "Average Top10 Error Per Batch: 0.8869721115537849 \t Current Top10 Error:  0.89 \n",
      "Average Top20 Error Per Batch: 0.8295617529880487 \t Current Top20 Error:  0.86\n",
      "Done Validation Batch: 300 \n",
      "Average Single Error Per Batch: 0.9769435215946848 \t Current Single Error:  0.99 \n",
      "Average Top5 Error Per Batch: 0.9268770764119596 \t Current Top5 Error:  0.97 \n",
      "Average Top10 Error Per Batch: 0.8878405315614615 \t Current Top10 Error:  0.92 \n",
      "Average Top20 Error Per Batch: 0.8314285714285728 \t Current Top20 Error:  0.9\n",
      "Done Validation Batch: 350 \n",
      "Average Single Error Per Batch: 0.9771509971509986 \t Current Single Error:  0.98 \n",
      "Average Top5 Error Per Batch: 0.9276353276353269 \t Current Top5 Error:  0.9299999999999999 \n",
      "Average Top10 Error Per Batch: 0.888290598290598 \t Current Top10 Error:  0.89 \n",
      "Average Top20 Error Per Batch: 0.8317663817663828 \t Current Top20 Error:  0.84\n",
      "Done Validation Batch: 400 \n",
      "Average Single Error Per Batch: 0.9771321695760622 \t Current Single Error:  0.96 \n",
      "Average Top5 Error Per Batch: 0.92788029925187 \t Current Top5 Error:  0.92 \n",
      "Average Top10 Error Per Batch: 0.888428927680798 \t Current Top10 Error:  0.89 \n",
      "Average Top20 Error Per Batch: 0.8317955112219462 \t Current Top20 Error:  0.83\n",
      "Done Validation Batch: 450 \n",
      "Average Single Error Per Batch: 0.9768292682926861 \t Current Single Error:  0.98 \n",
      "Average Top5 Error Per Batch: 0.9272062084257203 \t Current Top5 Error:  0.94 \n",
      "Average Top10 Error Per Batch: 0.8882705099778273 \t Current Top10 Error:  0.92 \n",
      "Average Top20 Error Per Batch: 0.8317073170731718 \t Current Top20 Error:  0.79\n",
      "Done Validation Batch: 500 \n",
      "Average Single Error Per Batch: 0.9769261477045946 \t Current Single Error:  0.98 \n",
      "Average Top5 Error Per Batch: 0.9282634730538923 \t Current Top5 Error:  0.94 \n",
      "Average Top10 Error Per Batch: 0.889321357285429 \t Current Top10 Error:  0.87 \n",
      "Average Top20 Error Per Batch: 0.8324750499002004 \t Current Top20 Error:  0.79\n",
      "Done Validation Batch: 550 \n",
      "Average Single Error Per Batch: 0.9766969147005492 \t Current Single Error:  0.99 \n",
      "Average Top5 Error Per Batch: 0.927985480943739 \t Current Top5 Error:  0.98 \n",
      "Average Top10 Error Per Batch: 0.8889655172413793 \t Current Top10 Error:  0.95 \n",
      "Average Top20 Error Per Batch: 0.8322141560798556 \t Current Top20 Error:  0.91\n",
      "Done Validation Batch: 600 \n",
      "Average Single Error Per Batch: 0.9767886855241324 \t Current Single Error:  0.98 \n",
      "Average Top5 Error Per Batch: 0.9280865224625631 \t Current Top5 Error:  0.95 \n",
      "Average Top10 Error Per Batch: 0.889151414309484 \t Current Top10 Error:  0.89 \n",
      "Average Top20 Error Per Batch: 0.8324958402662234 \t Current Top20 Error:  0.86\n",
      "Done Validation Batch: 650 \n",
      "Average Single Error Per Batch: 0.9766973886328797 \t Current Single Error:  0.96 \n",
      "Average Top5 Error Per Batch: 0.9280952380952383 \t Current Top5 Error:  0.9299999999999999 \n",
      "Average Top10 Error Per Batch: 0.889185867895545 \t Current Top10 Error:  0.9 \n",
      "Average Top20 Error Per Batch: 0.8325806451612914 \t Current Top20 Error:  0.83\n",
      "Done Validation Batch: 700 \n",
      "Average Single Error Per Batch: 0.9765905848787529 \t Current Single Error:  0.96 \n",
      "Average Top5 Error Per Batch: 0.9276034236804563 \t Current Top5 Error:  0.9 \n",
      "Average Top10 Error Per Batch: 0.8890299572039932 \t Current Top10 Error:  0.86 \n",
      "Average Top20 Error Per Batch: 0.8324679029957222 \t Current Top20 Error:  0.8\n",
      "Validation evaluation complete. \n",
      "Average Single Error Per Batch: 0.9766574585635446 \n",
      "Average Top5 Error Per Batch: 0.9276657458563533 \n",
      "Average Top10 Error Per Batch: 0.8890193370165734 \n",
      "Average Top20 Error Per Batch: 0.8323756906077366\n"
     ]
    }
   ],
   "source": [
    "USE_CUDA = True\n",
    "\n",
    "checkpoint = torch.load(\"outputs/anp_log_softmax_classifier_batch_31\")\n",
    "main_model = ANPClassifier(len(train_anp_tags))\n",
    "main_model.load_state_dict(checkpoint['state_dict'])\n",
    "main_model.eval()\n",
    "\n",
    "evaluate_validation(main_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done Validation Batch: 0 \n",
      "Average Single Error Per Batch: 0.95 \t Current Single Error:  0.95 \n",
      "Average Top5 Error Per Batch: 0.94 \t Current Top5 Error:  0.94 \n",
      "Average Top10 Error Per Batch: 0.88 \t Current Top10 Error:  0.88 \n",
      "Average Top20 Error Per Batch: 0.78 \t Current Top20 Error:  0.78\n",
      "Done Validation Batch: 50 \n",
      "Average Single Error Per Batch: 0.9790196078431368 \t Current Single Error:  0.96 \n",
      "Average Top5 Error Per Batch: 0.9376470588235294 \t Current Top5 Error:  0.92 \n",
      "Average Top10 Error Per Batch: 0.8982352941176472 \t Current Top10 Error:  0.92 \n",
      "Average Top20 Error Per Batch: 0.8409803921568627 \t Current Top20 Error:  0.87\n",
      "Done Validation Batch: 100 \n",
      "Average Single Error Per Batch: 0.9787128712871284 \t Current Single Error:  0.98 \n",
      "Average Top5 Error Per Batch: 0.9359405940594056 \t Current Top5 Error:  0.95 \n",
      "Average Top10 Error Per Batch: 0.8976237623762382 \t Current Top10 Error:  0.92 \n",
      "Average Top20 Error Per Batch: 0.8420792079207919 \t Current Top20 Error:  0.9\n",
      "Done Validation Batch: 150 \n",
      "Average Single Error Per Batch: 0.9782781456953639 \t Current Single Error:  1.0 \n",
      "Average Top5 Error Per Batch: 0.9354304635761583 \t Current Top5 Error:  0.95 \n",
      "Average Top10 Error Per Batch: 0.8968874172185435 \t Current Top10 Error:  0.91 \n",
      "Average Top20 Error Per Batch: 0.842119205298013 \t Current Top20 Error:  0.86\n",
      "Done Validation Batch: 200 \n",
      "Average Single Error Per Batch: 0.9774626865671638 \t Current Single Error:  0.98 \n",
      "Average Top5 Error Per Batch: 0.9339303482587054 \t Current Top5 Error:  0.9 \n",
      "Average Top10 Error Per Batch: 0.8946268656716418 \t Current Top10 Error:  0.87 \n",
      "Average Top20 Error Per Batch: 0.8392039800995028 \t Current Top20 Error:  0.84\n",
      "Done Validation Batch: 250 \n",
      "Average Single Error Per Batch: 0.9778486055776889 \t Current Single Error:  1.0 \n",
      "Average Top5 Error Per Batch: 0.9337848605577673 \t Current Top5 Error:  0.94 \n",
      "Average Top10 Error Per Batch: 0.8952589641434262 \t Current Top10 Error:  0.9299999999999999 \n",
      "Average Top20 Error Per Batch: 0.8401593625498014 \t Current Top20 Error:  0.9\n",
      "Done Validation Batch: 300 \n",
      "Average Single Error Per Batch: 0.9781063122923592 \t Current Single Error:  0.99 \n",
      "Average Top5 Error Per Batch: 0.934186046511626 \t Current Top5 Error:  0.97 \n",
      "Average Top10 Error Per Batch: 0.8951827242524915 \t Current Top10 Error:  0.9299999999999999 \n",
      "Average Top20 Error Per Batch: 0.8399335548172763 \t Current Top20 Error:  0.87\n",
      "Done Validation Batch: 350 \n",
      "Average Single Error Per Batch: 0.9782905982905997 \t Current Single Error:  0.99 \n",
      "Average Top5 Error Per Batch: 0.9337606837606823 \t Current Top5 Error:  0.95 \n",
      "Average Top10 Error Per Batch: 0.8950142450142446 \t Current Top10 Error:  0.89 \n",
      "Average Top20 Error Per Batch: 0.8399430199430205 \t Current Top20 Error:  0.84\n",
      "Done Validation Batch: 400 \n",
      "Average Single Error Per Batch: 0.9785286783042421 \t Current Single Error:  0.99 \n",
      "Average Top5 Error Per Batch: 0.9333665835411463 \t Current Top5 Error:  0.97 \n",
      "Average Top10 Error Per Batch: 0.8947880299251875 \t Current Top10 Error:  0.94 \n",
      "Average Top20 Error Per Batch: 0.8392768079800497 \t Current Top20 Error:  0.9\n",
      "Done Validation Batch: 450 \n",
      "Average Single Error Per Batch: 0.9781596452328196 \t Current Single Error:  0.97 \n",
      "Average Top5 Error Per Batch: 0.9332815964523277 \t Current Top5 Error:  0.92 \n",
      "Average Top10 Error Per Batch: 0.8951884700665199 \t Current Top10 Error:  0.91 \n",
      "Average Top20 Error Per Batch: 0.839911308203991 \t Current Top20 Error:  0.89\n",
      "Done Validation Batch: 500 \n",
      "Average Single Error Per Batch: 0.9780638722554934 \t Current Single Error:  0.97 \n",
      "Average Top5 Error Per Batch: 0.9332335329341317 \t Current Top5 Error:  0.92 \n",
      "Average Top10 Error Per Batch: 0.8951097804391227 \t Current Top10 Error:  0.86 \n",
      "Average Top20 Error Per Batch: 0.8395409181636727 \t Current Top20 Error:  0.8200000000000001\n",
      "Done Validation Batch: 550 \n",
      "Average Single Error Per Batch: 0.9778221415608043 \t Current Single Error:  0.98 \n",
      "Average Top5 Error Per Batch: 0.9327404718693288 \t Current Top5 Error:  0.94 \n",
      "Average Top10 Error Per Batch: 0.8945190562613442 \t Current Top10 Error:  0.9299999999999999 \n",
      "Average Top20 Error Per Batch: 0.8392196007259526 \t Current Top20 Error:  0.84\n",
      "Done Validation Batch: 600 \n",
      "Average Single Error Per Batch: 0.9778702163061633 \t Current Single Error:  0.99 \n",
      "Average Top5 Error Per Batch: 0.9326289517470886 \t Current Top5 Error:  0.95 \n",
      "Average Top10 Error Per Batch: 0.8946589018302841 \t Current Top10 Error:  0.9299999999999999 \n",
      "Average Top20 Error Per Batch: 0.8394841930116468 \t Current Top20 Error:  0.88\n",
      "Done Validation Batch: 650 \n",
      "Average Single Error Per Batch: 0.9777726574500848 \t Current Single Error:  0.96 \n",
      "Average Top5 Error Per Batch: 0.9327649769585262 \t Current Top5 Error:  0.89 \n",
      "Average Top10 Error Per Batch: 0.8949462365591405 \t Current Top10 Error:  0.87 \n",
      "Average Top20 Error Per Batch: 0.8399078341013825 \t Current Top20 Error:  0.83\n",
      "Done Validation Batch: 700 \n",
      "Average Single Error Per Batch: 0.9777888730385251 \t Current Single Error:  0.97 \n",
      "Average Top5 Error Per Batch: 0.9326818830242518 \t Current Top5 Error:  0.91 \n",
      "Average Top10 Error Per Batch: 0.8947931526390869 \t Current Top10 Error:  0.9 \n",
      "Average Top20 Error Per Batch: 0.8402710413694726 \t Current Top20 Error:  0.88\n",
      "Validation evaluation complete. \n",
      "Average Single Error Per Batch: 0.9778591160221084 \n",
      "Average Top5 Error Per Batch: 0.9325966850828734 \n",
      "Average Top10 Error Per Batch: 0.8946408839779001 \n",
      "Average Top20 Error Per Batch: 0.8399447513812158\n"
     ]
    }
   ],
   "source": [
    "USE_CUDA = True\n",
    "\n",
    "checkpoint = torch.load(\"outputs/anp_log_softmax_classifier_batch_29\")\n",
    "main_model = ANPClassifier(len(train_anp_tags))\n",
    "main_model.load_state_dict(checkpoint['state_dict'])\n",
    "main_model.eval()\n",
    "\n",
    "evaluate_validation(main_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done Validation Batch: 0 \n",
      "Average Single Error Per Batch: 0.97 \t Current Single Error:  0.97 \n",
      "Average Top5 Error Per Batch: 0.9299999999999999 \t Current Top5 Error:  0.9299999999999999 \n",
      "Average Top10 Error Per Batch: 0.89 \t Current Top10 Error:  0.89 \n",
      "Average Top20 Error Per Batch: 0.84 \t Current Top20 Error:  0.84\n",
      "Done Validation Batch: 50 \n",
      "Average Single Error Per Batch: 0.9782352941176471 \t Current Single Error:  0.97 \n",
      "Average Top5 Error Per Batch: 0.9325490196078433 \t Current Top5 Error:  0.92 \n",
      "Average Top10 Error Per Batch: 0.8941176470588237 \t Current Top10 Error:  0.92 \n",
      "Average Top20 Error Per Batch: 0.838627450980392 \t Current Top20 Error:  0.85\n",
      "Done Validation Batch: 100 \n",
      "Average Single Error Per Batch: 0.978316831683168 \t Current Single Error:  0.97 \n",
      "Average Top5 Error Per Batch: 0.9300990099009903 \t Current Top5 Error:  0.9299999999999999 \n",
      "Average Top10 Error Per Batch: 0.8920792079207923 \t Current Top10 Error:  0.9 \n",
      "Average Top20 Error Per Batch: 0.8353465346534651 \t Current Top20 Error:  0.87\n",
      "Done Validation Batch: 150 \n",
      "Average Single Error Per Batch: 0.9776821192052969 \t Current Single Error:  0.96 \n",
      "Average Top5 Error Per Batch: 0.930529801324503 \t Current Top5 Error:  0.92 \n",
      "Average Top10 Error Per Batch: 0.895562913907285 \t Current Top10 Error:  0.89 \n",
      "Average Top20 Error Per Batch: 0.8403311258278144 \t Current Top20 Error:  0.83\n",
      "Done Validation Batch: 200 \n",
      "Average Single Error Per Batch: 0.9772636815920387 \t Current Single Error:  0.99 \n",
      "Average Top5 Error Per Batch: 0.929253731343283 \t Current Top5 Error:  0.95 \n",
      "Average Top10 Error Per Batch: 0.8940796019900497 \t Current Top10 Error:  0.9299999999999999 \n",
      "Average Top20 Error Per Batch: 0.837960199004975 \t Current Top20 Error:  0.89\n",
      "Done Validation Batch: 250 \n",
      "Average Single Error Per Batch: 0.9778087649402379 \t Current Single Error:  1.0 \n",
      "Average Top5 Error Per Batch: 0.9287649402390433 \t Current Top5 Error:  0.95 \n",
      "Average Top10 Error Per Batch: 0.8928286852589639 \t Current Top10 Error:  0.91 \n",
      "Average Top20 Error Per Batch: 0.838326693227092 \t Current Top20 Error:  0.84\n",
      "Done Validation Batch: 300 \n",
      "Average Single Error Per Batch: 0.9774418604651168 \t Current Single Error:  0.99 \n",
      "Average Top5 Error Per Batch: 0.9287043189368762 \t Current Top5 Error:  0.95 \n",
      "Average Top10 Error Per Batch: 0.8917275747508303 \t Current Top10 Error:  0.9 \n",
      "Average Top20 Error Per Batch: 0.8380066445182731 \t Current Top20 Error:  0.84\n",
      "Done Validation Batch: 350 \n",
      "Average Single Error Per Batch: 0.9777207977207997 \t Current Single Error:  0.97 \n",
      "Average Top5 Error Per Batch: 0.9290028490028487 \t Current Top5 Error:  0.9299999999999999 \n",
      "Average Top10 Error Per Batch: 0.8923076923076922 \t Current Top10 Error:  0.91 \n",
      "Average Top20 Error Per Batch: 0.8376638176638181 \t Current Top20 Error:  0.87\n",
      "Done Validation Batch: 400 \n",
      "Average Single Error Per Batch: 0.9778054862842919 \t Current Single Error:  0.98 \n",
      "Average Top5 Error Per Batch: 0.9287281795511225 \t Current Top5 Error:  0.97 \n",
      "Average Top10 Error Per Batch: 0.8920698254364092 \t Current Top10 Error:  0.91 \n",
      "Average Top20 Error Per Batch: 0.837880299251871 \t Current Top20 Error:  0.87\n",
      "Done Validation Batch: 450 \n",
      "Average Single Error Per Batch: 0.9777605321507792 \t Current Single Error:  1.0 \n",
      "Average Top5 Error Per Batch: 0.9293569844789357 \t Current Top5 Error:  0.95 \n",
      "Average Top10 Error Per Batch: 0.8926607538802667 \t Current Top10 Error:  0.89 \n",
      "Average Top20 Error Per Batch: 0.8384035476718416 \t Current Top20 Error:  0.85\n",
      "Done Validation Batch: 500 \n",
      "Average Single Error Per Batch: 0.9777844311377285 \t Current Single Error:  0.97 \n",
      "Average Top5 Error Per Batch: 0.9296606786427146 \t Current Top5 Error:  0.94 \n",
      "Average Top10 Error Per Batch: 0.8929341317365274 \t Current Top10 Error:  0.9 \n",
      "Average Top20 Error Per Batch: 0.8385429141716575 \t Current Top20 Error:  0.8200000000000001\n",
      "Done Validation Batch: 550 \n",
      "Average Single Error Per Batch: 0.9775499092559031 \t Current Single Error:  0.98 \n",
      "Average Top5 Error Per Batch: 0.9290744101633396 \t Current Top5 Error:  0.95 \n",
      "Average Top10 Error Per Batch: 0.8921778584392018 \t Current Top10 Error:  0.9 \n",
      "Average Top20 Error Per Batch: 0.8378039927404726 \t Current Top20 Error:  0.85\n",
      "Done Validation Batch: 600 \n",
      "Average Single Error Per Batch: 0.9776871880199727 \t Current Single Error:  0.95 \n",
      "Average Top5 Error Per Batch: 0.9293677204658903 \t Current Top5 Error:  0.92 \n",
      "Average Top10 Error Per Batch: 0.89171381031614 \t Current Top10 Error:  0.87 \n",
      "Average Top20 Error Per Batch: 0.8375374376039938 \t Current Top20 Error:  0.83\n",
      "Done Validation Batch: 650 \n",
      "Average Single Error Per Batch: 0.9775422427035402 \t Current Single Error:  0.97 \n",
      "Average Top5 Error Per Batch: 0.9293548387096773 \t Current Top5 Error:  0.89 \n",
      "Average Top10 Error Per Batch: 0.8913671274961599 \t Current Top10 Error:  0.85 \n",
      "Average Top20 Error Per Batch: 0.8369738863287253 \t Current Top20 Error:  0.81\n",
      "Done Validation Batch: 700 \n",
      "Average Single Error Per Batch: 0.9775320970042877 \t Current Single Error:  0.96 \n",
      "Average Top5 Error Per Batch: 0.9294721825962913 \t Current Top5 Error:  0.9299999999999999 \n",
      "Average Top10 Error Per Batch: 0.8913409415121247 \t Current Top10 Error:  0.9 \n",
      "Average Top20 Error Per Batch: 0.8371184022824542 \t Current Top20 Error:  0.8200000000000001\n",
      "Validation evaluation complete. \n",
      "Average Single Error Per Batch: 0.9774033149171357 \n",
      "Average Top5 Error Per Batch: 0.9293784530386741 \n",
      "Average Top10 Error Per Batch: 0.891243093922651 \n",
      "Average Top20 Error Per Batch: 0.8370027624309402\n"
     ]
    }
   ],
   "source": [
    "USE_CUDA = True\n",
    "\n",
    "checkpoint = torch.load(\"outputs/anp_log_softmax_classifier_batch_27\")\n",
    "main_model = ANPClassifier(len(train_anp_tags))\n",
    "main_model.load_state_dict(checkpoint['state_dict'])\n",
    "main_model.eval()\n",
    "\n",
    "evaluate_validation(main_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done Validation Batch: 0 \n",
      "Average Single Error Per Batch: 0.98 \t Current Single Error:  0.98 \n",
      "Average Top5 Error Per Batch: 0.92 \t Current Top5 Error:  0.92 \n",
      "Average Top10 Error Per Batch: 0.89 \t Current Top10 Error:  0.89 \n",
      "Average Top20 Error Per Batch: 0.85 \t Current Top20 Error:  0.85\n",
      "Done Validation Batch: 50 \n",
      "Average Single Error Per Batch: 0.9792156862745096 \t Current Single Error:  0.96 \n",
      "Average Top5 Error Per Batch: 0.9362745098039217 \t Current Top5 Error:  0.9299999999999999 \n",
      "Average Top10 Error Per Batch: 0.8982352941176468 \t Current Top10 Error:  0.89 \n",
      "Average Top20 Error Per Batch: 0.8482352941176472 \t Current Top20 Error:  0.84\n",
      "Done Validation Batch: 100 \n",
      "Average Single Error Per Batch: 0.979603960396039 \t Current Single Error:  0.99 \n",
      "Average Top5 Error Per Batch: 0.9373267326732676 \t Current Top5 Error:  0.96 \n",
      "Average Top10 Error Per Batch: 0.9000990099009903 \t Current Top10 Error:  0.92 \n",
      "Average Top20 Error Per Batch: 0.8491089108910888 \t Current Top20 Error:  0.86\n",
      "Done Validation Batch: 150 \n",
      "Average Single Error Per Batch: 0.9791390728476819 \t Current Single Error:  0.99 \n",
      "Average Top5 Error Per Batch: 0.9378807947019869 \t Current Top5 Error:  0.96 \n",
      "Average Top10 Error Per Batch: 0.901788079470199 \t Current Top10 Error:  0.9299999999999999 \n",
      "Average Top20 Error Per Batch: 0.8511258278145692 \t Current Top20 Error:  0.85\n",
      "Done Validation Batch: 200 \n",
      "Average Single Error Per Batch: 0.9785572139303474 \t Current Single Error:  0.99 \n",
      "Average Top5 Error Per Batch: 0.9360199004975114 \t Current Top5 Error:  0.95 \n",
      "Average Top10 Error Per Batch: 0.8999004975124374 \t Current Top10 Error:  0.91 \n",
      "Average Top20 Error Per Batch: 0.8489054726368166 \t Current Top20 Error:  0.86\n",
      "Done Validation Batch: 250 \n",
      "Average Single Error Per Batch: 0.9795219123505966 \t Current Single Error:  1.0 \n",
      "Average Top5 Error Per Batch: 0.9369322709163334 \t Current Top5 Error:  0.98 \n",
      "Average Top10 Error Per Batch: 0.8997609561752985 \t Current Top10 Error:  0.95 \n",
      "Average Top20 Error Per Batch: 0.8482470119521919 \t Current Top20 Error:  0.87\n",
      "Done Validation Batch: 300 \n",
      "Average Single Error Per Batch: 0.9798006644518279 \t Current Single Error:  1.0 \n",
      "Average Top5 Error Per Batch: 0.9366445182724247 \t Current Top5 Error:  0.97 \n",
      "Average Top10 Error Per Batch: 0.8992358803986704 \t Current Top10 Error:  0.95 \n",
      "Average Top20 Error Per Batch: 0.8465116279069775 \t Current Top20 Error:  0.86\n",
      "Done Validation Batch: 350 \n",
      "Average Single Error Per Batch: 0.9803133903133924 \t Current Single Error:  0.99 \n",
      "Average Top5 Error Per Batch: 0.9378347578347579 \t Current Top5 Error:  0.97 \n",
      "Average Top10 Error Per Batch: 0.90031339031339 \t Current Top10 Error:  0.9299999999999999 \n",
      "Average Top20 Error Per Batch: 0.8474928774928783 \t Current Top20 Error:  0.9\n",
      "Done Validation Batch: 400 \n",
      "Average Single Error Per Batch: 0.9805985037406513 \t Current Single Error:  0.98 \n",
      "Average Top5 Error Per Batch: 0.9375062344139654 \t Current Top5 Error:  0.96 \n",
      "Average Top10 Error Per Batch: 0.8999501246882794 \t Current Top10 Error:  0.92 \n",
      "Average Top20 Error Per Batch: 0.8477057356608487 \t Current Top20 Error:  0.86\n",
      "Done Validation Batch: 450 \n",
      "Average Single Error Per Batch: 0.9806651884700703 \t Current Single Error:  0.99 \n",
      "Average Top5 Error Per Batch: 0.9375388026607544 \t Current Top5 Error:  0.96 \n",
      "Average Top10 Error Per Batch: 0.90039911308204 \t Current Top10 Error:  0.91 \n",
      "Average Top20 Error Per Batch: 0.8480266075388039 \t Current Top20 Error:  0.86\n",
      "Done Validation Batch: 500 \n",
      "Average Single Error Per Batch: 0.9807185628742556 \t Current Single Error:  0.98 \n",
      "Average Top5 Error Per Batch: 0.9374451097804396 \t Current Top5 Error:  0.95 \n",
      "Average Top10 Error Per Batch: 0.9002195608782438 \t Current Top10 Error:  0.91 \n",
      "Average Top20 Error Per Batch: 0.8479041916167678 \t Current Top20 Error:  0.84\n",
      "Done Validation Batch: 550 \n",
      "Average Single Error Per Batch: 0.9804718693284986 \t Current Single Error:  1.0 \n",
      "Average Top5 Error Per Batch: 0.9371869328493654 \t Current Top5 Error:  0.91 \n",
      "Average Top10 Error Per Batch: 0.9000000000000002 \t Current Top10 Error:  0.85 \n",
      "Average Top20 Error Per Batch: 0.8478584392014528 \t Current Top20 Error:  0.84\n",
      "Done Validation Batch: 600 \n",
      "Average Single Error Per Batch: 0.9804326123128181 \t Current Single Error:  0.99 \n",
      "Average Top5 Error Per Batch: 0.9373211314475883 \t Current Top5 Error:  0.9299999999999999 \n",
      "Average Top10 Error Per Batch: 0.9003161397670546 \t Current Top10 Error:  0.9 \n",
      "Average Top20 Error Per Batch: 0.8482196339434283 \t Current Top20 Error:  0.8200000000000001\n",
      "Done Validation Batch: 650 \n",
      "Average Single Error Per Batch: 0.9803533026113742 \t Current Single Error:  0.96 \n",
      "Average Top5 Error Per Batch: 0.9373579109062996 \t Current Top5 Error:  0.91 \n",
      "Average Top10 Error Per Batch: 0.9002611367127487 \t Current Top10 Error:  0.91 \n",
      "Average Top20 Error Per Batch: 0.84831029185868 \t Current Top20 Error:  0.84\n",
      "Done Validation Batch: 700 \n",
      "Average Single Error Per Batch: 0.9803566333808923 \t Current Single Error:  0.97 \n",
      "Average Top5 Error Per Batch: 0.9374037089871629 \t Current Top5 Error:  0.92 \n",
      "Average Top10 Error Per Batch: 0.9004850213980011 \t Current Top10 Error:  0.91 \n",
      "Average Top20 Error Per Batch: 0.8487018544935819 \t Current Top20 Error:  0.89\n",
      "Validation evaluation complete. \n",
      "Average Single Error Per Batch: 0.9804419889502843 \n",
      "Average Top5 Error Per Batch: 0.9373342541436481 \n",
      "Average Top10 Error Per Batch: 0.9005939226519315 \n",
      "Average Top20 Error Per Batch: 0.8486740331491728\n"
     ]
    }
   ],
   "source": [
    "USE_CUDA = True\n",
    "\n",
    "checkpoint = torch.load(\"outputs/anp_log_softmax_classifier_batch_25\")\n",
    "main_model = ANPClassifier(len(train_anp_tags))\n",
    "main_model.load_state_dict(checkpoint['state_dict'])\n",
    "main_model.eval()\n",
    "\n",
    "evaluate_validation(main_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done Validation Batch: 0 \n",
      "Average Single Error Per Batch: 0.98 \t Current Single Error:  0.98 \n",
      "Average Top5 Error Per Batch: 0.9299999999999999 \t Current Top5 Error:  0.9299999999999999 \n",
      "Average Top10 Error Per Batch: 0.88 \t Current Top10 Error:  0.88 \n",
      "Average Top20 Error Per Batch: 0.85 \t Current Top20 Error:  0.85\n",
      "Done Validation Batch: 50 \n",
      "Average Single Error Per Batch: 0.980392156862745 \t Current Single Error:  0.98 \n",
      "Average Top5 Error Per Batch: 0.9374509803921572 \t Current Top5 Error:  0.96 \n",
      "Average Top10 Error Per Batch: 0.9035294117647058 \t Current Top10 Error:  0.9299999999999999 \n",
      "Average Top20 Error Per Batch: 0.8464705882352943 \t Current Top20 Error:  0.87\n",
      "Done Validation Batch: 100 \n",
      "Average Single Error Per Batch: 0.9810891089108905 \t Current Single Error:  1.0 \n",
      "Average Top5 Error Per Batch: 0.9358415841584159 \t Current Top5 Error:  0.97 \n",
      "Average Top10 Error Per Batch: 0.8996039603960394 \t Current Top10 Error:  0.94 \n",
      "Average Top20 Error Per Batch: 0.8423762376237619 \t Current Top20 Error:  0.88\n",
      "Done Validation Batch: 150 \n",
      "Average Single Error Per Batch: 0.9799999999999993 \t Current Single Error:  0.97 \n",
      "Average Top5 Error Per Batch: 0.9354304635761588 \t Current Top5 Error:  0.96 \n",
      "Average Top10 Error Per Batch: 0.9 \t Current Top10 Error:  0.94 \n",
      "Average Top20 Error Per Batch: 0.8451655629139068 \t Current Top20 Error:  0.9\n",
      "Done Validation Batch: 200 \n",
      "Average Single Error Per Batch: 0.9795024875621886 \t Current Single Error:  0.98 \n",
      "Average Top5 Error Per Batch: 0.9349253731343278 \t Current Top5 Error:  0.95 \n",
      "Average Top10 Error Per Batch: 0.8989054726368156 \t Current Top10 Error:  0.91 \n",
      "Average Top20 Error Per Batch: 0.8434825870646767 \t Current Top20 Error:  0.86\n",
      "Done Validation Batch: 250 \n",
      "Average Single Error Per Batch: 0.9796812749003981 \t Current Single Error:  1.0 \n",
      "Average Top5 Error Per Batch: 0.9345019920318717 \t Current Top5 Error:  0.97 \n",
      "Average Top10 Error Per Batch: 0.8980876494023897 \t Current Top10 Error:  0.92 \n",
      "Average Top20 Error Per Batch: 0.8438247011952195 \t Current Top20 Error:  0.87\n",
      "Done Validation Batch: 300 \n",
      "Average Single Error Per Batch: 0.9799003322259145 \t Current Single Error:  0.97 \n",
      "Average Top5 Error Per Batch: 0.9356810631229233 \t Current Top5 Error:  0.95 \n",
      "Average Top10 Error Per Batch: 0.8980066445182717 \t Current Top10 Error:  0.9299999999999999 \n",
      "Average Top20 Error Per Batch: 0.843986710963456 \t Current Top20 Error:  0.87\n",
      "Done Validation Batch: 350 \n",
      "Average Single Error Per Batch: 0.9797150997151023 \t Current Single Error:  0.98 \n",
      "Average Top5 Error Per Batch: 0.9361253561253567 \t Current Top5 Error:  0.94 \n",
      "Average Top10 Error Per Batch: 0.8986609686609676 \t Current Top10 Error:  0.91 \n",
      "Average Top20 Error Per Batch: 0.8449572649572655 \t Current Top20 Error:  0.89\n",
      "Done Validation Batch: 400 \n",
      "Average Single Error Per Batch: 0.9794763092269363 \t Current Single Error:  0.97 \n",
      "Average Top5 Error Per Batch: 0.9356359102244395 \t Current Top5 Error:  0.94 \n",
      "Average Top10 Error Per Batch: 0.8983541147132157 \t Current Top10 Error:  0.9 \n",
      "Average Top20 Error Per Batch: 0.8445137157107234 \t Current Top20 Error:  0.87\n",
      "Done Validation Batch: 450 \n",
      "Average Single Error Per Batch: 0.9794013303769442 \t Current Single Error:  0.99 \n",
      "Average Top5 Error Per Batch: 0.9356541019955661 \t Current Top5 Error:  0.94 \n",
      "Average Top10 Error Per Batch: 0.8985809312638571 \t Current Top10 Error:  0.88 \n",
      "Average Top20 Error Per Batch: 0.8450110864745013 \t Current Top20 Error:  0.85\n",
      "Done Validation Batch: 500 \n",
      "Average Single Error Per Batch: 0.9793812375249543 \t Current Single Error:  0.95 \n",
      "Average Top5 Error Per Batch: 0.9356287425149709 \t Current Top5 Error:  0.9 \n",
      "Average Top10 Error Per Batch: 0.8978842315369256 \t Current Top10 Error:  0.86 \n",
      "Average Top20 Error Per Batch: 0.8443912175648703 \t Current Top20 Error:  0.78\n",
      "Done Validation Batch: 550 \n",
      "Average Single Error Per Batch: 0.9795099818511844 \t Current Single Error:  0.97 \n",
      "Average Top5 Error Per Batch: 0.9356442831215981 \t Current Top5 Error:  0.95 \n",
      "Average Top10 Error Per Batch: 0.8977676950998181 \t Current Top10 Error:  0.92 \n",
      "Average Top20 Error Per Batch: 0.844174228675136 \t Current Top20 Error:  0.87\n",
      "Done Validation Batch: 600 \n",
      "Average Single Error Per Batch: 0.979584026622302 \t Current Single Error:  0.99 \n",
      "Average Top5 Error Per Batch: 0.9358069883527474 \t Current Top5 Error:  0.94 \n",
      "Average Top10 Error Per Batch: 0.8982029950083186 \t Current Top10 Error:  0.91 \n",
      "Average Top20 Error Per Batch: 0.8445923460898502 \t Current Top20 Error:  0.86\n",
      "Done Validation Batch: 650 \n",
      "Average Single Error Per Batch: 0.9793701996927874 \t Current Single Error:  0.96 \n",
      "Average Top5 Error Per Batch: 0.9357910906298023 \t Current Top5 Error:  0.9299999999999999 \n",
      "Average Top10 Error Per Batch: 0.8979416282642078 \t Current Top10 Error:  0.91 \n",
      "Average Top20 Error Per Batch: 0.84457757296467 \t Current Top20 Error:  0.83\n",
      "Done Validation Batch: 700 \n",
      "Average Single Error Per Batch: 0.9795292439372402 \t Current Single Error:  0.98 \n",
      "Average Top5 Error Per Batch: 0.935791726105566 \t Current Top5 Error:  0.94 \n",
      "Average Top10 Error Per Batch: 0.8982168330955754 \t Current Top10 Error:  0.9299999999999999 \n",
      "Average Top20 Error Per Batch: 0.8448074179743232 \t Current Top20 Error:  0.9\n",
      "Validation evaluation complete. \n",
      "Average Single Error Per Batch: 0.9795718232044278 \n",
      "Average Top5 Error Per Batch: 0.9357734806629856 \n",
      "Average Top10 Error Per Batch: 0.8982734806629807 \n",
      "Average Top20 Error Per Batch: 0.8447375690607741\n"
     ]
    }
   ],
   "source": [
    "USE_CUDA = True\n",
    "\n",
    "checkpoint = torch.load(\"outputs/anp_log_softmax_classifier_batch_23\")\n",
    "main_model = ANPClassifier(len(train_anp_tags))\n",
    "main_model.load_state_dict(checkpoint['state_dict'])\n",
    "main_model.eval()\n",
    "\n",
    "evaluate_validation(main_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done Validation Batch: 0 \n",
      "Average Single Error Per Batch: 0.97 \t Current Single Error:  0.97 \n",
      "Average Top5 Error Per Batch: 0.92 \t Current Top5 Error:  0.92 \n",
      "Average Top10 Error Per Batch: 0.89 \t Current Top10 Error:  0.89 \n",
      "Average Top20 Error Per Batch: 0.83 \t Current Top20 Error:  0.83\n",
      "Done Validation Batch: 50 \n",
      "Average Single Error Per Batch: 0.9760784313725489 \t Current Single Error:  0.98 \n",
      "Average Top5 Error Per Batch: 0.9284313725490196 \t Current Top5 Error:  0.9299999999999999 \n",
      "Average Top10 Error Per Batch: 0.8862745098039216 \t Current Top10 Error:  0.88 \n",
      "Average Top20 Error Per Batch: 0.8288235294117646 \t Current Top20 Error:  0.84\n",
      "Done Validation Batch: 100 \n",
      "Average Single Error Per Batch: 0.9778217821782174 \t Current Single Error:  0.99 \n",
      "Average Top5 Error Per Batch: 0.9305940594059406 \t Current Top5 Error:  0.9299999999999999 \n",
      "Average Top10 Error Per Batch: 0.8905940594059406 \t Current Top10 Error:  0.91 \n",
      "Average Top20 Error Per Batch: 0.8315841584158417 \t Current Top20 Error:  0.86\n",
      "Done Validation Batch: 150 \n",
      "Average Single Error Per Batch: 0.9782119205298007 \t Current Single Error:  0.97 \n",
      "Average Top5 Error Per Batch: 0.9306622516556291 \t Current Top5 Error:  0.95 \n",
      "Average Top10 Error Per Batch: 0.89158940397351 \t Current Top10 Error:  0.9 \n",
      "Average Top20 Error Per Batch: 0.8353642384105958 \t Current Top20 Error:  0.85\n",
      "Done Validation Batch: 200 \n",
      "Average Single Error Per Batch: 0.9773631840796013 \t Current Single Error:  1.0 \n",
      "Average Top5 Error Per Batch: 0.9294527363184067 \t Current Top5 Error:  0.94 \n",
      "Average Top10 Error Per Batch: 0.8894527363184078 \t Current Top10 Error:  0.92 \n",
      "Average Top20 Error Per Batch: 0.8334328358208957 \t Current Top20 Error:  0.89\n",
      "Done Validation Batch: 250 \n",
      "Average Single Error Per Batch: 0.977250996015936 \t Current Single Error:  0.99 \n",
      "Average Top5 Error Per Batch: 0.9289641434262934 \t Current Top5 Error:  0.94 \n",
      "Average Top10 Error Per Batch: 0.8886454183266931 \t Current Top10 Error:  0.9299999999999999 \n",
      "Average Top20 Error Per Batch: 0.8333864541832671 \t Current Top20 Error:  0.87\n",
      "Done Validation Batch: 300 \n",
      "Average Single Error Per Batch: 0.977508305647842 \t Current Single Error:  0.98 \n",
      "Average Top5 Error Per Batch: 0.9290365448504974 \t Current Top5 Error:  0.94 \n",
      "Average Top10 Error Per Batch: 0.8892026578073086 \t Current Top10 Error:  0.91 \n",
      "Average Top20 Error Per Batch: 0.8335548172757479 \t Current Top20 Error:  0.85\n",
      "Done Validation Batch: 350 \n",
      "Average Single Error Per Batch: 0.9776638176638202 \t Current Single Error:  0.99 \n",
      "Average Top5 Error Per Batch: 0.93031339031339 \t Current Top5 Error:  0.94 \n",
      "Average Top10 Error Per Batch: 0.8902564102564107 \t Current Top10 Error:  0.92 \n",
      "Average Top20 Error Per Batch: 0.8335042735042735 \t Current Top20 Error:  0.87\n",
      "Done Validation Batch: 400 \n",
      "Average Single Error Per Batch: 0.9777556109725724 \t Current Single Error:  0.95 \n",
      "Average Top5 Error Per Batch: 0.9303740648379055 \t Current Top5 Error:  0.91 \n",
      "Average Top10 Error Per Batch: 0.8898254364089775 \t Current Top10 Error:  0.91 \n",
      "Average Top20 Error Per Batch: 0.8336907730673315 \t Current Top20 Error:  0.86\n",
      "Done Validation Batch: 450 \n",
      "Average Single Error Per Batch: 0.9777161862527758 \t Current Single Error:  0.99 \n",
      "Average Top5 Error Per Batch: 0.9304212860310427 \t Current Top5 Error:  0.95 \n",
      "Average Top10 Error Per Batch: 0.8901773835920179 \t Current Top10 Error:  0.91 \n",
      "Average Top20 Error Per Batch: 0.8345676274944561 \t Current Top20 Error:  0.88\n",
      "Done Validation Batch: 500 \n",
      "Average Single Error Per Batch: 0.9777245508982083 \t Current Single Error:  0.94 \n",
      "Average Top5 Error Per Batch: 0.930279441117765 \t Current Top5 Error:  0.9 \n",
      "Average Top10 Error Per Batch: 0.8903393213572857 \t Current Top10 Error:  0.85 \n",
      "Average Top20 Error Per Batch: 0.8345109780439113 \t Current Top20 Error:  0.8200000000000001\n",
      "Done Validation Batch: 550 \n",
      "Average Single Error Per Batch: 0.9777858439201508 \t Current Single Error:  0.97 \n",
      "Average Top5 Error Per Batch: 0.9300181488203267 \t Current Top5 Error:  0.88 \n",
      "Average Top10 Error Per Batch: 0.8901996370235938 \t Current Top10 Error:  0.86 \n",
      "Average Top20 Error Per Batch: 0.833974591651542 \t Current Top20 Error:  0.8\n",
      "Done Validation Batch: 600 \n",
      "Average Single Error Per Batch: 0.9780033277870283 \t Current Single Error:  0.97 \n",
      "Average Top5 Error Per Batch: 0.9301497504159734 \t Current Top5 Error:  0.9299999999999999 \n",
      "Average Top10 Error Per Batch: 0.8904991680532445 \t Current Top10 Error:  0.92 \n",
      "Average Top20 Error Per Batch: 0.8343594009983354 \t Current Top20 Error:  0.85\n",
      "Done Validation Batch: 650 \n",
      "Average Single Error Per Batch: 0.9777726574500846 \t Current Single Error:  0.96 \n",
      "Average Top5 Error Per Batch: 0.93010752688172 \t Current Top5 Error:  0.9 \n",
      "Average Top10 Error Per Batch: 0.890645161290322 \t Current Top10 Error:  0.89 \n",
      "Average Top20 Error Per Batch: 0.8346082949308747 \t Current Top20 Error:  0.86\n",
      "Done Validation Batch: 700 \n",
      "Average Single Error Per Batch: 0.9777888730385248 \t Current Single Error:  0.96 \n",
      "Average Top5 Error Per Batch: 0.9300855920114122 \t Current Top5 Error:  0.91 \n",
      "Average Top10 Error Per Batch: 0.8907845934379442 \t Current Top10 Error:  0.86 \n",
      "Average Top20 Error Per Batch: 0.8350356633380883 \t Current Top20 Error:  0.85\n",
      "Validation evaluation complete. \n",
      "Average Single Error Per Batch: 0.9779143646408927 \n",
      "Average Top5 Error Per Batch: 0.9300552486187842 \n",
      "Average Top10 Error Per Batch: 0.8909392265193351 \n",
      "Average Top20 Error Per Batch: 0.8352624309392264\n"
     ]
    }
   ],
   "source": [
    "USE_CUDA = True\n",
    "\n",
    "checkpoint = torch.load(\"outputs/anp_log_softmax_classifier_batch_21\")\n",
    "main_model = ANPClassifier(len(train_anp_tags))\n",
    "main_model.load_state_dict(checkpoint['state_dict'])\n",
    "main_model.eval()\n",
    "\n",
    "evaluate_validation(main_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done Validation Batch: 0 \n",
      "Average Single Error Per Batch: 0.97 \t Current Single Error:  0.97 \n",
      "Average Top5 Error Per Batch: 0.9 \t Current Top5 Error:  0.9 \n",
      "Average Top10 Error Per Batch: 0.89 \t Current Top10 Error:  0.89 \n",
      "Average Top20 Error Per Batch: 0.83 \t Current Top20 Error:  0.83\n",
      "Done Validation Batch: 50 \n",
      "Average Single Error Per Batch: 0.9786274509803918 \t Current Single Error:  0.97 \n",
      "Average Top5 Error Per Batch: 0.927843137254902 \t Current Top5 Error:  0.9299999999999999 \n",
      "Average Top10 Error Per Batch: 0.8886274509803923 \t Current Top10 Error:  0.87 \n",
      "Average Top20 Error Per Batch: 0.8294117647058824 \t Current Top20 Error:  0.8200000000000001\n",
      "Done Validation Batch: 100 \n",
      "Average Single Error Per Batch: 0.977722772277227 \t Current Single Error:  0.94 \n",
      "Average Top5 Error Per Batch: 0.9294059405940595 \t Current Top5 Error:  0.91 \n",
      "Average Top10 Error Per Batch: 0.8908910891089108 \t Current Top10 Error:  0.86 \n",
      "Average Top20 Error Per Batch: 0.8315841584158414 \t Current Top20 Error:  0.83\n",
      "Done Validation Batch: 150 \n",
      "Average Single Error Per Batch: 0.9792052980132444 \t Current Single Error:  1.0 \n",
      "Average Top5 Error Per Batch: 0.9313245033112586 \t Current Top5 Error:  0.97 \n",
      "Average Top10 Error Per Batch: 0.8931125827814569 \t Current Top10 Error:  0.9299999999999999 \n",
      "Average Top20 Error Per Batch: 0.8349668874172179 \t Current Top20 Error:  0.85\n",
      "Done Validation Batch: 200 \n",
      "Average Single Error Per Batch: 0.9785074626865662 \t Current Single Error:  0.99 \n",
      "Average Top5 Error Per Batch: 0.9300497512437808 \t Current Top5 Error:  0.96 \n",
      "Average Top10 Error Per Batch: 0.8907462686567161 \t Current Top10 Error:  0.9299999999999999 \n",
      "Average Top20 Error Per Batch: 0.8321890547263685 \t Current Top20 Error:  0.86\n",
      "Done Validation Batch: 250 \n",
      "Average Single Error Per Batch: 0.977968127490039 \t Current Single Error:  0.99 \n",
      "Average Top5 Error Per Batch: 0.9288844621513934 \t Current Top5 Error:  0.9299999999999999 \n",
      "Average Top10 Error Per Batch: 0.8895617529880475 \t Current Top10 Error:  0.89 \n",
      "Average Top20 Error Per Batch: 0.8317928286852596 \t Current Top20 Error:  0.85\n",
      "Done Validation Batch: 300 \n",
      "Average Single Error Per Batch: 0.9779734219269107 \t Current Single Error:  0.98 \n",
      "Average Top5 Error Per Batch: 0.9292358803986711 \t Current Top5 Error:  0.95 \n",
      "Average Top10 Error Per Batch: 0.8901328903654482 \t Current Top10 Error:  0.92 \n",
      "Average Top20 Error Per Batch: 0.8328571428571437 \t Current Top20 Error:  0.85\n",
      "Done Validation Batch: 350 \n",
      "Average Single Error Per Batch: 0.9781196581196598 \t Current Single Error:  0.97 \n",
      "Average Top5 Error Per Batch: 0.9294301994301994 \t Current Top5 Error:  0.9299999999999999 \n",
      "Average Top10 Error Per Batch: 0.8899715099715096 \t Current Top10 Error:  0.9 \n",
      "Average Top20 Error Per Batch: 0.8335897435897444 \t Current Top20 Error:  0.84\n",
      "Done Validation Batch: 400 \n",
      "Average Single Error Per Batch: 0.9781047381546165 \t Current Single Error:  0.98 \n",
      "Average Top5 Error Per Batch: 0.9295012468827935 \t Current Top5 Error:  0.9299999999999999 \n",
      "Average Top10 Error Per Batch: 0.8903740648379048 \t Current Top10 Error:  0.91 \n",
      "Average Top20 Error Per Batch: 0.8340897755610978 \t Current Top20 Error:  0.85\n",
      "Done Validation Batch: 450 \n",
      "Average Single Error Per Batch: 0.9781596452328198 \t Current Single Error:  0.98 \n",
      "Average Top5 Error Per Batch: 0.9301330376940137 \t Current Top5 Error:  0.95 \n",
      "Average Top10 Error Per Batch: 0.8909977827050994 \t Current Top10 Error:  0.9 \n",
      "Average Top20 Error Per Batch: 0.8349223946784929 \t Current Top20 Error:  0.85\n",
      "Done Validation Batch: 500 \n",
      "Average Single Error Per Batch: 0.9782834331337371 \t Current Single Error:  0.94 \n",
      "Average Top5 Error Per Batch: 0.9301796407185634 \t Current Top5 Error:  0.9 \n",
      "Average Top10 Error Per Batch: 0.8907784431137721 \t Current Top10 Error:  0.86 \n",
      "Average Top20 Error Per Batch: 0.8344311377245512 \t Current Top20 Error:  0.77\n",
      "Done Validation Batch: 550 \n",
      "Average Single Error Per Batch: 0.9783303085299511 \t Current Single Error:  0.99 \n",
      "Average Top5 Error Per Batch: 0.9301088929219608 \t Current Top5 Error:  0.96 \n",
      "Average Top10 Error Per Batch: 0.8908348457350271 \t Current Top10 Error:  0.9299999999999999 \n",
      "Average Top20 Error Per Batch: 0.834446460980036 \t Current Top20 Error:  0.84\n",
      "Done Validation Batch: 600 \n",
      "Average Single Error Per Batch: 0.9784193011647321 \t Current Single Error:  0.99 \n",
      "Average Top5 Error Per Batch: 0.9302828618968386 \t Current Top5 Error:  0.92 \n",
      "Average Top10 Error Per Batch: 0.890848585690516 \t Current Top10 Error:  0.86 \n",
      "Average Top20 Error Per Batch: 0.8343594009983359 \t Current Top20 Error:  0.8200000000000001\n",
      "Done Validation Batch: 650 \n",
      "Average Single Error Per Batch: 0.9785253456221273 \t Current Single Error:  0.97 \n",
      "Average Top5 Error Per Batch: 0.9306144393241171 \t Current Top5 Error:  0.91 \n",
      "Average Top10 Error Per Batch: 0.8915053763440853 \t Current Top10 Error:  0.9 \n",
      "Average Top20 Error Per Batch: 0.8349615975422432 \t Current Top20 Error:  0.8200000000000001\n",
      "Done Validation Batch: 700 \n",
      "Average Single Error Per Batch: 0.9783594864479399 \t Current Single Error:  0.97 \n",
      "Average Top5 Error Per Batch: 0.9302139800285307 \t Current Top5 Error:  0.94 \n",
      "Average Top10 Error Per Batch: 0.8913266761768887 \t Current Top10 Error:  0.9299999999999999 \n",
      "Average Top20 Error Per Batch: 0.8349928673323833 \t Current Top20 Error:  0.87\n",
      "Validation evaluation complete. \n",
      "Average Single Error Per Batch: 0.9782458563536 \n",
      "Average Top5 Error Per Batch: 0.9302486187845302 \n",
      "Average Top10 Error Per Batch: 0.8913950276243076 \n",
      "Average Top20 Error Per Batch: 0.8349171270718242\n"
     ]
    }
   ],
   "source": [
    "USE_CUDA = True\n",
    "\n",
    "checkpoint = torch.load(\"outputs/anp_log_softmax_classifier_batch_19\")\n",
    "main_model = ANPClassifier(len(train_anp_tags))\n",
    "main_model.load_state_dict(checkpoint['state_dict'])\n",
    "main_model.eval()\n",
    "\n",
    "evaluate_validation(main_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done Validation Batch: 0 \n",
      "Average Single Error Per Batch: 0.97 \t Current Single Error:  0.97 \n",
      "Average Top5 Error Per Batch: 0.92 \t Current Top5 Error:  0.92 \n",
      "Average Top10 Error Per Batch: 0.88 \t Current Top10 Error:  0.88 \n",
      "Average Top20 Error Per Batch: 0.84 \t Current Top20 Error:  0.84\n",
      "Done Validation Batch: 50 \n",
      "Average Single Error Per Batch: 0.9752941176470588 \t Current Single Error:  0.98 \n",
      "Average Top5 Error Per Batch: 0.9241176470588236 \t Current Top5 Error:  0.91 \n",
      "Average Top10 Error Per Batch: 0.8837254901960783 \t Current Top10 Error:  0.88 \n",
      "Average Top20 Error Per Batch: 0.8288235294117647 \t Current Top20 Error:  0.85\n",
      "Done Validation Batch: 100 \n",
      "Average Single Error Per Batch: 0.9760396039603959 \t Current Single Error:  0.95 \n",
      "Average Top5 Error Per Batch: 0.9252475247524752 \t Current Top5 Error:  0.9 \n",
      "Average Top10 Error Per Batch: 0.8860396039603956 \t Current Top10 Error:  0.85 \n",
      "Average Top20 Error Per Batch: 0.8317821782178219 \t Current Top20 Error:  0.78\n",
      "Done Validation Batch: 150 \n",
      "Average Single Error Per Batch: 0.976622516556291 \t Current Single Error:  0.98 \n",
      "Average Top5 Error Per Batch: 0.9260927152317887 \t Current Top5 Error:  0.9299999999999999 \n",
      "Average Top10 Error Per Batch: 0.8878807947019866 \t Current Top10 Error:  0.9 \n",
      "Average Top20 Error Per Batch: 0.8332450331125827 \t Current Top20 Error:  0.8200000000000001\n",
      "Done Validation Batch: 200 \n",
      "Average Single Error Per Batch: 0.9766169154228849 \t Current Single Error:  0.96 \n",
      "Average Top5 Error Per Batch: 0.9272139303482585 \t Current Top5 Error:  0.91 \n",
      "Average Top10 Error Per Batch: 0.8883582089552237 \t Current Top10 Error:  0.87 \n",
      "Average Top20 Error Per Batch: 0.8310447761194035 \t Current Top20 Error:  0.8200000000000001\n",
      "Done Validation Batch: 250 \n",
      "Average Single Error Per Batch: 0.9767729083665335 \t Current Single Error:  0.99 \n",
      "Average Top5 Error Per Batch: 0.9261354581673297 \t Current Top5 Error:  0.96 \n",
      "Average Top10 Error Per Batch: 0.8866135458167329 \t Current Top10 Error:  0.94 \n",
      "Average Top20 Error Per Batch: 0.8298007968127495 \t Current Top20 Error:  0.89\n",
      "Done Validation Batch: 300 \n",
      "Average Single Error Per Batch: 0.9767774086378747 \t Current Single Error:  1.0 \n",
      "Average Top5 Error Per Batch: 0.9272757475083051 \t Current Top5 Error:  0.94 \n",
      "Average Top10 Error Per Batch: 0.887840531561461 \t Current Top10 Error:  0.9 \n",
      "Average Top20 Error Per Batch: 0.8307641196013296 \t Current Top20 Error:  0.85\n",
      "Done Validation Batch: 350 \n",
      "Average Single Error Per Batch: 0.9768660968660989 \t Current Single Error:  0.97 \n",
      "Average Top5 Error Per Batch: 0.9280056980056972 \t Current Top5 Error:  0.89 \n",
      "Average Top10 Error Per Batch: 0.8880626780626777 \t Current Top10 Error:  0.89 \n",
      "Average Top20 Error Per Batch: 0.8312250712250717 \t Current Top20 Error:  0.85\n",
      "Done Validation Batch: 400 \n",
      "Average Single Error Per Batch: 0.9769077306733198 \t Current Single Error:  0.98 \n",
      "Average Top5 Error Per Batch: 0.928104738154613 \t Current Top5 Error:  0.96 \n",
      "Average Top10 Error Per Batch: 0.8880548628428924 \t Current Top10 Error:  0.92 \n",
      "Average Top20 Error Per Batch: 0.8312219451371572 \t Current Top20 Error:  0.87\n",
      "Done Validation Batch: 450 \n",
      "Average Single Error Per Batch: 0.9773392461197379 \t Current Single Error:  0.99 \n",
      "Average Top5 Error Per Batch: 0.9287804878048777 \t Current Top5 Error:  0.96 \n",
      "Average Top10 Error Per Batch: 0.8886917960088689 \t Current Top10 Error:  0.92 \n",
      "Average Top20 Error Per Batch: 0.8319955654101997 \t Current Top20 Error:  0.88\n",
      "Done Validation Batch: 500 \n",
      "Average Single Error Per Batch: 0.9771257485029985 \t Current Single Error:  0.95 \n",
      "Average Top5 Error Per Batch: 0.9281437125748495 \t Current Top5 Error:  0.89 \n",
      "Average Top10 Error Per Batch: 0.8883433133732535 \t Current Top10 Error:  0.84 \n",
      "Average Top20 Error Per Batch: 0.8315169660678645 \t Current Top20 Error:  0.81\n",
      "Done Validation Batch: 550 \n",
      "Average Single Error Per Batch: 0.9772776769510035 \t Current Single Error:  0.98 \n",
      "Average Top5 Error Per Batch: 0.9283484573502717 \t Current Top5 Error:  0.94 \n",
      "Average Top10 Error Per Batch: 0.8884210526315787 \t Current Top10 Error:  0.92 \n",
      "Average Top20 Error Per Batch: 0.8317422867513615 \t Current Top20 Error:  0.88\n",
      "Done Validation Batch: 600 \n",
      "Average Single Error Per Batch: 0.9773710482529184 \t Current Single Error:  0.97 \n",
      "Average Top5 Error Per Batch: 0.9284359400998321 \t Current Top5 Error:  0.91 \n",
      "Average Top10 Error Per Batch: 0.8882529118136432 \t Current Top10 Error:  0.87 \n",
      "Average Top20 Error Per Batch: 0.8315973377703827 \t Current Top20 Error:  0.84\n",
      "Done Validation Batch: 650 \n",
      "Average Single Error Per Batch: 0.9773425499232028 \t Current Single Error:  0.97 \n",
      "Average Top5 Error Per Batch: 0.928786482334868 \t Current Top5 Error:  0.89 \n",
      "Average Top10 Error Per Batch: 0.8886175115207363 \t Current Top10 Error:  0.86 \n",
      "Average Top20 Error Per Batch: 0.8321505376344088 \t Current Top20 Error:  0.79\n",
      "Done Validation Batch: 700 \n",
      "Average Single Error Per Batch: 0.977532097004288 \t Current Single Error:  0.98 \n",
      "Average Top5 Error Per Batch: 0.9289300998573454 \t Current Top5 Error:  0.9299999999999999 \n",
      "Average Top10 Error Per Batch: 0.8887303851640498 \t Current Top10 Error:  0.9 \n",
      "Average Top20 Error Per Batch: 0.8322681883024253 \t Current Top20 Error:  0.88\n",
      "Validation evaluation complete. \n",
      "Average Single Error Per Batch: 0.977513812154705 \n",
      "Average Top5 Error Per Batch: 0.9290193370165735 \n",
      "Average Top10 Error Per Batch: 0.8886740331491695 \n",
      "Average Top20 Error Per Batch: 0.8325138121546969\n"
     ]
    }
   ],
   "source": [
    "USE_CUDA = True\n",
    "\n",
    "checkpoint = torch.load(\"outputs/anp_log_softmax_classifier_batch_17\")\n",
    "main_model = ANPClassifier(len(train_anp_tags))\n",
    "main_model.load_state_dict(checkpoint['state_dict'])\n",
    "main_model.eval()\n",
    "\n",
    "evaluate_validation(main_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done Validation Batch: 0 \n",
      "Average Single Error Per Batch: 0.97 \t Current Single Error:  0.97 \n",
      "Average Top5 Error Per Batch: 0.89 \t Current Top5 Error:  0.89 \n",
      "Average Top10 Error Per Batch: 0.85 \t Current Top10 Error:  0.85 \n",
      "Average Top20 Error Per Batch: 0.84 \t Current Top20 Error:  0.84\n",
      "Done Validation Batch: 50 \n",
      "Average Single Error Per Batch: 0.980196078431372 \t Current Single Error:  0.98 \n",
      "Average Top5 Error Per Batch: 0.9288235294117646 \t Current Top5 Error:  0.9299999999999999 \n",
      "Average Top10 Error Per Batch: 0.8925490196078432 \t Current Top10 Error:  0.92 \n",
      "Average Top20 Error Per Batch: 0.8347058823529411 \t Current Top20 Error:  0.85\n",
      "Done Validation Batch: 100 \n",
      "Average Single Error Per Batch: 0.9800990099009892 \t Current Single Error:  0.96 \n",
      "Average Top5 Error Per Batch: 0.9323762376237626 \t Current Top5 Error:  0.9299999999999999 \n",
      "Average Top10 Error Per Batch: 0.894752475247525 \t Current Top10 Error:  0.88 \n",
      "Average Top20 Error Per Batch: 0.8399009900990098 \t Current Top20 Error:  0.8200000000000001\n",
      "Done Validation Batch: 150 \n",
      "Average Single Error Per Batch: 0.978211920529801 \t Current Single Error:  0.97 \n",
      "Average Top5 Error Per Batch: 0.9321854304635765 \t Current Top5 Error:  0.9299999999999999 \n",
      "Average Top10 Error Per Batch: 0.8952317880794706 \t Current Top10 Error:  0.88 \n",
      "Average Top20 Error Per Batch: 0.8400662251655626 \t Current Top20 Error:  0.84\n",
      "Done Validation Batch: 200 \n",
      "Average Single Error Per Batch: 0.9776119402985072 \t Current Single Error:  0.95 \n",
      "Average Top5 Error Per Batch: 0.9318407960198999 \t Current Top5 Error:  0.9299999999999999 \n",
      "Average Top10 Error Per Batch: 0.8947263681592037 \t Current Top10 Error:  0.9 \n",
      "Average Top20 Error Per Batch: 0.8384079601990053 \t Current Top20 Error:  0.84\n",
      "Done Validation Batch: 250 \n",
      "Average Single Error Per Batch: 0.9776095617529877 \t Current Single Error:  0.98 \n",
      "Average Top5 Error Per Batch: 0.9307569721115532 \t Current Top5 Error:  0.94 \n",
      "Average Top10 Error Per Batch: 0.8936254980079675 \t Current Top10 Error:  0.9 \n",
      "Average Top20 Error Per Batch: 0.8375697211155385 \t Current Top20 Error:  0.87\n",
      "Done Validation Batch: 300 \n",
      "Average Single Error Per Batch: 0.9779734219269115 \t Current Single Error:  0.99 \n",
      "Average Top5 Error Per Batch: 0.9315282392026574 \t Current Top5 Error:  0.9299999999999999 \n",
      "Average Top10 Error Per Batch: 0.8935215946843846 \t Current Top10 Error:  0.89 \n",
      "Average Top20 Error Per Batch: 0.8377076411960142 \t Current Top20 Error:  0.85\n",
      "Done Validation Batch: 350 \n",
      "Average Single Error Per Batch: 0.9779487179487204 \t Current Single Error:  0.97 \n",
      "Average Top5 Error Per Batch: 0.9322507122507119 \t Current Top5 Error:  0.94 \n",
      "Average Top10 Error Per Batch: 0.894188034188034 \t Current Top10 Error:  0.9 \n",
      "Average Top20 Error Per Batch: 0.8384615384615389 \t Current Top20 Error:  0.87\n",
      "Done Validation Batch: 400 \n",
      "Average Single Error Per Batch: 0.97760598503741 \t Current Single Error:  0.97 \n",
      "Average Top5 Error Per Batch: 0.9324937655860343 \t Current Top5 Error:  0.96 \n",
      "Average Top10 Error Per Batch: 0.8939900249376554 \t Current Top10 Error:  0.9 \n",
      "Average Top20 Error Per Batch: 0.8380548628428933 \t Current Top20 Error:  0.86\n",
      "Done Validation Batch: 450 \n",
      "Average Single Error Per Batch: 0.9779157427937961 \t Current Single Error:  1.0 \n",
      "Average Top5 Error Per Batch: 0.9326829268292678 \t Current Top5 Error:  0.96 \n",
      "Average Top10 Error Per Batch: 0.8947450110864742 \t Current Top10 Error:  0.9299999999999999 \n",
      "Average Top20 Error Per Batch: 0.838669623059867 \t Current Top20 Error:  0.86\n",
      "Done Validation Batch: 500 \n",
      "Average Single Error Per Batch: 0.9779041916167717 \t Current Single Error:  0.96 \n",
      "Average Top5 Error Per Batch: 0.9317165668662671 \t Current Top5 Error:  0.91 \n",
      "Average Top10 Error Per Batch: 0.89377245508982 \t Current Top10 Error:  0.88 \n",
      "Average Top20 Error Per Batch: 0.8380838323353293 \t Current Top20 Error:  0.78\n",
      "Done Validation Batch: 550 \n",
      "Average Single Error Per Batch: 0.977967332123418 \t Current Single Error:  0.96 \n",
      "Average Top5 Error Per Batch: 0.9322323049001813 \t Current Top5 Error:  0.9299999999999999 \n",
      "Average Top10 Error Per Batch: 0.8937931034482751 \t Current Top10 Error:  0.91 \n",
      "Average Top20 Error Per Batch: 0.8379673321234116 \t Current Top20 Error:  0.87\n",
      "Done Validation Batch: 600 \n",
      "Average Single Error Per Batch: 0.9780199667221369 \t Current Single Error:  0.99 \n",
      "Average Top5 Error Per Batch: 0.9323460898502496 \t Current Top5 Error:  0.97 \n",
      "Average Top10 Error Per Batch: 0.8939101497504148 \t Current Top10 Error:  0.92 \n",
      "Average Top20 Error Per Batch: 0.8382529118136434 \t Current Top20 Error:  0.87\n",
      "Done Validation Batch: 650 \n",
      "Average Single Error Per Batch: 0.9780337941628343 \t Current Single Error:  0.94 \n",
      "Average Top5 Error Per Batch: 0.9322273425499233 \t Current Top5 Error:  0.89 \n",
      "Average Top10 Error Per Batch: 0.8940706605222715 \t Current Top10 Error:  0.88 \n",
      "Average Top20 Error Per Batch: 0.8388172043010746 \t Current Top20 Error:  0.83\n",
      "Done Validation Batch: 700 \n",
      "Average Single Error Per Batch: 0.9779315263908791 \t Current Single Error:  0.98 \n",
      "Average Top5 Error Per Batch: 0.9321968616262478 \t Current Top5 Error:  0.92 \n",
      "Average Top10 Error Per Batch: 0.8940370898716099 \t Current Top10 Error:  0.86 \n",
      "Average Top20 Error Per Batch: 0.8388445078459351 \t Current Top20 Error:  0.83\n",
      "Validation evaluation complete. \n",
      "Average Single Error Per Batch: 0.9779696132596777 \n",
      "Average Top5 Error Per Batch: 0.9323895027624307 \n",
      "Average Top10 Error Per Batch: 0.8940469613259647 \n",
      "Average Top20 Error Per Batch: 0.8391160220994486\n"
     ]
    }
   ],
   "source": [
    "USE_CUDA = True\n",
    "\n",
    "checkpoint = torch.load(\"outputs/anp_log_softmax_classifier_batch_15\")\n",
    "main_model = ANPClassifier(len(train_anp_tags))\n",
    "main_model.load_state_dict(checkpoint['state_dict'])\n",
    "main_model.eval()\n",
    "\n",
    "evaluate_validation(main_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done Validation Batch: 0 \n",
      "Average Single Error Per Batch: 0.99 \t Current Single Error:  0.99 \n",
      "Average Top5 Error Per Batch: 0.95 \t Current Top5 Error:  0.95 \n",
      "Average Top10 Error Per Batch: 0.88 \t Current Top10 Error:  0.88 \n",
      "Average Top20 Error Per Batch: 0.86 \t Current Top20 Error:  0.86\n",
      "Done Validation Batch: 50 \n",
      "Average Single Error Per Batch: 0.9723529411764705 \t Current Single Error:  0.98 \n",
      "Average Top5 Error Per Batch: 0.9160784313725489 \t Current Top5 Error:  0.92 \n",
      "Average Top10 Error Per Batch: 0.8670588235294115 \t Current Top10 Error:  0.9 \n",
      "Average Top20 Error Per Batch: 0.8050980392156865 \t Current Top20 Error:  0.86\n",
      "Done Validation Batch: 100 \n",
      "Average Single Error Per Batch: 0.9750495049504951 \t Current Single Error:  0.97 \n",
      "Average Top5 Error Per Batch: 0.9213861386138611 \t Current Top5 Error:  0.92 \n",
      "Average Top10 Error Per Batch: 0.8734653465346534 \t Current Top10 Error:  0.89 \n",
      "Average Top20 Error Per Batch: 0.8135643564356437 \t Current Top20 Error:  0.8200000000000001\n",
      "Done Validation Batch: 150 \n",
      "Average Single Error Per Batch: 0.9739735099337746 \t Current Single Error:  0.97 \n",
      "Average Top5 Error Per Batch: 0.9208609271523174 \t Current Top5 Error:  0.95 \n",
      "Average Top10 Error Per Batch: 0.8746357615894036 \t Current Top10 Error:  0.88 \n",
      "Average Top20 Error Per Batch: 0.8149006622516555 \t Current Top20 Error:  0.81\n",
      "Done Validation Batch: 200 \n",
      "Average Single Error Per Batch: 0.9735820895522381 \t Current Single Error:  0.96 \n",
      "Average Top5 Error Per Batch: 0.9207462686567149 \t Current Top5 Error:  0.91 \n",
      "Average Top10 Error Per Batch: 0.8742288557213924 \t Current Top10 Error:  0.86 \n",
      "Average Top20 Error Per Batch: 0.8143781094527365 \t Current Top20 Error:  0.8\n",
      "Done Validation Batch: 250 \n",
      "Average Single Error Per Batch: 0.9732669322709158 \t Current Single Error:  0.98 \n",
      "Average Top5 Error Per Batch: 0.9203187250995997 \t Current Top5 Error:  0.97 \n",
      "Average Top10 Error Per Batch: 0.8742231075697205 \t Current Top10 Error:  0.9 \n",
      "Average Top20 Error Per Batch: 0.8137450199203196 \t Current Top20 Error:  0.87\n",
      "Done Validation Batch: 300 \n",
      "Average Single Error Per Batch: 0.9734883720930235 \t Current Single Error:  1.0 \n",
      "Average Top5 Error Per Batch: 0.9197009966777395 \t Current Top5 Error:  0.95 \n",
      "Average Top10 Error Per Batch: 0.8740531561461787 \t Current Top10 Error:  0.9 \n",
      "Average Top20 Error Per Batch: 0.8129568106312298 \t Current Top20 Error:  0.83\n",
      "Done Validation Batch: 350 \n",
      "Average Single Error Per Batch: 0.9735897435897441 \t Current Single Error:  0.96 \n",
      "Average Top5 Error Per Batch: 0.9192307692307684 \t Current Top5 Error:  0.91 \n",
      "Average Top10 Error Per Batch: 0.8739316239316236 \t Current Top10 Error:  0.86 \n",
      "Average Top20 Error Per Batch: 0.8132193732193737 \t Current Top20 Error:  0.85\n",
      "Done Validation Batch: 400 \n",
      "Average Single Error Per Batch: 0.9738902743142159 \t Current Single Error:  0.98 \n",
      "Average Top5 Error Per Batch: 0.9197256857855355 \t Current Top5 Error:  0.92 \n",
      "Average Top10 Error Per Batch: 0.8740897755610973 \t Current Top10 Error:  0.89 \n",
      "Average Top20 Error Per Batch: 0.8131920199501248 \t Current Top20 Error:  0.86\n",
      "Done Validation Batch: 450 \n",
      "Average Single Error Per Batch: 0.9736363636363656 \t Current Single Error:  0.99 \n",
      "Average Top5 Error Per Batch: 0.9193791574279375 \t Current Top5 Error:  0.96 \n",
      "Average Top10 Error Per Batch: 0.8740576496674055 \t Current Top10 Error:  0.91 \n",
      "Average Top20 Error Per Batch: 0.8134368070953435 \t Current Top20 Error:  0.85\n",
      "Done Validation Batch: 500 \n",
      "Average Single Error Per Batch: 0.9735129740518985 \t Current Single Error:  0.98 \n",
      "Average Top5 Error Per Batch: 0.9192015968063868 \t Current Top5 Error:  0.9299999999999999 \n",
      "Average Top10 Error Per Batch: 0.8739121756487024 \t Current Top10 Error:  0.87 \n",
      "Average Top20 Error Per Batch: 0.8135528942115765 \t Current Top20 Error:  0.77\n",
      "Done Validation Batch: 550 \n",
      "Average Single Error Per Batch: 0.9734664246823992 \t Current Single Error:  0.95 \n",
      "Average Top5 Error Per Batch: 0.9197096188747729 \t Current Top5 Error:  0.94 \n",
      "Average Top10 Error Per Batch: 0.874337568058076 \t Current Top10 Error:  0.89 \n",
      "Average Top20 Error Per Batch: 0.8137386569872953 \t Current Top20 Error:  0.85\n",
      "Done Validation Batch: 600 \n",
      "Average Single Error Per Batch: 0.9732778702163112 \t Current Single Error:  0.98 \n",
      "Average Top5 Error Per Batch: 0.919717138103161 \t Current Top5 Error:  0.94 \n",
      "Average Top10 Error Per Batch: 0.8740099833610646 \t Current Top10 Error:  0.87 \n",
      "Average Top20 Error Per Batch: 0.8132778702163054 \t Current Top20 Error:  0.8\n",
      "Done Validation Batch: 650 \n",
      "Average Single Error Per Batch: 0.9730107526881786 \t Current Single Error:  0.91 \n",
      "Average Top5 Error Per Batch: 0.9191397849462355 \t Current Top5 Error:  0.86 \n",
      "Average Top10 Error Per Batch: 0.8740092165898616 \t Current Top10 Error:  0.8200000000000001 \n",
      "Average Top20 Error Per Batch: 0.8133333333333324 \t Current Top20 Error:  0.78\n",
      "Done Validation Batch: 700 \n",
      "Average Single Error Per Batch: 0.9729386590584954 \t Current Single Error:  0.95 \n",
      "Average Top5 Error Per Batch: 0.9191012838801699 \t Current Top5 Error:  0.9299999999999999 \n",
      "Average Top10 Error Per Batch: 0.8740941512125533 \t Current Top10 Error:  0.89 \n",
      "Average Top20 Error Per Batch: 0.8134664764621956 \t Current Top20 Error:  0.84\n",
      "Validation evaluation complete. \n",
      "Average Single Error Per Batch: 0.9731215469613339 \n",
      "Average Top5 Error Per Batch: 0.9193646408839765 \n",
      "Average Top10 Error Per Batch: 0.8745718232044196 \n",
      "Average Top20 Error Per Batch: 0.8138535911602196\n"
     ]
    }
   ],
   "source": [
    "USE_CUDA = True\n",
    "\n",
    "checkpoint = torch.load(\"outputs/anp_log_softmax_classifier_batch_13\")\n",
    "main_model = ANPClassifier(len(train_anp_tags))\n",
    "main_model.load_state_dict(checkpoint['state_dict'])\n",
    "main_model.eval()\n",
    "\n",
    "evaluate_validation(main_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done Validation Batch: 0 \n",
      "Average Single Error Per Batch: 0.99 \t Current Single Error:  0.99 \n",
      "Average Top5 Error Per Batch: 0.88 \t Current Top5 Error:  0.88 \n",
      "Average Top10 Error Per Batch: 0.86 \t Current Top10 Error:  0.86 \n",
      "Average Top20 Error Per Batch: 0.81 \t Current Top20 Error:  0.81\n",
      "Done Validation Batch: 50 \n",
      "Average Single Error Per Batch: 0.9713725490196075 \t Current Single Error:  0.97 \n",
      "Average Top5 Error Per Batch: 0.9131372549019606 \t Current Top5 Error:  0.94 \n",
      "Average Top10 Error Per Batch: 0.8741176470588237 \t Current Top10 Error:  0.91 \n",
      "Average Top20 Error Per Batch: 0.8107843137254901 \t Current Top20 Error:  0.87\n",
      "Done Validation Batch: 100 \n",
      "Average Single Error Per Batch: 0.9723762376237616 \t Current Single Error:  1.0 \n",
      "Average Top5 Error Per Batch: 0.9121782178217825 \t Current Top5 Error:  0.92 \n",
      "Average Top10 Error Per Batch: 0.8694059405940593 \t Current Top10 Error:  0.87 \n",
      "Average Top20 Error Per Batch: 0.8097029702970294 \t Current Top20 Error:  0.8200000000000001\n",
      "Done Validation Batch: 150 \n",
      "Average Single Error Per Batch: 0.9734437086092707 \t Current Single Error:  0.99 \n",
      "Average Top5 Error Per Batch: 0.9145695364238414 \t Current Top5 Error:  0.91 \n",
      "Average Top10 Error Per Batch: 0.8690066225165562 \t Current Top10 Error:  0.84 \n",
      "Average Top20 Error Per Batch: 0.8076821192052976 \t Current Top20 Error:  0.78\n",
      "Done Validation Batch: 200 \n",
      "Average Single Error Per Batch: 0.9731840796019893 \t Current Single Error:  0.94 \n",
      "Average Top5 Error Per Batch: 0.91502487562189 \t Current Top5 Error:  0.9 \n",
      "Average Top10 Error Per Batch: 0.8701990049751243 \t Current Top10 Error:  0.87 \n",
      "Average Top20 Error Per Batch: 0.8074626865671641 \t Current Top20 Error:  0.83\n",
      "Done Validation Batch: 250 \n",
      "Average Single Error Per Batch: 0.9723505976095609 \t Current Single Error:  0.98 \n",
      "Average Top5 Error Per Batch: 0.9147808764940232 \t Current Top5 Error:  0.9299999999999999 \n",
      "Average Top10 Error Per Batch: 0.8699601593625497 \t Current Top10 Error:  0.9 \n",
      "Average Top20 Error Per Batch: 0.8077290836653389 \t Current Top20 Error:  0.84\n",
      "Done Validation Batch: 300 \n",
      "Average Single Error Per Batch: 0.9718936877076415 \t Current Single Error:  1.0 \n",
      "Average Top5 Error Per Batch: 0.9143521594684383 \t Current Top5 Error:  0.94 \n",
      "Average Top10 Error Per Batch: 0.8694684385382059 \t Current Top10 Error:  0.9 \n",
      "Average Top20 Error Per Batch: 0.8072093023255822 \t Current Top20 Error:  0.86\n",
      "Done Validation Batch: 350 \n",
      "Average Single Error Per Batch: 0.9716239316239327 \t Current Single Error:  0.98 \n",
      "Average Top5 Error Per Batch: 0.9140170940170943 \t Current Top5 Error:  0.94 \n",
      "Average Top10 Error Per Batch: 0.8689458689458688 \t Current Top10 Error:  0.89 \n",
      "Average Top20 Error Per Batch: 0.8065811965811971 \t Current Top20 Error:  0.8200000000000001\n",
      "Done Validation Batch: 400 \n",
      "Average Single Error Per Batch: 0.9719451371571092 \t Current Single Error:  0.98 \n",
      "Average Top5 Error Per Batch: 0.9140897755610982 \t Current Top5 Error:  0.96 \n",
      "Average Top10 Error Per Batch: 0.86785536159601 \t Current Top10 Error:  0.9299999999999999 \n",
      "Average Top20 Error Per Batch: 0.8063840399002494 \t Current Top20 Error:  0.9\n",
      "Done Validation Batch: 450 \n",
      "Average Single Error Per Batch: 0.9725720620842597 \t Current Single Error:  0.99 \n",
      "Average Top5 Error Per Batch: 0.9141906873614197 \t Current Top5 Error:  0.96 \n",
      "Average Top10 Error Per Batch: 0.8683148558758309 \t Current Top10 Error:  0.91 \n",
      "Average Top20 Error Per Batch: 0.8066518847006647 \t Current Top20 Error:  0.84\n",
      "Done Validation Batch: 500 \n",
      "Average Single Error Per Batch: 0.9726746506986057 \t Current Single Error:  0.99 \n",
      "Average Top5 Error Per Batch: 0.9145508982035939 \t Current Top5 Error:  0.9 \n",
      "Average Top10 Error Per Batch: 0.8687624750498996 \t Current Top10 Error:  0.89 \n",
      "Average Top20 Error Per Batch: 0.8068263473053887 \t Current Top20 Error:  0.83\n",
      "Done Validation Batch: 550 \n",
      "Average Single Error Per Batch: 0.9724500907441056 \t Current Single Error:  0.97 \n",
      "Average Top5 Error Per Batch: 0.9145190562613439 \t Current Top5 Error:  0.91 \n",
      "Average Top10 Error Per Batch: 0.8684573502722314 \t Current Top10 Error:  0.84 \n",
      "Average Top20 Error Per Batch: 0.8066606170598907 \t Current Top20 Error:  0.8\n",
      "Done Validation Batch: 600 \n",
      "Average Single Error Per Batch: 0.9727953410981751 \t Current Single Error:  0.99 \n",
      "Average Top5 Error Per Batch: 0.9144259567387691 \t Current Top5 Error:  0.9 \n",
      "Average Top10 Error Per Batch: 0.8681198003327781 \t Current Top10 Error:  0.84 \n",
      "Average Top20 Error Per Batch: 0.8064226289517465 \t Current Top20 Error:  0.8200000000000001\n",
      "Done Validation Batch: 650 \n",
      "Average Single Error Per Batch: 0.9726881720430174 \t Current Single Error:  0.97 \n",
      "Average Top5 Error Per Batch: 0.9144546850998463 \t Current Top5 Error:  0.9 \n",
      "Average Top10 Error Per Batch: 0.868248847926267 \t Current Top10 Error:  0.87 \n",
      "Average Top20 Error Per Batch: 0.8067281105990779 \t Current Top20 Error:  0.8200000000000001\n",
      "Done Validation Batch: 700 \n",
      "Average Single Error Per Batch: 0.9730670470756135 \t Current Single Error:  0.97 \n",
      "Average Top5 Error Per Batch: 0.9149358059914403 \t Current Top5 Error:  0.92 \n",
      "Average Top10 Error Per Batch: 0.8685021398002849 \t Current Top10 Error:  0.92 \n",
      "Average Top20 Error Per Batch: 0.8068188302425098 \t Current Top20 Error:  0.84\n",
      "Validation evaluation complete. \n",
      "Average Single Error Per Batch: 0.9731353591160299 \n",
      "Average Top5 Error Per Batch: 0.9149861878453032 \n",
      "Average Top10 Error Per Batch: 0.8687016574585633 \n",
      "Average Top20 Error Per Batch: 0.807265193370165\n"
     ]
    }
   ],
   "source": [
    "USE_CUDA = True\n",
    "\n",
    "checkpoint = torch.load(\"outputs/anp_log_softmax_classifier_batch_11\")\n",
    "main_model = ANPClassifier(len(train_anp_tags))\n",
    "main_model.load_state_dict(checkpoint['state_dict'])\n",
    "main_model.eval()\n",
    "\n",
    "evaluate_validation(main_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done Validation Batch: 0 \n",
      "Average Single Error Per Batch: 0.98 \t Current Single Error:  0.98 \n",
      "Average Top5 Error Per Batch: 0.9299999999999999 \t Current Top5 Error:  0.9299999999999999 \n",
      "Average Top10 Error Per Batch: 0.91 \t Current Top10 Error:  0.91 \n",
      "Average Top20 Error Per Batch: 0.81 \t Current Top20 Error:  0.81\n",
      "Done Validation Batch: 50 \n",
      "Average Single Error Per Batch: 0.9678431372549017 \t Current Single Error:  0.97 \n",
      "Average Top5 Error Per Batch: 0.9109803921568624 \t Current Top5 Error:  0.9299999999999999 \n",
      "Average Top10 Error Per Batch: 0.8603921568627453 \t Current Top10 Error:  0.9 \n",
      "Average Top20 Error Per Batch: 0.7956862745098043 \t Current Top20 Error:  0.84\n",
      "Done Validation Batch: 100 \n",
      "Average Single Error Per Batch: 0.9703960396039596 \t Current Single Error:  0.97 \n",
      "Average Top5 Error Per Batch: 0.9100990099009898 \t Current Top5 Error:  0.9299999999999999 \n",
      "Average Top10 Error Per Batch: 0.8606930693069309 \t Current Top10 Error:  0.86 \n",
      "Average Top20 Error Per Batch: 0.7931683168316833 \t Current Top20 Error:  0.8\n",
      "Done Validation Batch: 150 \n",
      "Average Single Error Per Batch: 0.9708609271523168 \t Current Single Error:  0.98 \n",
      "Average Top5 Error Per Batch: 0.9082781456953641 \t Current Top5 Error:  0.86 \n",
      "Average Top10 Error Per Batch: 0.8598675496688744 \t Current Top10 Error:  0.79 \n",
      "Average Top20 Error Per Batch: 0.7909271523178808 \t Current Top20 Error:  0.69\n",
      "Done Validation Batch: 200 \n",
      "Average Single Error Per Batch: 0.9708457711442777 \t Current Single Error:  0.9299999999999999 \n",
      "Average Top5 Error Per Batch: 0.9069154228855718 \t Current Top5 Error:  0.88 \n",
      "Average Top10 Error Per Batch: 0.8575621890547267 \t Current Top10 Error:  0.85 \n",
      "Average Top20 Error Per Batch: 0.789353233830846 \t Current Top20 Error:  0.8\n",
      "Done Validation Batch: 250 \n",
      "Average Single Error Per Batch: 0.9709561752988036 \t Current Single Error:  0.98 \n",
      "Average Top5 Error Per Batch: 0.9064143426294816 \t Current Top5 Error:  0.9 \n",
      "Average Top10 Error Per Batch: 0.8570119521912356 \t Current Top10 Error:  0.87 \n",
      "Average Top20 Error Per Batch: 0.7888047808764946 \t Current Top20 Error:  0.81\n",
      "Done Validation Batch: 300 \n",
      "Average Single Error Per Batch: 0.9705315614617942 \t Current Single Error:  0.98 \n",
      "Average Top5 Error Per Batch: 0.906378737541528 \t Current Top5 Error:  0.95 \n",
      "Average Top10 Error Per Batch: 0.8565448504983392 \t Current Top10 Error:  0.87 \n",
      "Average Top20 Error Per Batch: 0.7882724252491701 \t Current Top20 Error:  0.76\n",
      "Done Validation Batch: 350 \n",
      "Average Single Error Per Batch: 0.9703988603988617 \t Current Single Error:  0.95 \n",
      "Average Top5 Error Per Batch: 0.9063247863247865 \t Current Top5 Error:  0.92 \n",
      "Average Top10 Error Per Batch: 0.8566096866096868 \t Current Top10 Error:  0.87 \n",
      "Average Top20 Error Per Batch: 0.7878347578347578 \t Current Top20 Error:  0.8200000000000001\n",
      "Done Validation Batch: 400 \n",
      "Average Single Error Per Batch: 0.9706982543640919 \t Current Single Error:  0.98 \n",
      "Average Top5 Error Per Batch: 0.9060349127182049 \t Current Top5 Error:  0.9299999999999999 \n",
      "Average Top10 Error Per Batch: 0.8565336658354117 \t Current Top10 Error:  0.88 \n",
      "Average Top20 Error Per Batch: 0.7886783042394009 \t Current Top20 Error:  0.81\n",
      "Done Validation Batch: 450 \n",
      "Average Single Error Per Batch: 0.9702439024390267 \t Current Single Error:  1.0 \n",
      "Average Top5 Error Per Batch: 0.9058093126385817 \t Current Top5 Error:  0.99 \n",
      "Average Top10 Error Per Batch: 0.856341463414634 \t Current Top10 Error:  0.88 \n",
      "Average Top20 Error Per Batch: 0.788847006651884 \t Current Top20 Error:  0.8\n",
      "Done Validation Batch: 500 \n",
      "Average Single Error Per Batch: 0.9701996007984063 \t Current Single Error:  0.96 \n",
      "Average Top5 Error Per Batch: 0.9056087824351305 \t Current Top5 Error:  0.91 \n",
      "Average Top10 Error Per Batch: 0.8562075848303394 \t Current Top10 Error:  0.81 \n",
      "Average Top20 Error Per Batch: 0.7889421157684623 \t Current Top20 Error:  0.75\n",
      "Done Validation Batch: 550 \n",
      "Average Single Error Per Batch: 0.9703811252268645 \t Current Single Error:  0.98 \n",
      "Average Top5 Error Per Batch: 0.9050816696914709 \t Current Top5 Error:  0.9 \n",
      "Average Top10 Error Per Batch: 0.8554990925589835 \t Current Top10 Error:  0.85 \n",
      "Average Top20 Error Per Batch: 0.788874773139745 \t Current Top20 Error:  0.8\n",
      "Done Validation Batch: 600 \n",
      "Average Single Error Per Batch: 0.9700665557404387 \t Current Single Error:  0.97 \n",
      "Average Top5 Error Per Batch: 0.9046921797004994 \t Current Top5 Error:  0.94 \n",
      "Average Top10 Error Per Batch: 0.8548086522462562 \t Current Top10 Error:  0.88 \n",
      "Average Top20 Error Per Batch: 0.7879866888519125 \t Current Top20 Error:  0.78\n",
      "Done Validation Batch: 650 \n",
      "Average Single Error Per Batch: 0.9700307219662132 \t Current Single Error:  0.97 \n",
      "Average Top5 Error Per Batch: 0.9045775729646689 \t Current Top5 Error:  0.84 \n",
      "Average Top10 Error Per Batch: 0.8548387096774194 \t Current Top10 Error:  0.84 \n",
      "Average Top20 Error Per Batch: 0.7882949308755748 \t Current Top20 Error:  0.81\n",
      "Done Validation Batch: 700 \n",
      "Average Single Error Per Batch: 0.9702995720399513 \t Current Single Error:  0.97 \n",
      "Average Top5 Error Per Batch: 0.9048502139800273 \t Current Top5 Error:  0.96 \n",
      "Average Top10 Error Per Batch: 0.8552496433666196 \t Current Top10 Error:  0.94 \n",
      "Average Top20 Error Per Batch: 0.7887161198288146 \t Current Top20 Error:  0.85\n",
      "Validation evaluation complete. \n",
      "Average Single Error Per Batch: 0.9704005524861967 \n",
      "Average Top5 Error Per Batch: 0.904861878453037 \n",
      "Average Top10 Error Per Batch: 0.8551933701657465 \n",
      "Average Top20 Error Per Batch: 0.7889226519337001\n"
     ]
    }
   ],
   "source": [
    "USE_CUDA = True\n",
    "\n",
    "checkpoint = torch.load(\"outputs/anp_log_softmax_classifier_batch_9\")\n",
    "main_model = ANPClassifier(len(train_anp_tags))\n",
    "main_model.load_state_dict(checkpoint['state_dict'])\n",
    "main_model.eval()\n",
    "\n",
    "evaluate_validation(main_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done Validation Batch: 0 \n",
      "Average Single Error Per Batch: 1.0 \t Current Single Error:  1.0 \n",
      "Average Top5 Error Per Batch: 0.94 \t Current Top5 Error:  0.94 \n",
      "Average Top10 Error Per Batch: 0.88 \t Current Top10 Error:  0.88 \n",
      "Average Top20 Error Per Batch: 0.84 \t Current Top20 Error:  0.84\n",
      "Done Validation Batch: 50 \n",
      "Average Single Error Per Batch: 0.9684313725490195 \t Current Single Error:  0.94 \n",
      "Average Top5 Error Per Batch: 0.9050980392156861 \t Current Top5 Error:  0.89 \n",
      "Average Top10 Error Per Batch: 0.8584313725490199 \t Current Top10 Error:  0.84 \n",
      "Average Top20 Error Per Batch: 0.7882352941176469 \t Current Top20 Error:  0.75\n",
      "Done Validation Batch: 100 \n",
      "Average Single Error Per Batch: 0.9673267326732671 \t Current Single Error:  0.97 \n",
      "Average Top5 Error Per Batch: 0.9014851485148512 \t Current Top5 Error:  0.89 \n",
      "Average Top10 Error Per Batch: 0.8523762376237622 \t Current Top10 Error:  0.8200000000000001 \n",
      "Average Top20 Error Per Batch: 0.7841584158415842 \t Current Top20 Error:  0.73\n",
      "Done Validation Batch: 150 \n",
      "Average Single Error Per Batch: 0.9668874172185429 \t Current Single Error:  0.98 \n",
      "Average Top5 Error Per Batch: 0.8999337748344372 \t Current Top5 Error:  0.88 \n",
      "Average Top10 Error Per Batch: 0.8470198675496685 \t Current Top10 Error:  0.83 \n",
      "Average Top20 Error Per Batch: 0.7784105960264899 \t Current Top20 Error:  0.77\n",
      "Done Validation Batch: 200 \n",
      "Average Single Error Per Batch: 0.9663184079601983 \t Current Single Error:  0.96 \n",
      "Average Top5 Error Per Batch: 0.8999999999999994 \t Current Top5 Error:  0.94 \n",
      "Average Top10 Error Per Batch: 0.8476119402985075 \t Current Top10 Error:  0.91 \n",
      "Average Top20 Error Per Batch: 0.7782587064676613 \t Current Top20 Error:  0.8200000000000001\n",
      "Done Validation Batch: 250 \n",
      "Average Single Error Per Batch: 0.9658167330677281 \t Current Single Error:  0.98 \n",
      "Average Top5 Error Per Batch: 0.8990836653386446 \t Current Top5 Error:  0.9299999999999999 \n",
      "Average Top10 Error Per Batch: 0.845737051792829 \t Current Top10 Error:  0.87 \n",
      "Average Top20 Error Per Batch: 0.7751792828685259 \t Current Top20 Error:  0.78\n",
      "Done Validation Batch: 300 \n",
      "Average Single Error Per Batch: 0.9657807308970103 \t Current Single Error:  1.0 \n",
      "Average Top5 Error Per Batch: 0.8992026578073079 \t Current Top5 Error:  0.92 \n",
      "Average Top10 Error Per Batch: 0.8463787375415285 \t Current Top10 Error:  0.88 \n",
      "Average Top20 Error Per Batch: 0.7749501661129568 \t Current Top20 Error:  0.77\n",
      "Done Validation Batch: 350 \n",
      "Average Single Error Per Batch: 0.966182336182338 \t Current Single Error:  0.98 \n",
      "Average Top5 Error Per Batch: 0.8990598290598281 \t Current Top5 Error:  0.9 \n",
      "Average Top10 Error Per Batch: 0.8466951566951565 \t Current Top10 Error:  0.85 \n",
      "Average Top20 Error Per Batch: 0.7745584045584043 \t Current Top20 Error:  0.81\n",
      "Done Validation Batch: 400 \n",
      "Average Single Error Per Batch: 0.9670074812967606 \t Current Single Error:  0.98 \n",
      "Average Top5 Error Per Batch: 0.8999002493765579 \t Current Top5 Error:  0.9299999999999999 \n",
      "Average Top10 Error Per Batch: 0.8478553615960094 \t Current Top10 Error:  0.89 \n",
      "Average Top20 Error Per Batch: 0.7756359102244385 \t Current Top20 Error:  0.8200000000000001\n",
      "Done Validation Batch: 450 \n",
      "Average Single Error Per Batch: 0.9673614190687391 \t Current Single Error:  0.99 \n",
      "Average Top5 Error Per Batch: 0.8998004434589791 \t Current Top5 Error:  0.96 \n",
      "Average Top10 Error Per Batch: 0.848337028824833 \t Current Top10 Error:  0.87 \n",
      "Average Top20 Error Per Batch: 0.7763192904656313 \t Current Top20 Error:  0.81\n",
      "Done Validation Batch: 500 \n",
      "Average Single Error Per Batch: 0.9672654690618796 \t Current Single Error:  0.99 \n",
      "Average Top5 Error Per Batch: 0.8999201596806383 \t Current Top5 Error:  0.92 \n",
      "Average Top10 Error Per Batch: 0.8480239520958076 \t Current Top10 Error:  0.88 \n",
      "Average Top20 Error Per Batch: 0.7756886227544907 \t Current Top20 Error:  0.76\n",
      "Done Validation Batch: 550 \n",
      "Average Single Error Per Batch: 0.9673321234119826 \t Current Single Error:  0.95 \n",
      "Average Top5 Error Per Batch: 0.9001088929219597 \t Current Top5 Error:  0.9 \n",
      "Average Top10 Error Per Batch: 0.8478947368421045 \t Current Top10 Error:  0.84 \n",
      "Average Top20 Error Per Batch: 0.7753357531760431 \t Current Top20 Error:  0.78\n",
      "Done Validation Batch: 600 \n",
      "Average Single Error Per Batch: 0.9673876871880256 \t Current Single Error:  0.96 \n",
      "Average Top5 Error Per Batch: 0.8996006655574029 \t Current Top5 Error:  0.9 \n",
      "Average Top10 Error Per Batch: 0.8473044925124784 \t Current Top10 Error:  0.86 \n",
      "Average Top20 Error Per Batch: 0.7751247920133109 \t Current Top20 Error:  0.79\n",
      "Done Validation Batch: 650 \n",
      "Average Single Error Per Batch: 0.9671274961597616 \t Current Single Error:  0.94 \n",
      "Average Top5 Error Per Batch: 0.8994316436251902 \t Current Top5 Error:  0.89 \n",
      "Average Top10 Error Per Batch: 0.846927803379416 \t Current Top10 Error:  0.86 \n",
      "Average Top20 Error Per Batch: 0.7744239631336403 \t Current Top20 Error:  0.8\n",
      "Done Validation Batch: 700 \n",
      "Average Single Error Per Batch: 0.9674607703281111 \t Current Single Error:  0.98 \n",
      "Average Top5 Error Per Batch: 0.899942938659056 \t Current Top5 Error:  0.94 \n",
      "Average Top10 Error Per Batch: 0.8473038516405139 \t Current Top10 Error:  0.88 \n",
      "Average Top20 Error Per Batch: 0.7748074179743216 \t Current Top20 Error:  0.85\n",
      "Validation evaluation complete. \n",
      "Average Single Error Per Batch: 0.9675552486187932 \n",
      "Average Top5 Error Per Batch: 0.9002071823204395 \n",
      "Average Top10 Error Per Batch: 0.8475276243093929 \n",
      "Average Top20 Error Per Batch: 0.7749999999999991\n"
     ]
    }
   ],
   "source": [
    "USE_CUDA = True\n",
    "\n",
    "checkpoint = torch.load(\"outputs/anp_log_softmax_classifier_batch_7\")\n",
    "main_model = ANPClassifier(len(train_anp_tags))\n",
    "main_model.load_state_dict(checkpoint['state_dict'])\n",
    "main_model.eval()\n",
    "\n",
    "evaluate_validation(main_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done Validation Batch: 0 \n",
      "Average Single Error Per Batch: 0.98 \t Current Single Error:  0.98 \n",
      "Average Top5 Error Per Batch: 0.87 \t Current Top5 Error:  0.87 \n",
      "Average Top10 Error Per Batch: 0.85 \t Current Top10 Error:  0.85 \n",
      "Average Top20 Error Per Batch: 0.73 \t Current Top20 Error:  0.73\n",
      "Done Validation Batch: 50 \n",
      "Average Single Error Per Batch: 0.9598039215686274 \t Current Single Error:  0.96 \n",
      "Average Top5 Error Per Batch: 0.868235294117647 \t Current Top5 Error:  0.86 \n",
      "Average Top10 Error Per Batch: 0.8019607843137257 \t Current Top10 Error:  0.76 \n",
      "Average Top20 Error Per Batch: 0.7111764705882354 \t Current Top20 Error:  0.64\n",
      "Done Validation Batch: 100 \n",
      "Average Single Error Per Batch: 0.9595049504950489 \t Current Single Error:  0.94 \n",
      "Average Top5 Error Per Batch: 0.868415841584158 \t Current Top5 Error:  0.83 \n",
      "Average Top10 Error Per Batch: 0.8018811881188121 \t Current Top10 Error:  0.76 \n",
      "Average Top20 Error Per Batch: 0.7159405940594061 \t Current Top20 Error:  0.64\n",
      "Done Validation Batch: 150 \n",
      "Average Single Error Per Batch: 0.957350993377482 \t Current Single Error:  0.95 \n",
      "Average Top5 Error Per Batch: 0.8663576158940391 \t Current Top5 Error:  0.8200000000000001 \n",
      "Average Top10 Error Per Batch: 0.8000000000000002 \t Current Top10 Error:  0.75 \n",
      "Average Top20 Error Per Batch: 0.7143046357615895 \t Current Top20 Error:  0.65\n",
      "Done Validation Batch: 200 \n",
      "Average Single Error Per Batch: 0.9575621890547246 \t Current Single Error:  0.96 \n",
      "Average Top5 Error Per Batch: 0.8664676616915417 \t Current Top5 Error:  0.91 \n",
      "Average Top10 Error Per Batch: 0.8004477611940309 \t Current Top10 Error:  0.83 \n",
      "Average Top20 Error Per Batch: 0.7151741293532337 \t Current Top20 Error:  0.79\n",
      "Done Validation Batch: 250 \n",
      "Average Single Error Per Batch: 0.9574900398406359 \t Current Single Error:  0.97 \n",
      "Average Top5 Error Per Batch: 0.8663745019920316 \t Current Top5 Error:  0.9 \n",
      "Average Top10 Error Per Batch: 0.8001195219123517 \t Current Top10 Error:  0.85 \n",
      "Average Top20 Error Per Batch: 0.7150199203187247 \t Current Top20 Error:  0.74\n",
      "Done Validation Batch: 300 \n",
      "Average Single Error Per Batch: 0.9570431893687692 \t Current Single Error:  0.96 \n",
      "Average Top5 Error Per Batch: 0.8658139534883718 \t Current Top5 Error:  0.89 \n",
      "Average Top10 Error Per Batch: 0.7999667774086391 \t Current Top10 Error:  0.86 \n",
      "Average Top20 Error Per Batch: 0.7145514950166109 \t Current Top20 Error:  0.81\n",
      "Done Validation Batch: 350 \n",
      "Average Single Error Per Batch: 0.9564672364672354 \t Current Single Error:  0.97 \n",
      "Average Top5 Error Per Batch: 0.8663247863247863 \t Current Top5 Error:  0.89 \n",
      "Average Top10 Error Per Batch: 0.7997435897435905 \t Current Top10 Error:  0.83 \n",
      "Average Top20 Error Per Batch: 0.7143019943019937 \t Current Top20 Error:  0.7\n",
      "Done Validation Batch: 400 \n",
      "Average Single Error Per Batch: 0.9571820448877799 \t Current Single Error:  0.97 \n",
      "Average Top5 Error Per Batch: 0.8664089775561096 \t Current Top5 Error:  0.89 \n",
      "Average Top10 Error Per Batch: 0.7991271820448883 \t Current Top10 Error:  0.83 \n",
      "Average Top20 Error Per Batch: 0.7134912718204485 \t Current Top20 Error:  0.74\n",
      "Done Validation Batch: 450 \n",
      "Average Single Error Per Batch: 0.9572062084257204 \t Current Single Error:  0.98 \n",
      "Average Top5 Error Per Batch: 0.866829268292683 \t Current Top5 Error:  0.91 \n",
      "Average Top10 Error Per Batch: 0.7991796008869182 \t Current Top10 Error:  0.8 \n",
      "Average Top20 Error Per Batch: 0.7139689578713962 \t Current Top20 Error:  0.71\n",
      "Done Validation Batch: 500 \n",
      "Average Single Error Per Batch: 0.9572055888223557 \t Current Single Error:  0.98 \n",
      "Average Top5 Error Per Batch: 0.8665069860279443 \t Current Top5 Error:  0.92 \n",
      "Average Top10 Error Per Batch: 0.7994011976047907 \t Current Top10 Error:  0.84 \n",
      "Average Top20 Error Per Batch: 0.7133333333333327 \t Current Top20 Error:  0.69\n",
      "Done Validation Batch: 550 \n",
      "Average Single Error Per Batch: 0.9569509981851195 \t Current Single Error:  0.94 \n",
      "Average Top5 Error Per Batch: 0.8656442831215974 \t Current Top5 Error:  0.89 \n",
      "Average Top10 Error Per Batch: 0.7986025408348458 \t Current Top10 Error:  0.85 \n",
      "Average Top20 Error Per Batch: 0.713357531760435 \t Current Top20 Error:  0.77\n",
      "Done Validation Batch: 600 \n",
      "Average Single Error Per Batch: 0.9566389351081557 \t Current Single Error:  0.95 \n",
      "Average Top5 Error Per Batch: 0.8655241264559069 \t Current Top5 Error:  0.86 \n",
      "Average Top10 Error Per Batch: 0.7981031613976703 \t Current Top10 Error:  0.83 \n",
      "Average Top20 Error Per Batch: 0.7125457570715471 \t Current Top20 Error:  0.76\n",
      "Done Validation Batch: 650 \n",
      "Average Single Error Per Batch: 0.9565284178187449 \t Current Single Error:  0.94 \n",
      "Average Top5 Error Per Batch: 0.8660522273425504 \t Current Top5 Error:  0.85 \n",
      "Average Top10 Error Per Batch: 0.7985560675883256 \t Current Top10 Error:  0.79 \n",
      "Average Top20 Error Per Batch: 0.7125038402457755 \t Current Top20 Error:  0.6699999999999999\n",
      "Done Validation Batch: 700 \n",
      "Average Single Error Per Batch: 0.9564336661911611 \t Current Single Error:  0.9299999999999999 \n",
      "Average Top5 Error Per Batch: 0.8661198288159778 \t Current Top5 Error:  0.88 \n",
      "Average Top10 Error Per Batch: 0.7986019971469328 \t Current Top10 Error:  0.83 \n",
      "Average Top20 Error Per Batch: 0.712710413694722 \t Current Top20 Error:  0.71\n",
      "Validation evaluation complete. \n",
      "Average Single Error Per Batch: 0.9566160220994535 \n",
      "Average Top5 Error Per Batch: 0.8662154696132606 \n",
      "Average Top10 Error Per Batch: 0.7985220994475133 \n",
      "Average Top20 Error Per Batch: 0.7129143646408845\n"
     ]
    }
   ],
   "source": [
    "USE_CUDA = True\n",
    "\n",
    "checkpoint = torch.load(\"outputs/anp_log_softmax_classifier_batch_5\")\n",
    "main_model = ANPClassifier(len(train_anp_tags))\n",
    "main_model.load_state_dict(checkpoint['state_dict'])\n",
    "main_model.eval()\n",
    "\n",
    "evaluate_validation(main_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done Validation Batch: 0 \n",
      "Average Single Error Per Batch: 0.98 \t Current Single Error:  0.98 \n",
      "Average Top5 Error Per Batch: 0.94 \t Current Top5 Error:  0.94 \n",
      "Average Top10 Error Per Batch: 0.83 \t Current Top10 Error:  0.83 \n",
      "Average Top20 Error Per Batch: 0.76 \t Current Top20 Error:  0.76\n",
      "Done Validation Batch: 50 \n",
      "Average Single Error Per Batch: 0.9660784313725486 \t Current Single Error:  0.98 \n",
      "Average Top5 Error Per Batch: 0.8958823529411766 \t Current Top5 Error:  0.88 \n",
      "Average Top10 Error Per Batch: 0.8403921568627453 \t Current Top10 Error:  0.77 \n",
      "Average Top20 Error Per Batch: 0.7658823529411763 \t Current Top20 Error:  0.69\n",
      "Done Validation Batch: 100 \n",
      "Average Single Error Per Batch: 0.9675247524752469 \t Current Single Error:  0.97 \n",
      "Average Top5 Error Per Batch: 0.8958415841584159 \t Current Top5 Error:  0.86 \n",
      "Average Top10 Error Per Batch: 0.8404950495049504 \t Current Top10 Error:  0.78 \n",
      "Average Top20 Error Per Batch: 0.7653465346534656 \t Current Top20 Error:  0.64\n",
      "Done Validation Batch: 150 \n",
      "Average Single Error Per Batch: 0.96728476821192 \t Current Single Error:  0.97 \n",
      "Average Top5 Error Per Batch: 0.8960264900662255 \t Current Top5 Error:  0.86 \n",
      "Average Top10 Error Per Batch: 0.8408609271523175 \t Current Top10 Error:  0.81 \n",
      "Average Top20 Error Per Batch: 0.7656291390728479 \t Current Top20 Error:  0.73\n",
      "Done Validation Batch: 200 \n",
      "Average Single Error Per Batch: 0.9669154228855713 \t Current Single Error:  0.97 \n",
      "Average Top5 Error Per Batch: 0.8948756218905468 \t Current Top5 Error:  0.92 \n",
      "Average Top10 Error Per Batch: 0.8395024875621899 \t Current Top10 Error:  0.87 \n",
      "Average Top20 Error Per Batch: 0.7650248756218907 \t Current Top20 Error:  0.81\n",
      "Done Validation Batch: 250 \n",
      "Average Single Error Per Batch: 0.9670916334661346 \t Current Single Error:  0.98 \n",
      "Average Top5 Error Per Batch: 0.8949003984063737 \t Current Top5 Error:  0.94 \n",
      "Average Top10 Error Per Batch: 0.8396414342629496 \t Current Top10 Error:  0.88 \n",
      "Average Top20 Error Per Batch: 0.7648207171314745 \t Current Top20 Error:  0.79\n",
      "Done Validation Batch: 300 \n",
      "Average Single Error Per Batch: 0.9670099667774094 \t Current Single Error:  0.98 \n",
      "Average Top5 Error Per Batch: 0.8946511627906972 \t Current Top5 Error:  0.94 \n",
      "Average Top10 Error Per Batch: 0.8394352159468453 \t Current Top10 Error:  0.9 \n",
      "Average Top20 Error Per Batch: 0.7653488372093027 \t Current Top20 Error:  0.85\n",
      "Done Validation Batch: 350 \n",
      "Average Single Error Per Batch: 0.9668945868945888 \t Current Single Error:  1.0 \n",
      "Average Top5 Error Per Batch: 0.8949857549857545 \t Current Top5 Error:  0.91 \n",
      "Average Top10 Error Per Batch: 0.8398290598290611 \t Current Top10 Error:  0.86 \n",
      "Average Top20 Error Per Batch: 0.7651282051282051 \t Current Top20 Error:  0.77\n",
      "Done Validation Batch: 400 \n",
      "Average Single Error Per Batch: 0.9671820448877826 \t Current Single Error:  0.98 \n",
      "Average Top5 Error Per Batch: 0.8955860349127176 \t Current Top5 Error:  0.9299999999999999 \n",
      "Average Top10 Error Per Batch: 0.8400249376558613 \t Current Top10 Error:  0.9 \n",
      "Average Top20 Error Per Batch: 0.7652119700748133 \t Current Top20 Error:  0.8200000000000001\n",
      "Done Validation Batch: 450 \n",
      "Average Single Error Per Batch: 0.9675166297117546 \t Current Single Error:  0.98 \n",
      "Average Top5 Error Per Batch: 0.8965631929046555 \t Current Top5 Error:  0.95 \n",
      "Average Top10 Error Per Batch: 0.8405986696230606 \t Current Top10 Error:  0.91 \n",
      "Average Top20 Error Per Batch: 0.7652771618625278 \t Current Top20 Error:  0.8\n",
      "Done Validation Batch: 500 \n",
      "Average Single Error Per Batch: 0.9679840319361314 \t Current Single Error:  0.98 \n",
      "Average Top5 Error Per Batch: 0.8966866267465062 \t Current Top5 Error:  0.9299999999999999 \n",
      "Average Top10 Error Per Batch: 0.8410778443113773 \t Current Top10 Error:  0.9 \n",
      "Average Top20 Error Per Batch: 0.7654491017964071 \t Current Top20 Error:  0.77\n",
      "Done Validation Batch: 550 \n",
      "Average Single Error Per Batch: 0.9679310344827631 \t Current Single Error:  0.94 \n",
      "Average Top5 Error Per Batch: 0.8963702359346633 \t Current Top5 Error:  0.94 \n",
      "Average Top10 Error Per Batch: 0.8408892921960069 \t Current Top10 Error:  0.89 \n",
      "Average Top20 Error Per Batch: 0.7653720508166969 \t Current Top20 Error:  0.84\n",
      "Done Validation Batch: 600 \n",
      "Average Single Error Per Batch: 0.9678369384359461 \t Current Single Error:  0.95 \n",
      "Average Top5 Error Per Batch: 0.8960732113144748 \t Current Top5 Error:  0.87 \n",
      "Average Top10 Error Per Batch: 0.8403993344425955 \t Current Top10 Error:  0.81 \n",
      "Average Top20 Error Per Batch: 0.7647254575707153 \t Current Top20 Error:  0.75\n",
      "Done Validation Batch: 650 \n",
      "Average Single Error Per Batch: 0.9677112135176723 \t Current Single Error:  0.9299999999999999 \n",
      "Average Top5 Error Per Batch: 0.896513056835635 \t Current Top5 Error:  0.89 \n",
      "Average Top10 Error Per Batch: 0.8406605222734258 \t Current Top10 Error:  0.85 \n",
      "Average Top20 Error Per Batch: 0.7651920122887863 \t Current Top20 Error:  0.78\n",
      "Done Validation Batch: 700 \n",
      "Average Single Error Per Batch: 0.9679457917261136 \t Current Single Error:  0.97 \n",
      "Average Top5 Error Per Batch: 0.8962339514978575 \t Current Top5 Error:  0.92 \n",
      "Average Top10 Error Per Batch: 0.8405848787446513 \t Current Top10 Error:  0.86 \n",
      "Average Top20 Error Per Batch: 0.7651212553495004 \t Current Top20 Error:  0.76\n",
      "Validation evaluation complete. \n",
      "Average Single Error Per Batch: 0.9680801104972462 \n",
      "Average Top5 Error Per Batch: 0.8963950276243069 \n",
      "Average Top10 Error Per Batch: 0.840621546961327 \n",
      "Average Top20 Error Per Batch: 0.7650966850828724\n"
     ]
    }
   ],
   "source": [
    "USE_CUDA = True\n",
    "\n",
    "checkpoint = torch.load(\"outputs/anp_log_softmax_classifier_batch_3\")\n",
    "main_model = ANPClassifier(len(train_anp_tags))\n",
    "main_model.load_state_dict(checkpoint['state_dict'])\n",
    "main_model.eval()\n",
    "\n",
    "evaluate_validation(main_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done Validation Batch: 0 \n",
      "Average Single Error Per Batch: 0.97 \t Current Single Error:  0.97 \n",
      "Average Top5 Error Per Batch: 0.9299999999999999 \t Current Top5 Error:  0.9299999999999999 \n",
      "Average Top10 Error Per Batch: 0.85 \t Current Top10 Error:  0.85 \n",
      "Average Top20 Error Per Batch: 0.74 \t Current Top20 Error:  0.74\n",
      "Done Validation Batch: 50 \n",
      "Average Single Error Per Batch: 0.9705882352941171 \t Current Single Error:  0.98 \n",
      "Average Top5 Error Per Batch: 0.9049019607843137 \t Current Top5 Error:  0.91 \n",
      "Average Top10 Error Per Batch: 0.8529411764705881 \t Current Top10 Error:  0.8200000000000001 \n",
      "Average Top20 Error Per Batch: 0.7770588235294118 \t Current Top20 Error:  0.74\n",
      "Done Validation Batch: 100 \n",
      "Average Single Error Per Batch: 0.971881188118811 \t Current Single Error:  0.98 \n",
      "Average Top5 Error Per Batch: 0.9073267326732675 \t Current Top5 Error:  0.88 \n",
      "Average Top10 Error Per Batch: 0.8542574257425741 \t Current Top10 Error:  0.77 \n",
      "Average Top20 Error Per Batch: 0.7775247524752474 \t Current Top20 Error:  0.7\n",
      "Done Validation Batch: 150 \n",
      "Average Single Error Per Batch: 0.9717218543046348 \t Current Single Error:  0.94 \n",
      "Average Top5 Error Per Batch: 0.9073509933774834 \t Current Top5 Error:  0.91 \n",
      "Average Top10 Error Per Batch: 0.8533112582781455 \t Current Top10 Error:  0.8200000000000001 \n",
      "Average Top20 Error Per Batch: 0.7769536423841058 \t Current Top20 Error:  0.77\n",
      "Done Validation Batch: 200 \n",
      "Average Single Error Per Batch: 0.9723880597014918 \t Current Single Error:  0.96 \n",
      "Average Top5 Error Per Batch: 0.9069651741293524 \t Current Top5 Error:  0.9299999999999999 \n",
      "Average Top10 Error Per Batch: 0.8525870646766166 \t Current Top10 Error:  0.87 \n",
      "Average Top20 Error Per Batch: 0.7780597014925376 \t Current Top20 Error:  0.84\n",
      "Done Validation Batch: 250 \n",
      "Average Single Error Per Batch: 0.9721513944223099 \t Current Single Error:  0.99 \n",
      "Average Top5 Error Per Batch: 0.9065737051792817 \t Current Top5 Error:  0.9299999999999999 \n",
      "Average Top10 Error Per Batch: 0.8509163346613549 \t Current Top10 Error:  0.89 \n",
      "Average Top20 Error Per Batch: 0.7775298804780879 \t Current Top20 Error:  0.79\n",
      "Done Validation Batch: 300 \n",
      "Average Single Error Per Batch: 0.9723588039867108 \t Current Single Error:  0.97 \n",
      "Average Top5 Error Per Batch: 0.9063122923588028 \t Current Top5 Error:  0.91 \n",
      "Average Top10 Error Per Batch: 0.8503322259136218 \t Current Top10 Error:  0.87 \n",
      "Average Top20 Error Per Batch: 0.7772757475083062 \t Current Top20 Error:  0.8200000000000001\n",
      "Done Validation Batch: 350 \n",
      "Average Single Error Per Batch: 0.9723931623931636 \t Current Single Error:  0.99 \n",
      "Average Top5 Error Per Batch: 0.9073219373219368 \t Current Top5 Error:  0.94 \n",
      "Average Top10 Error Per Batch: 0.8508547008547006 \t Current Top10 Error:  0.84 \n",
      "Average Top20 Error Per Batch: 0.7778062678062683 \t Current Top20 Error:  0.76\n",
      "Done Validation Batch: 400 \n",
      "Average Single Error Per Batch: 0.9723192019950142 \t Current Single Error:  0.98 \n",
      "Average Top5 Error Per Batch: 0.9074064837905227 \t Current Top5 Error:  0.92 \n",
      "Average Top10 Error Per Batch: 0.851246882793017 \t Current Top10 Error:  0.89 \n",
      "Average Top20 Error Per Batch: 0.7781296758104739 \t Current Top20 Error:  0.8200000000000001\n",
      "Done Validation Batch: 450 \n",
      "Average Single Error Per Batch: 0.972416851441244 \t Current Single Error:  0.98 \n",
      "Average Top5 Error Per Batch: 0.9081818181818175 \t Current Top5 Error:  0.96 \n",
      "Average Top10 Error Per Batch: 0.8525055432372503 \t Current Top10 Error:  0.92 \n",
      "Average Top20 Error Per Batch: 0.7789356984478933 \t Current Top20 Error:  0.8200000000000001\n",
      "Done Validation Batch: 500 \n",
      "Average Single Error Per Batch: 0.9724950099800427 \t Current Single Error:  0.98 \n",
      "Average Top5 Error Per Batch: 0.9081836327345304 \t Current Top5 Error:  0.92 \n",
      "Average Top10 Error Per Batch: 0.8526147704590812 \t Current Top10 Error:  0.86 \n",
      "Average Top20 Error Per Batch: 0.7788023952095806 \t Current Top20 Error:  0.77\n",
      "Done Validation Batch: 550 \n",
      "Average Single Error Per Batch: 0.9722867513611653 \t Current Single Error:  0.98 \n",
      "Average Top5 Error Per Batch: 0.9076588021778581 \t Current Top5 Error:  0.9 \n",
      "Average Top10 Error Per Batch: 0.8526134301270413 \t Current Top10 Error:  0.84 \n",
      "Average Top20 Error Per Batch: 0.7789836660617057 \t Current Top20 Error:  0.8\n",
      "Done Validation Batch: 600 \n",
      "Average Single Error Per Batch: 0.9722296173044978 \t Current Single Error:  0.97 \n",
      "Average Top5 Error Per Batch: 0.9076206322795334 \t Current Top5 Error:  0.91 \n",
      "Average Top10 Error Per Batch: 0.8530782029950078 \t Current Top10 Error:  0.84 \n",
      "Average Top20 Error Per Batch: 0.7797337770382695 \t Current Top20 Error:  0.8\n",
      "Done Validation Batch: 650 \n",
      "Average Single Error Per Batch: 0.9719508448540771 \t Current Single Error:  0.94 \n",
      "Average Top5 Error Per Batch: 0.907695852534561 \t Current Top5 Error:  0.89 \n",
      "Average Top10 Error Per Batch: 0.8533947772657448 \t Current Top10 Error:  0.87 \n",
      "Average Top20 Error Per Batch: 0.7797849462365593 \t Current Top20 Error:  0.83\n",
      "Done Validation Batch: 700 \n",
      "Average Single Error Per Batch: 0.9720684736091375 \t Current Single Error:  0.96 \n",
      "Average Top5 Error Per Batch: 0.9075748930099843 \t Current Top5 Error:  0.88 \n",
      "Average Top10 Error Per Batch: 0.853552068473609 \t Current Top10 Error:  0.8200000000000001 \n",
      "Average Top20 Error Per Batch: 0.7799001426533524 \t Current Top20 Error:  0.77\n",
      "Validation evaluation complete. \n",
      "Average Single Error Per Batch: 0.9721270718232127 \n",
      "Average Top5 Error Per Batch: 0.9079281767955789 \n",
      "Average Top10 Error Per Batch: 0.8541436464088396 \n",
      "Average Top20 Error Per Batch: 0.7802624309392262\n"
     ]
    }
   ],
   "source": [
    "USE_CUDA = True\n",
    "\n",
    "checkpoint = torch.load(\"outputs/anp_log_softmax_classifier_batch_1\")\n",
    "main_model = ANPClassifier(len(train_anp_tags))\n",
    "main_model.load_state_dict(checkpoint['state_dict'])\n",
    "main_model.eval()\n",
    "\n",
    "evaluate_validation(main_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
